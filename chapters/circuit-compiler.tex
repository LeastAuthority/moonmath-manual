\chapter{Circuit Compiler} 
It is possible to verify the the correct execution of arbitrary bounded computation in terms of rank-$1$ contraints systems and it is possible to encode such computations in terms of algebraic circuits. However writing actual computer programs as circuits and the associated verification in terms of rank-1 constraint systems is as least as troublesome as writing any other low level language like assembler code. 

From a practical point of view it is therefore necessary to have some kind of compiler framework at hand, capable to transform high level languages into arithmetic circuits and associated rank-1 constraint systems. 

As we have seen in XXX as well as XXX and XXX, both arithmetic circuits and rank-1 constraint systems have a modularity property by which it is possible to synthezise  complex circuits and constraint systems from simpler ones. A basic approach taken by circuit/R1CS compilers like (LIST) is therefore to provide a library of atomic and simple circuits and then define a way to combine them into arbitrary complex systems. 

In this chapter we will provide an introduction to basic concepts of so called \textit{circuit/R1CS compilers} and derive a toy language which we can "compile" in a pen and paper approach into algebraic circuits and their associated rank-1 constraints systems. The language is very simple, informal, somewhat typed and similar to the hardware description language paradigm but without a notion of time. 

We start with a general introduction to our language and then intoduce atomic types like booleans, unsigned integers and define the fundamental control flow primitives like the ternery operator or the bounded loop. We will also look at basic functionality primitives like elliptic curve cryptography and cryptographic hash functions. Primitives like those are often called \textbf{gadgets} in the literature. We also give a very shallow introduction to how compilers for complex circuit design might be constructed, also we only scratch the surface. In particular we focus on a kind of typed hardware description language style of compilers and will only look into more elaborate approaches like RAM-model compilers in future versions of the book.

From a developers perspective, it is desireable to design statements in formal languages like normal computer programs and then let a compiler handle the technical details. 

However in contrast to normal executable programs, programs for circuit compilers have two modes of execution. The first mode is usally called \textit{setup phase} is executed in order to generate the circuit and its associated rank-1 constraint system, the letter of which is then usually used as input to some zero knowledge proofing system.

The second mode of execution is usually called the \textit{proofer phase} and in this phase a proofer usually computes a valid assignment to the circuit. Depending on the usecase this valid assignment is then either directly used as constructive proof for proper circuit execution or is transfered as input to the proof generation algorithm of some zero knowledge proofing system, where the full size, non hiding constructive proof is processed into a succinct proof with various levels of zero-knowledge.

Modern circuit languages and their associated compilers abstract over those two phase and provide a unified interphase to the developer, who then writes a single program that can be used in both phases. 

\subsection{The PAPER Language} To explain basic concepts of circuit compiler, we derive a toy language and associate compiler nemed \texttt{PAPER} which is an acronym for the "(P)en (A)nd (P)aper (E)xecution (R)ules", which we develop.

PAPER allows programmers to define algebraic circuit in VERILOG like pseudo-code. Complex circuits are build from simple once. 

A circuit description starts with the reserved keyword \textit{circuit} and is followed by the field where the circuit is defined on, followed by a list of unsigned integers, known at compile time.

After this the circuits input and output nodes are defined. Every input and output nodes has a specifier that declares the associated edge label to be public or private. Constant input and output nodes are always private.

% used the command line style formalized e.g. here: http://docopt.org/
\begin{lstlisting}
module <Name> {<F : Field> [ , <N_1: unsigned>,... ] } {
	[ input ( public | private ) <Name> : <Type> ;... ]
	[ input const <Name> : <Type> = <Value> ;... ]
	[ output ( public | private ) <Name> : <Type> ;... ]
	[ output const <Name> : <Type> = <Value> ;... ]

	circuit
		[ output <== expression(input) ;... ]
}
\end{lstlisting} 
where expression is SOMETHING

The language only accepts binary field operations, that is expressions like $x_1\cdot x_2\cdot x_3$ are not allowed.

\begin{itemize}
\item For every input:
\begin{itemize}
\item  draw an input node and label the node with the name of the input.
\item If the input is a variable draw the edge label $S_j$ on ever outgoing edge
\item draw the constraining circuit of the type
\end{itemize}
\item For every output:
\begin{itemize}
\item  draw an output node and label the node with the name of the out.
\item If the output is a variable draw the edge label $S_j$ on the ingoing edge
\item If the output is a constant draw the edge label $S_value$ on its ingoing edge
\end{itemize}
\item check if all modules are defined
\item type check the module body 
\item Draw edges from the input nodes to the output nodes by inductively rewriting all modules in the module body with their associated circuits and replace all edges label with witness labels.
\end{itemize}
\begin{example}[A trivial Circuit] To give an intuition of how to write and compile circuits in the \texttt{PAPER} language, consider a trivial circuit, that has three inputs and four outputs and forwards its inputs to the outputs.
\begin{lstlisting}
module trivial_circuit {F_13} {
	input private in_1 : F ; 
	input public in_2 : F ; 
	input const inc_1 : F = 7 ;
	output private out_1 : F ;
	output public out_2 : F ;
	output const outc_1 : F = 0 ;   

	circuit
		out_1 <== inc_1;
		out_2 <== in_1;
		outc_1 <== in_2;
}
\end{lstlisting} 
Using \texttt{PAPER}, we start with an empty circuit and then add $3$ input nodes and $3$ output node to that circuit. Nodes that corresponse to variables are labeled with the variable's names and nodes that corresponse to constants are labeled with the associated values. As we will see in paragraph XXX, the field type has no associated constraints, so we don't need to draw any circuits associated the types of the variables. 

We then check the validity of every expression in the \texttt{circuit} section of the module, including a type check. Since the circuit only wires inputs to outputs and all inputs are of the same type as all outputs, the check is valid.

We then draw the edges from the input nodes to the output nodes, by inductive rewriting all modules in all exprssions. Since we don't have any modules in the expression, we don't have to evaluate anything and can simply wire inputs to outputs directly. We get the following edges
\begin{align*}
E(inc_1, out_1) \\
E(in_1, out_2) \\
E(in_2, outc_1)
\end{align*}
To label those edges, we use the general rules of algebraic circuits as defined in XXX. According to those rules every incoming edge of a sink node has a label and every outgoing edge of a source node has a label, if the node is labeled with a variable.

Since nodes that represent constants are implicitly assumed to be private and since the private public specifiers determine if we have to choose the symbol $W$ or $I$ for the edge labels, we get the following circuit:
\begin{center}
\digraph[scale=0.4]{TRIVIAL1}{
	forcelabels=true;
	center=true;
	splines=ortho;
	nodesep= 2.0;
	n1 [shape=box, label="in_1"];
	n2 [shape=box, label="in_2"];
	n3 [shape=box, label="7"];
	n4 [shape=box, label="out_1"];
	n5 [shape=box, label="out_2"];
	n6 [shape=box, label="0"];
	
	n1 -> n5 [xlabel="W_(in_1)"] ;
	n2 -> n6 [xlabel="I_(in_2)"] ;
	n3 -> n4 ;
}
\end{center}
\end{example}
\subsection{Atomic Types} 
% https://zeroknowledge.fm/172-2/ reference for all the languages
Since both algebraic circuits and their associated rank-1 constraint systems are defined over finite fields, the natural underlying informational units are elements from those fields. In a sense field elements $x\in \F$ are for algebraic circuits what bits are for NORMAL computers. However most computer programs are optimized for machine words, which are arrays of bits. To compile computer programs into R1CS it is therefore often necessary to simulate atomic types like booleans and uInts inside algebraic circuits.
\subsubsection{The Basefield type} 
TECHNO:

It should be noted that subtraction gates can be simulated in a two step process by first multiplying the subtrahend with $-1$ and then use an addition gate. Also division gates can be simulated for example by using Fermat's little theorem, which states that the multiplicative inverse of a field element $x\in\F$ is given by the power $x^{p-2}$ and powers are nothing but repeated multiplication. However simulating division this way might be inefficient.


The most basic type of an algebraic circuit or its associated R1CS is the type $\F$ of the field that underlays the circuit. In a sense elements $x\in\F$ are for algebraic circuits what binary machine words are for computers. They are the fundamental computational units that everything else needs to be expressed in. 

As this type is the basis for everything else, no circuit or constraint is needed to represent this type. The properties of this type are inherited from $\F$, that is we a notion of \textit{addition}, \textit{subtraction}, \textit{multiplication} and \textit{division}. 

A commonn strategy from the perspective of circuit/r1cs compiler design, is that allocating variables of the base field type in expressions like
\begin{lstlisting}
input (public) x : F ; 
input (private) y : F ; 
const c : F = value ; 
\end{lstlisting} 
are compiled into input nodes of the circuit the compiler generates. Public and private inputes are compiled into source nodes decorated with input variables and constants are compiled into source nodes decorated with the allocated constant. In out notation, outgoing edges of public inputs are labeled with instance variables $I_j$ and outgoing edges of private inputs are labeled with witness variables $W_j$.
\begin{example}[$3$-factorization] Consider our $3$-factorization problem from example XXX and the associated circuit $C_{3.fac\_zk}(\F_{13})$ we provided in example XXX. We want to define a module in the \texttt{PAPER} language, that we can compile into an algebraic circuit equivalent to $C_{3.fac\_zk}(\F_{13})$. We write
\begin{lstlisting}
module 3_fac_zk (F_13) (
	input private x_1 : F ; 
	input private x_2 : F ;
	input private x_3 : F ;  
	output public f_3.fac_zk : F ; 

	circuit
		f_3.fac_zk <== (x_1 * x_2) * x_3;
)
\end{lstlisting} 
Using \texttt{PAPER}, we start with an empty circuit and then add $3$ input nodes and $1$ output node to that circuit. All these nodes are decorated with the associated variable names. 

\end{example}

\begin{example} To give an intuition of how a simple hardware description language like circuit compiler might work, lets consider the following VERILOG like pseudo-code:
\begin{lstlisting}
input (public) x : F ; 
input (private) y : F ; 
const c : F = 1 ; 

begin
	x * y == c ;
end ;
\end{lstlisting}
A very simple compiler would then allocate two input nodes for the variables $x$ and $y$, a multiplication gate for the single multiplication gate and another node for the field constant $c=1$. The outgoing edges of $x$ are public edges and the outgoing edges of $y$ are private edges. If the compiler assumes the rule that all edges not explicitly declared as public are private, it could generate the following circuit: 
\begin{center}
\digraph[scale=0.4]{SIMPLEMUL}{
	forcelabels=true;
	center=true;
	splines=ortho;
	nodesep= 2.0;
	n1 -> n4 [xlabel="I_1"] ;
	n2 -> n4 [xlabel="W_2"] ;
	n4 -> n3 [xlabel="W_3=1"];
	n1 [shape=box, label="x"];
	n2 [shape=box, label="y"];
	n3 [shape=box, label="1"];
	n4 [label="*"];
}
\end{center} 
And using the general process of deriving an associated rank-1 constraint system to this circuit, the compiler might produce:
$$
I_1 \cdot W_2 =1
$$
\end{example} 
\paragraph{The Subtraction Constraints System}Since algebraic circuits, by definition only contain addition and multiplication gates, there is no single gate for field subtraction, despite field division being a native single op in every field. Algebraic circuits therefore need another way to deal with subtraction. To see how this can be achieved, recall that subtraction is defined as addition with the multipilcative inverse in the field and the additive inverse can be computed efficiently by multiplication with the additive inverse of $-1$, which is a field conatant that only needs to be computed once during setup time. An associated circuit could the look like:
\begin{center}
\digraph[scale=0.4]{BTSUB}{
	forcelabels=true;
	center=true;
	splines=ortho;
	nodesep= 2.0;
	n1 -> n5 [xlabel="E_1    "];
	n2 -> n4 [xlabel="E_2    "];
	n3 -> n4 ;
	n4 -> n5 ;
	n5 -> n6 [xlabel="E_3    "];
	n1 [shape=box, label="x"];
	n2 [shape=box, label="y"];
	n3 [shape=box, label="-1"];
	n4 [label="*"];	
	n5 [label="+"];
	n6 [shape=box, label="SUB(x,y)"]
}
\end{center}
Any valid assignment $\{E_1,E_2, E_3\}$ to this circuit enforces that $E_3$ is the difference $E_1- E_2$.

Using the mothod from XXX, we transform this circuit in the following rank-1 constraint system:
\begin{equation}
\left(E_1 + (-1)\cdot E_2\right)\cdot 1 = E_3
\end{equation}

\paragraph{The Inversion Constraint System} Since algebraic circuits, by definition only contain addition and multiplication gates, there is no single gate for field inversion, despite field inversion being a native single op in every field. Algebraic circuits therefore need another way to deal with field inversion. 

If the underlying field is a prime field, one approach would be to use Fermat's little theorem XXX to compute the multiplicative inverse inside the circuit. To see how this works let $\F_p$ be the prime field. Then the multiplicative inverse of a field elemen $x\in\F$ with $x\neq 0$ is given by $x^{-1}= x^{p-2}$. We therefore need to compute $x^{p-2}$ in the circuit. For large prime numbers $p$ as they are used in cryptographically relevent prime fields, $p-2$ repeaded multiplication gates to compute $x\cdot\ldots \cdot x$ is infeasible and a double andcmultiply approach as in XXX is needed to compute $x^{p-2}$ in roughly $log_2(p)$ steps. This is possible but adds a lot of constraints to the circuit, ignoring that inversion is a field native operation.

A more constraints friendly approach is to allow inversion outside of the circuit and then only enforce correctness of the inversion in the circuit. To understand what this means, observe that by defintion, a field element $y\in \F$ is the mutiplicative inverse of a field element $x\in \F$, if and only if $x\cdot y =1$ in $\F$. We can therefore define a circuit, that takes not only $x$ as input but another input $y$ and design it, such that a valid assignment enforces $y$ to be the multiplicative inverse of $x$. The following cicuit represents the equation $x\cdot y =1$ and therefore does the trick:
\begin{center}
\digraph[scale=0.4]{BTINV}{
	forcelabels=true;
	center=true;
	splines=ortho;
	nodesep= 2.0;
	n1 -> n3 [xlabel="E_1  "];
	n2 -> n3 [xlabel="E_2  "];
	n3 -> n4 [xlabel="E_3 =1  "];
	n1 [shape=box, label="x"];
	n2 [shape=box, label="x_INV"];
	n3 [label="*"];	
	n4 [shape=box, label="1"];	
}
\end{center}
Any valid assignment $\{E_1,E_2\}$ to this circuit enforces that $E_2$ is the multiplicative inverse of $E_1$ and since there is no field element $E_2$, such that $0\cdot E_2=1$, it also handles the fact, that the mltiplicative inverse of $0$ is not defined in any field.

Using the mothod from XXX, we transform this circuit in the following rank-1 constraint system, enforcing that input $y$ is the multiplicative :
\begin{equation}
E_1 \cdot E_2 = 1
\end{equation}
\paragraph{The Division Constraint System} Since algebraic circuits, by definition only contain addition and multiplication gates, there is no single gate for field division, despite field division being a native single op in every field. Algebraic circuits therefore need another way to deal with division.

Since by definition, division is nothing but multiplication with the multiplicative inverse, we can define divsion in circuits using the inversion circuit and constraint system from the prvious paragraph XXX, executing the expensive inversion part outside of the circuit. We get 
\begin{center}
\digraph[scale=0.4]{BTDIV}{
	forcelabels=true;
	center=true;
	splines=ortho;
	nodesep= 2.0;
	n1 -> n6 [xlabel="E_1  "];
	n2 -> n4 [xlabel="E_2  "];
	n3 -> n6 [xlabel="E_3  "];
	n3 -> n4 [xlabel="E_3  "];
	n4 -> n5 [xlabel="E_4  "];
	n6 -> n7 [xlabel="E_5  "];
	n1 [shape=box, label="x"];
	n2 [shape=box, label="y"];
	n3 [shape=box, label="y_INV"];
	n4 [label="*"];	
	n5 [shape=box, label="1"];
	n6 [label="*"];	
	n7 [shape=box, label="DIV(x,y)"];	
}
\end{center} 
Any valid assignment $\{E_1,E_2,E_3,E_4,E_5\}$ to this circuit enforces that $E_5$ is the output of the field divsion of $E_1$ by $E_2$. It handles the fact, that division by $0$ is not defined, by not having any valid assignment in case $E_2=0$.

Using the mothod from XXX, we transform this circuit in the following rank-1 constraint system, enforcing that input $y$ is the multiplicative :
\begin{align*}
E_2 \cdot E_3 &= 1\\
E_1 \cdot E_3 &= E_5
\end{align*}
\paragraph{Modularity} Implementing bounded computation in algebraic circuits it is often necessary to deal with complex expressions of the field type. As we have seen in XXX and XXX, both algebraic circuits and R1CS have a modularity property, which enables a compiler to derive algebraic circuit implementations for arbitrary circuits. 
\begin{comment}
\begin{example} Consider the prime field $\F_{13}$. In this example, we want to derive an algebraic circuit and associated R1CS that enforces a pair $(x,y)\in \F_{13}^2$ to be the sum of two tiny jubjub curve points $(x_1,y_1)$ and $(x_2,y_2)$. We assume that we already know that $(x_1,x_2)$ as well as $(x_2,y_2)$ are tiny jubjub points, that is we assume that they are the inputs to valid assignments of circuit XXX. 

To synthezise the associated circuit, we start with the twisted Edwards addition law XXX of the tiny jubjub curve:
$$
(x,y) = \left(\frac{x_1y_2+y_1x_2}{1+8x_1y_1x_2y_2}, \frac{y_1y_2-3x_1x_2}{1-8x_1y_1x_2y_2} \right)
$$ 
To transformation this expression into a circuit we rewrite it in terms of the binary operators $ADD$, $SUB$, $MUL$, $DIV$ that represent the four fundamental field operations in $\F_{13}$. We get
\begin{align*}
(x,y) & = (\\
  & \scriptstyle DIV(ADD(MUL(x_1,y_2),MUL(y_1,x_2)),
         ADD(1,MUL(8,MUL(MUL(x_1,y_1),MUL(x_2,y_2))))), \\   
  & \scriptstyle DIV(ADD(MUL(y_1,y_2),MUL(MUL(3,x_1),x_2)),
         ADD(1,MUL(8,MUL(MUL(x_1,y_1),MUL(x_2,y_2)))))\\
  & )
\end{align*}
We then proceed inductively choosing circuits for the outer most operators, which in this case are two division circuits. We don't expand their inputs into circuits yet, but only represent the inputs symbolically. For better readability we use the symbols of the next operator only, because otherwise the circuit becomes unreadable. We get:
\begin{center}
\digraph[scale=0.4]{TEA}{
	forcelabels=true;
	center=true;
	splines=ortho;
	nodesep= 2.0;
	// x-value
	nx1 -> nx6 [xlabel="Ex_1  "];
	nx2 -> nx4 [xlabel="Ex_2  "];
	nx3 -> nx6 [xlabel="Ex_3  "];
	nx3 -> nx4 [xlabel="Ex_3  "];
	nx4 -> nx5 [xlabel="Ex_4  "];
	nx6 -> nx7 [xlabel="Ex_5  "];
	nx1 [shape=box, label="ADD(.,.)"];
	nx2 [shape=box, label="ADD(.,.)"];
	nx3 [shape=box, label="ADD(.,.)_INV"];
	nx4 [label="*"];	
	nx5 [shape=box, label="1"];
	nx6 [label="*"];	
	nx7 [shape=box, label="DIV(ADD(.,.),ADD(.,.))"];
	// y-value
	ny1 -> ny6 [xlabel="Ey_1  "];
	ny2 -> ny4 [xlabel="Ey_2  "];
	ny3 -> ny6 [xlabel="Ey_3  "];
	ny3 -> ny4 [xlabel="Ey_3  "];
	ny4 -> ny5 [xlabel="Ey_4  "];
	ny6 -> ny7 [xlabel="Ey_5  "];
	ny1 [shape=box, label="ADD(.,.)"];
	ny2 [shape=box, label="ADD(.,.)"];
	ny3 [shape=box, label="ADD(.,.)_INV"];
	ny4 [label="*"];	
	ny5 [shape=box, label="1"];
	ny6 [label="*"];	
	ny7 [shape=box, label="DIV(ADD(.,.),ADD(.,.))"];	
}
\end{center}

\end{example}
\end{comment}

\subsubsection{The Boolean Type} 
% implementations can be found here: https://github.com/filecoin-project/zexe/tree/master/snark-gadgets/src/bits

It is often necessary to assume that a statement contains expressions of boolean variables. However by definition the alphabet of a statement is a finite field, which is usually the scalar field of a large prime order cyclic group. So developers need a way to simulate boolean algebra inside finite fields.

The most common way to do this in algebraic circuits and their associated rank-1 constraint systems, is to interpret the additive and multiplicate neutral element $\{0,1\}\subset \F$ as boolean values, such that $0$ represents $false$ and $1$ represents $true$. Boolean functions like $and$, $or$, $xor$ and so on are the expressable in terms of computations inside $\F$. 

Representing booleans this way is convinient because the elements $0$ and $1$ are defined in any field. The representation is therefore independent of the actual field in consideration. 

To fix Boolean algebra notation in what follows, we often write $0$ to represent $false$ and $1$ to represent $true$. We also write $\wedge$ to represent the boolean AND as well as $\vee$ to represent the boolean OR operator. The boolean NOT operator is written as $\lnot$. 
\paragraph{The Boolean Constraint System}
If boolean variables appear as part of a statement, a constraint is required to actually enforces the variable to be either $1$ or $0$. In fact many of the following constraints for boolean functions, are only correct under the assumption that their input variables are boolean constraint. Not constraining boolean variables is a common issue in circuit design.

In order to constrain an arbitrary field element $x\in \F$ to be $1$ or $0$, the key observatio is that the equation $x \cdot (1-x) =0$ has only two solutions $0$ and $1$.
\begin{center}
\digraph[scale=0.4]{BOOLCONS}{
  forcelabels=true;
  center=true;
  splines=ortho;
  nodesep= 2.0;
  nCONS1 -> nCONS4 [xlabel="E_1"] ;
  nCONS1 -> nCONS6 [xlabel="E_1  "] ;
  nCONS2 -> nCONS5 ;
  nCONS3 -> nCONS4 ;
  nCONS4 -> nCONS5 ;
  nCONS5 -> nCONS6 ;
  nCONS6 -> nCONS7 [xlabel="E_2  "] ;
  nCONS1 [shape=box, label="x"] ;
  nCONS2 [shape=box, label="1"] ;
  nCONS3 [shape=box, label="-1"] ;
  nCONS4 [label="*"] ;
  nCONS5 [label="+"] ;
  nCONS6 [label="*"] ;
  nCONS7 [shape=box, label="0"] ;
}
\end{center}
To enfore a field element to be boolean constraint the following R1CS is therefore enough
\begin{equation}
W_1 \cdot (1-W_1) =0
\end{equation}
In designing more complex circuits from simple ones it is often conceptually as well as visually useful to collaps circuits into simple representative description. To do so, we write 
\begin{center}
\digraph[scale=0.6]{BOOLMETA}{
  forcelabels=true;
  center=true;
  splines=ortho;
  nodesep= 2.0;
  n1 [shape=box, label="BOOL"] ;
  n2 [shape=none, label="  "] ;
  n2 -> n1 ;
}
\end{center}
indicating that the boolean constraint circuit takes one input, has no outputs and constraints the input to be either $0$ or $1$.
\paragraph{The AND operator constraint system} Given two field elements $x$ and $y$ from $\F$ that are constrained to represent boolean variables, we want to find a circuit that computes the logical \textit{and} operator $AND(x,y)$ as well as its associated R1CS, that enforces $x$, $y$, $AND(x,y)$ to satisfy the constraint system if and only if $x\; \&\& \; y =AND(x,y)$ holds true. 

Assuming that three variables $x$, $y$ and $z$ are boolean constraint, the equation $x\cdot y = z$ is satisfied in $\F$ if and only if the equation $x\text{ AND }y = z$ is satisfied in boolean algebra. The logical operator AND is therefore implementable in $\F$ as multiplication of its arguments. 

The following circuit computes the AND operator in $\F$, assuming all inputs are restricted to be $0$ or $1$:
\begin{center}
\digraph[scale=0.4]{BOOLAND}{
  forcelabels=true;
  center=true;
  splines=ortho;
  nodesep= 2.0;
  nAND1 -> nAND3 [xlabel="E_1  "] ;
  nAND2 -> nAND3 [xlabel="E_2"] ;
  nAND3 -> nAND4 [xlabel="E_3  "] ;

  nAND1 [shape=box, label="x"] ;
  nAND2 [shape=box, label="y"] ;
  nAND3 [label="*"] ;
  nAND4 [shape=box, label="AnANDD(x,y)"] ;
}
\end{center}
The associated rank-1 constraint system can be deduced from the general process XXX and consists of the following constraint
\begin{equation}
 W_1 \cdot W_2 = W_3
\end{equation}
In designing more complex circuits from simple ones it is often conceptually as well as visually useful to collaps circuits into simple representative description. To do so, we write 
\begin{center}
\digraph[scale=0.6]{ANDMETA}{
  forcelabels=true;
  center=true;
  splines=ortho;
  nodesep= 2.0;
  n1 [shape=box, label="AND"] ;
  n2 [shape=none, label="  "] ;
  n3 [shape=none, label="  "] ;
  n4 [shape=none, label="  "] ;
  n2 -> n1 ;
  n3 -> n1 ;
  n1 -> n4 ;
}
\end{center}
indicating that the AND circuit takes two input, has one outputs and constraints the output to be the logical AND of the inputs, providing the inputs are boolean constraint.
\paragraph{The OR operator constraint system} Given two field elements $x$ and $y$ from $\F$ that are constrained to represent boolean variables, we want to find a circuit that computes the logical \textit{or} operator $OR(x,y)$ as well as its associated R1CS, that enforces $x$, $y$, $OR(x,y)$ to satisfy the constraint system if and only if $x\; || \; y =OR(x,y)$ holds true. 

Assuming that three variables $x$, $y$ and $z$ are boolean constraint, the equation $1-(1-x)\cdot(1-y) = z$ is satisfied in $\F$ if and only if the equation $x\text{ OR }y = z$ is satisfied in boolean algebra. The logical operator OR is therefore implementable in $\F$ by the following circuit, assuming all inputs are restricted to be $0$ or $1$:
\begin{center}
\digraph[scale=0.4]{BOOLOR}{
  forcelabels=true;
  center=true;
  splines=ortho;
  nodesep= 2.0;
   nOR1 -> nOR5 [xlabel="E_1  "] ;
  nOR2 -> nOR7 [xlabel="E_2  "] ;
  nOR3 -> {nOR5, nOR7, nOR10} ;
  nOR4 -> {nOR6, nOR8, nOR11} ;
  nOR5 -> nOR6; 
  nOR6 -> nOR9 ;
  nOR7 -> nOR8 ;
  nOR8 -> nOR9 ;
  nOR9 -> nOR10 [xlabel="E_3  "] ;
  nOR10 -> nOR11 ;
  nOR11 -> nOR12 [xlabel="E_4  "] ;

  nOR1 [shape=box, label="x"] ;
  nOR2 [shape=box, label="y"] ;
  nOR3 [shape=box, label="-1"] ;
  nOR4 [shape=box, label="1"] ;
  nOR5 [label="*"] ;
  nOR6 [label="+"] ;
  nOR7 [label="*"] ;
  nOR8 [label="+"] ;
  nOR9 [label="*"] ;
  nOR10 [label="*"] ;
  nOR11 [label="+"] ;
  nOR12 [shape=box, label="OR(x,y)"] ;
}
\end{center}
The associated rank-1 constraint system can be deduced from the general process XXX and consists of the following constrainst
\begin{align*}
 (1- W_1) \cdot (1-W_2) & = W_3\\
  (1-W_3)\cdot 1 &= W_4
\end{align*}
In designing more complex circuits from simple ones it is often conceptually as well as visually useful to collaps circuits into simple representative description. To do so, we write 
\begin{center}
\digraph[scale=0.6]{ORMETA}{
  forcelabels=true;
  center=true;
  splines=ortho;
  nodesep= 2.0;
  n1 [shape=box, label="OR"] ;
  n2 [shape=none, label="  "] ;
  n3 [shape=none, label="  "] ;
  n4 [shape=none, label="  "] ;
  n2 -> n1 ;
  n3 -> n1 ;
  n1 -> n4 ;
}
\end{center}
indicating that the OR circuit takes two input, has one outputs and constraints the output to be the logical OR of the inputs, providing the inputs are boolean constraint.
\begin{exercise} Let $\F$ be a finite field and let $b_1$ as well as $b_2$ two boolean constraint variables from $\F$. Show that the equation 
$OR(b_1,b_2) = b_1 + b_2 - b_1\cdot b_2$ holds true.

Use this equation to derive an algebraic circuit with ingoing variables $b_1$ and $b_2$ and outgoing variable $OR(b_1,b_2)$, such that $b_1$ and $b_2$ are boolean constraint and the circuit has a valid assignment, if and only if $OR(b_1,b_2) = b_1 \vee b_2$.  

Use the technique from XXX to transform this circuit into a rank-1 constraint system and find its full solution set. 
\end{exercise}
\paragraph{The NOT operator constraint system} Given a field element $x$ from $\F$ that is constrained to represent a boolean variable, we want to find a circuit that computes the logical \textit{NOT} operator $NOT(x)$ as well as its associated R1CS, that enforces $x$, $NOT(x)$ to satisfy the constraint system if and only if $\lnot x = NOT(x)$ holds true. 

Assuming that two variables $x$ and $y$ are boolean constraint, the equation $(1-x) = y$ is satisfied in $\F$ if and only if the equation $\lnot x = y$ is satisfied in boolean algebra. The logical operator NOT is therefore implementable in $\F$ by the following circuit, assuming all inputs are restricted to be $0$ or $1$:
\begin{center}
\digraph[scale=0.4]{BOOLNOT}{
  forcelabels=true;
  center=true;
  splines=ortho;
  nodesep= 2.0;
  nNOT1 -> nNOT4 [xlabel="E_1  "] ;
  nNOT2 -> nNOT4 ;
  nNOT3 -> nNOT5 ;
  nNOT4 -> nNOT5 ;
  nNOT5 -> nNOT6 [xlabel="E_2  "] ;

  nNOT1 [shape=box, label="x"] ;
  nNOT2 [shape=box, label="-1"] ;
  nNOT3 [shape=box, label="1"] ;
  nNOT4 [label="*"] ;
  nNOT5 [label="+"] ;
  nNOT6 [shape=box, label="nNOT(x)"] ;
}
\end{center}
The associated rank-1 constraint system can be deduced from the general process XXX and consists of the following constrainst
\begin{align*}
  (1-W_1)\cdot 1 &= W_2
\end{align*}
In designing more complex circuits from simple ones it is often conceptually as well as visually useful to collaps circuits into simple representative description. To do so, we write 
\begin{center}
\digraph[scale=0.6]{NOTMETA}{
  forcelabels=true;
  center=true;
  splines=ortho;
  nodesep= 2.0;
  n1 [shape=box, label="NOT"] ;
  n2 [shape=none, label="  "] ;
  n4 [shape=none, label="  "] ;
  n2 -> n1 ;
  n1 -> n4 ;
}
\end{center}
indicating that the NOT circuit takes one input, has one outputs and constraints the output to be the logical NOT of the input, providing the input is boolean constraint.
\begin{exercise}
Let $\F$ be a finite field. Derive the algebraic circuit and associated rank-1 constraint system for the following operators: OR, XOR, NAND, EQU.
\end{exercise}
\paragraph{Modularity} Implementing bounded computation in algebraic circuits it is often necessary to deal with complex boolean expressions. As we have seen in XXX and XXX, both algebraic circuits and R1CS have a modularity property, which enables a compiler to derive algebraic circuit implementations for arbitrary boolean circuits. 

This is quite remarkable property, because it shows that the expressiveness of algebraic circuits and therefore rank-1 constraint systems is as general as the expressiveness of boolen circuits.  
\begin{example} To give an intuitive example of how a very simple compiler might construct complex boolean circuit representations in algebraic circuits and how to derive associated rank-1 constraint systems, lets look at the following VERILOG like pseudo code:
\begin{lstlisting}
module boolean_circuit (
	input (public) b_1 : BOOL ; 
	input (public) b_2 : BOOL ;
	input (public) b_3 : BOOL ; 
	input (public) b_4 : BOOL ; 
	output (public) b_5 : BOOL ; 

	begin
		b_5 <= (b_1 or b_2) and (b_3 and not b_4 ) ;
	end ;
)
\end{lstlisting}
The code describes a circuit, that takes four public inputs $b_1$, $b_2$, $b_3$ and $b_4$ of boolean type and computes a public output $b_5$, such that the following boolean expression holds true:
$$
\left( b_1 \vee b_2 \right) \wedge (b_3 \wedge \lnot b_4) = b_5
$$
In order to understand a possible way to transform this hardware description language style expression into a circuit, we first rewrite the actual computation of the boolean expression into operator notation. We get
$$
b_5 \leftarrow AND(OR(b_1,b_2),AND(b_3,NOT(b_4))
$$
Using the boolean operator notion, makes it conceptually more clear to derive the circuit. To see how the circuit is computed we start at the outer most operator and  write down its defining circuit as well as it associated R1CS using placeholder names for its arguments. We then inductively substitute every placeholder by its defining circuit and add the associated constraint, to the constrain system. We get
\begin{center}
\digraph[scale=0.4]{BOOLCOMPLEX}{
  forcelabels=true;
  center=true;
  splines=ortho;
  nodesep= 2.0;

  subgraph clusterORb1b2 {
    nOR1 -> nOR5 [xlabel="E_1  "] ;
    nOR2 -> nOR7 [xlabel="E_2  "] ;
    nOR3 -> {nOR5, nOR7, nOR10} ;
    nOR4 -> {nOR6, nOR8, nOR11} ;
    nOR5 -> nOR6; 
    nOR6 -> nOR9 ;
    nOR7 -> nOR8 ;
    nOR8 -> nOR9 ;
    nOR9 -> nOR10 [xlabel="E_3  "] ;
    nOR10 -> nOR11 ;
    nOR11 -> nOR12 [xlabel="E_4  "] ;
    nOR1 [shape=box, label="b_1"] ;
    nOR2 [shape=box, label="b_2"] ;
    nOR3 [shape=box, label="-1"] ;
    nOR4 [shape=box, label="1"] ;
    nOR5 [label="*"] ;
    nOR6 [label="+"] ;
    nOR7 [label="*"] ;
    nOR8 [label="+"] ;
    nOR9 [label="*"] ;
    nOR10 [label="*"] ;
    nOR11 [label="+"] ;
    nOR12 [shape=box, label="OR( b1 , b2 )", color=lightgray] ;
    label = "OR( b1 , b2 )";
    color=lightgray;
    label="Circuit_2 -- OR-Circuit"
  }

  subgraph clusterANDb3NOTb4 {
    nAND21 -> nAND23 [xlabel="E_1  "] ;
    nAND22 -> nAND23 [xlabel="E_2"] ;
    nAND23 -> nAND24 [xlabel="E_3  "] ;

    nAND21 [shape=box, label="b3"] ;
    nAND22 [shape=box, label="NOT( b4 )", color=lightgray ] ;
    nAND23 [label="*"] ;
    nAND24 [shape=box, label="AND( b3 , NOT( b4 ) )", color=lightgray] ;
    color=lightgray;
    label="Circuit_3 -- AND-Circuit"
  }

  subgraph clusterNOTb4 {
    nNOT1 -> nNOT4 [xlabel="E_1  "] ;
    nNOT2 -> nNOT4 ;
    nNOT3 -> nNOT5 ;
    nNOT4 -> nNOT5 ;
    nNOT5 -> nNOT6 [xlabel="E_2  "] ;

    nNOT1 [shape=box, label="b4"] ;
    nNOT2 [shape=box, label="-1"] ;
    nNOT3 [shape=box, label="1"] ;
    nNOT4 [label="*"] ;
    nNOT5 [label="+"] ;
    nNOT6 [shape=box, label="NOT( b4 )", color=lightgray ] ;
    color=lightgray;
    label="Circuit_4 -- NOT-Circuit"
  }

  subgraph clusterAND1 {
    nAND1_1 -> nAND1_3 [headlabel="E_1    ."] ;
    nAND1_2 -> nAND1_3 [xlabel="E_2  "] ;
    nAND1_3 -> nAND1_4 [xlabel="E_3  "] ;
    nAND1_1 [shape=box, label="OR( b1 , b2 )", color=lightgray ] ;
    nAND1_2 [shape=box, label="AND( b3 , NOT( b4 ) )", color=lightgray] ;
    nAND1_3 [label="*"] ;
    nAND1_4 [shape=box, label="AND( OR( b1 , b2 ) , AND( b3 , NOT( b4 ) )"] ;
    color=lightgray;
    label="Circuit_1 -- AND-Circuit"
  }

  // outer circuit
    nNOT6 -> nAND22 [style=dashed, color=grey] ;
    nOR12 -> nAND1_1 [style=dashed, color=grey] ;
    nAND24 -> nAND1_2 [style=dashed, color=grey] ;
}
\end{center}
For better readability, this circuit does not boolean constraint the four input variables $b_1$, $b_2$, $b_3$ and $b_4$. After adding the boolean constraints, relabeling the edges and optimizing the constants, using graphviz, the following circuit represents the boolean expression:
\begin{center}
\digraph[scale=0.5]{BOOLCOMPLEXOPTI}{
  forcelabels=true;
  center=true;
  splines=ortho;

  one -> nCONSb15 ;
  minusone -> nCONSb14 ;
  nCONSb14 -> nCONSb15 ;
  nCONSb15 -> nCONSb16 ;
  nCONSb16 -> zero [taillabel="  W_1"] ;
  nCONSb14 [label="*"] ;
  nCONSb15 [label="+"] ;
  nCONSb16 [label="*"] ;

  one -> nCONSb25 ;
  minusone -> nCONSb24 ;
  nCONSb24 -> nCONSb25 ;
  nCONSb25 -> nCONSb26 ;
  nCONSb26 -> zero [taillabel="W_2 "] ;
  nCONSb24 [label="*"] ;
  nCONSb25 [label="+"] ;
  nCONSb26 [label="*"] ;

  one -> nCONSb35 ;
  minusone -> nCONSb34 ;
  nCONSb34 -> nCONSb35 ;
  nCONSb35 -> nCONSb36 ;
  nCONSb36 -> zero [xlabel="W_3"] ;
  nCONSb34 [label="*"] ;
  nCONSb35 [label="+"] ;
  nCONSb36 [label="*"] ;

  one -> nCONSb45 ;
  minusone -> nCONSb44 ;
  nCONSb44 -> nCONSb45 ;
  nCONSb45 -> nCONSb46 ;
  nCONSb46 -> zero [xlabel="W_4"] ;
  nCONSb44 [label="*"] ;
  nCONSb45 [label="+"] ;
  nCONSb46 [label="*"] ;

  minusone -> {nOR5, nOR7, nOR10} ;
  one -> {nOR6, nOR8, nOR11} ;
  nOR5 -> nOR6; 
  nOR6 -> nOR9 ;
  nOR7 -> nOR8 ;
  nOR8 -> nOR9 ;
  nOR9 -> nOR10 [headlabel="W_5  "] ;
  nOR10 -> nOR11 ;
  nOR5 [label="*"] ;
  nOR6 [label="+"] ;
  nOR7 [label="*"] ;
  nOR8 [label="+"] ;
  nOR9 [label="*"] ;
  nOR10 [label="*"] ;
  nOR11 [label="+"] ;

  nAND23 [label="*"] ;
  minusone -> nNOT4 ;
  one -> nNOT5 ;
  nNOT4 -> nNOT5 ;
  nNOT4 [label="*"] ;
  nNOT5 [label="+"] ;

  nOR11 -> nAND1_3;
  nAND23 -> nAND1_3 [xlabel="W_6  "] ;
  nAND1_3 -> nAND1_4 [xlabel="I_5  "] ;
  nAND1_3 [label="*"] ;
  nAND1_4 [shape=box, label="AND( OR( b1 , b2 ) , AND( b3 , NOT( b4 ) )"] ;

  // outer circuit
    nNOT5 -> nAND23 ;
    b1 -> {nOR5, nCONSb14, nCONSb16} [taillabel="I1 "] ;
    b2 -> {nOR7, nCONSb24, nCONSb26} [taillabel=" I2"] ;
    b3 -> {nAND23, nCONSb34, nCONSb36} [taillabel="I3 "] ;
    b4 -> {nNOT4, nCONSb44, nCONSb46} [taillabel="I4"] ;
    b1 [shape=box, label="b1"] ;
    b2 [shape=box, label="b2"] ;
    b3 [shape=box, label="b3"] ;
    b4 [shape=box, label="b4"] ;
    minusone [shape=box, label="-1"] ;
    one [shape=box, label="1"] ;
    zero [shape=box, label="0"] ;
}
\end{center}
Valid assignments to this circuits consists of public inputs $I_1$, $I_2$, $I_3$, $I_4$ and $I_5$ from $\F_{13}$, such that the equation $I_5 = \left( I_1 \vee I_2 \right) \wedge (I_3 \wedge \lnot I_4)$ has to hold true. In addition a valid assignment also has to contain private inputs $W_1$, $W_2$, $W_3$, $W_4$, $W_4$ and $W_6$, which can be derived from circuit execution. The inputs $W_1$, $\ldots$, $W_4$ ensure that the first four public inputs are either $0$ or $1$ but not any other field element and the others enforce the boolean expression.  

To compute the associated R1CS we can use the general method from XXX and look at every labeled outgoing edge not coming from a source node. Declaring the edges coming from input nodes as well as the edge going to the single output node as public and every other edge as private input. In this case we get:
\begin{align*}
W_1:\;\; & I_1 \cdot (1- I_1) = 0  & \text{boolean constraints}\\
W_2:\;\; & I_2 \cdot (1- I_2) = 0 \\
W_3:\;\; & I_3 \cdot (1- I_3) = 0 \\
W_4:\;\; & I_4 \cdot (1- I_4) = 0 \\
W_5:\;\; & (1- I_1)\cdot (1-I_2) = W_5 & \text{OR-operator constraint}\\
W_6:\;\; & I_3 \cdot (1-I_4) = W_6 & \text{AND(.,NOT(.))-operator constraints}\\
I_5:\;\; & (1-W_5) \cdot W_6 = I_5 & \text{AND-operator constraints}\\
\end{align*}
The reason why this R1CS only contains a single contraint for the OR-operator, while the general definition XXX requires two, is that the second constraint in XXX only appears since the final addition gate is connected to a sink node. In this example however the addition gate is sub-circuit and internal addition gates do not lead to new constraints. The same holds true for the negation circuit. 
\end{example}
\subsubsection{The Unsigned Integer Type} In computer science, an unsigned integer of size $N$, where $N$ is usually a power of two, is an atomic type that represents counting numbers in the range $0\ldots 2^N-1$ together with addition, subtraction and multiplication laws that are somewhat similar to the (semi) ring laws of natural numbers except for overflow and underflow effects. The associated type is usually written as $uN$ or $uIntN$.

On compuer hardware elements of the unsigned integer type $uIntN$ are commonly represented as $N$-tuples of bits, that is if $x : uIntN$ is of $uIntN$ type it is represented as
$$
x = (b_0,b_1,\ldots, b_{N-1})
$$
For suteable $N$ like $N=32$ or $N=64$, addition, subtraction and multiplication is realized in hardware by appropriate digital circuits like the binary adder oder the binary multiplier. 

To understand how unsigned integer types can be represented as algebraic circuits, basically two different approaches can be taken.

To understand the first approach, recall that addition and multiplication in a prime field $\F_p$ is equal to addition and multiplication of integers, as long as the sum or the product does not exceed the modulus $p$. It is therefore possible to represent the $uIntN$ type inside the basefield type, whenever $N$ is small enough. However care has to be taken to never overflow the modulus. It is also important to make sure that in subtraction the subtrahend is never larger then the minuent.

An advantage of this approach is that it is very efficient to represent elements of the uIntN type in this way, as they can be storred in a single element of the base field type. The diadvantage is that care must be taken to constrain the elements and to enforce that no overflow or underflow situations occure.

The second approach in conceptually cleaner but requires more space and constraints for addition and multiplication. Much like machines represents uInt's as binary tuples, this approach represents elements of uIntN types as $N$-typles $(b_0,b_1,\ldots, b_{N-1})$ of elements from the base field $\F$, such that each $b_j$ itself is of boolean type. All operations, like addition, multiplication, bit-shifts and so on, are then realized by addoptations of the digital circuits that implement these operations in hardware.

An advantage of this representation is that the number $N$ is independend of the modulus of the underlying prime field and the representation moreover works over arbitrary fields. It can therefore abstract over the field.


In what follows we will describe the second approach in more detail.


\paragraph{The uIntN Constraint System} In the approach we are taking in this section, elements of uIntN type are represented by $N$-tuples of field elements that are themself binary constraint. Declaring an element of uIntN type therefore means to declare $N$ elements of boolean type. We write this as 
\begin{center}
\digraph[scale=0.6]{UINTN}{
  forcelabels=true;
  center=true;
  splines=ortho;
  //nodesep= 2.0;
  n1 [shape=box, label="UINT_N"] ;
  n2 [shape=none, label="  "] ;
  n3 [shape=none, label="  "] ;
  n4 [shape=none, label="  "] ;
  n5 [shape=none, label="  "] ;
  n2 -> n1 ;
  n3 -> n1 ;
  n4 -> n1 [style=dashed, color=lightgrey] ;
  n5 -> n1 ;
}
\end{center}
To enfore an $N$-tuple of field elements $(b_0,\ldots,b_{N_1})$ to represent an element of UintN type we therefore need $N$ constraints 
\begin{align*}
E_0 \cdot (1-E_0) & = 0\\
E_1 \cdot (1-E_1) & = 0\\
\cdots &\\
E_{N-1} \cdot (1-E_{N-1}) & = 0\\
\end{align*}
\begin{example}
Consider the Uint4 type over the prime field $\F_{17}$. Since $2^4=16$, Uint4 can represent the numbers $0,\ldots, 15$ and it would be possible to interpret them as elements in $\F_{17}$. However addition 
\end{example} 
\paragraph{UintN Addition} Since we representat the unsigned integer type as an $N$-tuple of field elements that are boolean constraint, we can define addition in the same way as hardeare does. The way this is usually done is by first defining the \textit{full adder} circuit and then combining $N$ of this these circuits into a circuit that add to elements from the UintN type.

To understand the algebraic circuit for the $1$-bit full, recall that we already defined circuits for boolean algebra in the previous section. Abstracting over those circuits, a full adder circuit can then be defined as:
\begin{center}
\digraph[scale=0.4]{ONEBFULLADD}{
  forcelabels=true;
  center=true;
  splines=ortho;
  nodesep= 2.0;
  
  subgraph clusterin {
    nADD01 [shape=box, label="bx_j"] ;
    nADD02 [shape=box, label="by_j"] ;
    nADD03 [shape=box, label="c_(j-1)"] ;
    color = white ;
  }
  
  subgraph clustermid {
    nADD04 [shape=box, label="XOR"] ;
    nADD05 [shape=box, label="XOR"] ;
    nADD06 [shape=box, label="AND"] ;
    nADD07 [shape=box, label="AND"] ;
    nADD08 [shape=box, label="OR"] ;
    
    nADD04 -> {nADD05, nADD06} ;
    nADD06 -> nADD08 ;
    nADD07 -> nADD08 ;
    
    color = white ;
  }
  
  subgraph clusterout {
    nADD09 [shape=box, label="bz_j"] ;
    nADD010 [shape=box, label="c_j"] ;
    color = white ;
  }
  
  nADD01 -> {nADD04, nADD07} ;
  nADD02 -> {nADD04, nADD07} ;
  nADD03 -> {nADD05, nADD06} ;
  nADD05 -> nADD09 ;
  nADD08 -> nADD010 ; 
}
\end{center}
In this circuit the output $bz_j$ is the result of the binary input $bx_j$ and $by_j$, where $bx_j$ is the $j$-th bit of the binary representation of the first summand and $by_j$ is the $j$-th bit of the binary representation of the second summand. The output $c_j$ is the carry bit of the addition and the input $c_{j-1}$ is is the carry bit which is supposed to be either $0$ for $j=0$ or the carry bit output of the previous full adder circiut. Abstacting the $1$-bit adder, we write:
\begin{center}
\digraph[scale=0.6]{BADDMETA}{
  forcelabels=true;
  center=true;
  splines=ortho;
  //nodesep= 2.0;
  n1 [shape=box, label="FULLADD"] ;
  n2 [shape=none, label="bx_j"] ;
  n3 [shape=none, label="by_j"] ;
  n4 [shape=none, label="c_(j-1)"] ;
  n5 [shape=none, label="bz_j"] ;
  n6 [shape=none, label="c_j"] ;
  n2 -> n1 ;
  n3 -> n1 ;
  n4 -> n1 ;
  n1 -> {n5, n6} ;
}
\end{center}
With a circuit definition of the $1$-bit full adder at hand, addition of two uIntN type elements can then be defined as
\begin{center}
\digraph[scale=0.4]{UINTADD}{
  forcelabels=true;
  center=true;
  splines=ortho;
  //nodesep= 2.0;
  
  subgraph clusterin0 {
    nADD01 [shape=box, label="FULLADD"] ;
    nADD02 [shape=none, label="bx_0"] ;
    nADD03 [shape=none, label="by_0"] ;
    nADD05 [shape=none, label="bz_0"] ;
    nADD02 -> nADD01 ;
    nADD03 -> nADD01 ;
    nADD01 -> nADD05 ;
    color = white ;
  }
  
  subgraph clusterin1 {
    nADD11 [shape=box, label="FULLADD"] ;
    nADD12 [shape=none, label="bx_1"] ;
    nADD13 [shape=none, label="by_1"] ;
    //nADD14 [shape=none, label="c_0"] ;
    nADD15 [shape=none, label="bz_1"] ;
    nADD12 -> nADD11 ;
    nADD13 -> nADD11 ;
    //nADD14 -> nADD11 ;
    nADD11 -> nADD15 ;
    color = white ;
  }

  subgraph clusterin2 {
    nADD21 [shape=box, label="FULLADD", color=lightgray] ;
    nADD22 [shape=none, label="bx_j", color=lightgray] ;
    nADD23 [shape=none, label="by_j", color=lightgray] ;
    //nADD24 [shape=none, label="c_(j-1)", color=lightgray] ;
    nADD25 [shape=none, label="bz_j", color=lightgray] ;
    nADD22 -> nADD21 [color=lightgray];
    nADD23 -> nADD21 [color=lightgray];
    //nADD24 -> nADD21 ;
    nADD21 -> nADD25 [color=lightgray] ;
    color = white ;
  }
  
  subgraph clusterinN {
    nADDN1 [shape=box, label="FULLADD"] ;
    nADDN2 [shape=none, label="bx_(N-1)"] ;
    nADDN3 [shape=none, label="by_(N-1)"] ;
    //nADDN4 [shape=none, label="c_(N-2)"] ;
    nADDN5 [shape=none, label="bz_(N-1)"] ;
    nADDN2 -> nADDN1 ;
    nADDN3 -> nADDN1 ;
    //nADDN4 -> nADDN1 ;
    nADDN1 -> nADDN5
    color = white ;
  }
  
  nADD04 [shape=none, label="0"] ;
  nADD04 -> nADD01 ;
  nADD01 -> nADD11 ;
  nADD11 -> nADD21 [style=dashed, color=lightgrey] ;
  nADD21 -> nADDN1  [style=dashed, color=lightgrey] ;
  nADDN6 [shape=none, label="c_out"] ;
  nADDN1 -> nADDN6 ;
  
}
\end{center}
Depending on how the output carry bit is handled we get different definition of addition in this type. One way would be to enforce it to be zero. This way addition in the circuit is only possible if the sum does not exceed $2^N-1$. On the other hand if the carry bit is unconstraint, then the resulting addition is equivalent to modulo $2^N$ arithmetics. Good  compilers should therefore always describe explicitly how exactly their implementation of the uintN type behaves, such that users don't build their system on false assumptions. 

The associated constraint system consists of XXX constraints, including the boolean constraints of the representing bits
\paragraph{The Boolean Operators} In implementations it is often necesarry to execute boolean operations like $ans$, $or$, or $xor$ on elements of the uInt type. Fortunately this easily done by simply applying those operatons to every bit seperately as shown in XXX.  
\begin{exercise}
Let $k$ be a counting number with $k<N$. Define circuits and associated R1CS for the left and righr bishift operators $x<<k$ as well as $x>>k$ for the uint type. 
\end{exercise}
\begin{exercise}
Define the multiplication circuits for the uintN type.
\end{exercise}
\begin{exercise} Let $N=4$ be fixed and consider the finite field $\F_{13}$ from example XXX. The following pseudo code describes a high level circuit description in a VERILOG like style. Transform the pseudo code into a circuit and then derive the associated R1CS. 
\begin{lstlisting}
module mask_merge(N) (
	input (public) a : Uint_N ;
	input (public) b : Uint_N ;
	input (public) mask : Uint_N ;
	output (public) r : Uint_N ;

	begin
		r == a xor ((a xor b) & mask) ;
	end ;
)
\end{lstlisting}
Let $L_{mask\_merge}$ be the language defined by the R1CS of the circuit. Provide a knowledge proof in $L_{mask\_merge}$ for the instance $I=(I_a, I_b, I_{mask}, I_r) = (14, 5, 10, 4)$. Also show that there is no knowledge proof in $L_{mask\_merge}$ for the instance $(11, 6, 10, 7)$.
\end{exercise}
\subsection{Control Flow}
\subsubsection{The Conditional Assignment} Implementing complex control flow in circuits, it is often necessary to have a way for conditional assignment of values or computational output to variables.

One way to realize this in more common programming languages is by the conditional ternary operator $?:$, that branches the control flow of a program according to some condition and then assigns the output of the computational branch to some variable. A common way to write this is as
\begin{lstlisting}
	variable = condition ? value_if_true : value_if_false  
\end{lstlisting}
where \textsc{condition} is a boolean expression and \textsc{value\_if\_true} as well as \textsc{value\_if\_false} are expressions that evaluate to the same type as \textsc{variable}.

In programming languages like Rust another way to write the conditional assignment operator that is more familiar to many programmers is given by 
\begin{lstlisting}
	variable = if condition { value_if_true } else { value_if_false } 
\end{lstlisting}
One particular property of this operator is that the expression \textsc{value\_if\_true} is only evaluated if \textsc{condition} evaluates to true and the expression \textsc{value\_if\_false} is only evaluated if \textsc{condition} evaluates to false. In fact computer programs would soon become very inefficient if the operator would evaluate both expressions regardless of the value of \textsc{condition}.

If drop the requirement that only one branch of the conditional operator is executed, we can implement it in a simple way as a circuit. To see that observe that if $b$, $c$ and $d$ are values from a finite field, such that $b$ is boolean constraint (XXX), we can use the following equation to enforce a field element $x$ to be the result of the conditional assignment operator: 
\begin{equation}
x = b\cdot c + (1-b)\cdot d
\end{equation}
Flattening this equation into an algebraic circuit gives
\begin{center}
\digraph[scale=0.4]{CONDASSIGN}{
	forcelabels=true;
	center=true;
	splines=ortho;
	nodesep= 2.0;

  n1 [shape=box, label="b"]
  n2 [shape=box, label="c"]
  n3 [shape=box, label="d"]
  n4 [shape=box, label="b ? c : d"]
  n5 [shape=box, label="-1"]
  n6 [shape=box, label="1"]
  n7 [shape=box, label="*"]
  n8 [shape=box, label="*"]
  n9 [shape=box, label="*"]
  n10 [shape=box, label="+"]
  n11 [shape=box, label="+"]
 
  n1 -> n7 [taillabel= "E_1  "] ;
  n1 -> n8 [taillabel= "E_1 "] ;
  n2 -> n7 [xlabel= "E_2"] ;
  n3 -> n9 [xlabel= "E_4"] ;
  n5 -> n8 ;
  n6 -> n10 ;
  n7 -> n11 [xlabel= "E_3  "] ;
  n8 -> n10 ;
  n9 -> n11 [xlabel= "E_5"] ;
  n10 -> n9 ;
  n11 -> n4 [xlabel= "E_6  "] ;
}
\end{center}
Note that in order to compute a valid assignment to this circuit, both values for $W_?$ and $W_?$ are necessary. If the inputs to thoses edges are circuits themself, both circuits needs valid assignments. As a consequence this implementation of the conditional assigment opperator has to execute alll branches of all circuits, which is very different from the execution of common computer programs. 

Starting at this circuit we can use the general tenchnique from XXX to derive its associated rank-1 constraint system. We get
\begin{align*}
E_1 \cdot E_2 & = E_3 \\
(1 - E_1) \cdot E_4 & = E_5 \\
(E_3 + E_5)\cdot 1 &= E_6
\end{align*}
\begin{example} Let $N=4$ be fixed.
\begin{lstlisting}
module conditional_bit_set(N) (
	input (public) c : BOOL ;
	input (public) mask : Uint_N ;
	input (public) w : Uint_N ;
	output (public) r : Uint_N ;

	begin
		r == if c { w or mask } else { w and not mask } ;
	end ;
)
\end{lstlisting}
\end{example}

% NOTE: ZK-Podcast with Alex zdemir for the proper branching thing in version 2 of the book.

\subsubsection{Loops} Circuits and R1CS are not general enough to express arbitrary computations, but bounded computations only. As a consequence it is not possible to represent unbounded loops like $while TRUE do {}$ in algebraic circuits or rank-1 constraints systems. This can be easily seen since circuits are acyclic graphs and hence unbounded loops would require circuits of unbounded sizes. However bounded loops are expressible, simply by enrolling the loop. 

\begin{example}
\begin{lstlisting}
module counting_bits(N) (
for (c = 0; v; v >>= 1)
{
  c += v & 1;
}

	begin
		r == a xor ((a xor b) & mask) ;
	end ;
)
\end{lstlisting}
\end{example}

\subsection{Gadgets}
\subsubsection{Binary representations}
If the underlying field has a modulus $p$, such that $2^N-1 < p$, then there is a standard way to transform field elements $x\in \F_p$ of size $x<2^N$ into a UIntN bit representation and vice versa.

To make the UintN type more human readable, compilers might introduce some synthactic suggar and outside of the circuit converging back and forth between the base $2$ and base $10$ representation of the UintN type. A standard way to do it is as follows: 

Consider a base $10$ representation $x$ of a UintN type. Then its binary representation 
$(b_0,\ldots,b_{N-1})$ can be computed by 
\begin{lstlisting}
input x : UINT_N ; 
output b[N] : BOOL ; 
var lc1=0;
var e2=1;
for (var i = 0; i < N; i++) {
    b[i] <-- (in >> i) & 1;
    lc1 += b[i] * e2;
    e2 = e2+e2;
}
\end{lstlisting}
This computation is of course done outside of the circuit as a high level inteface for human friendly input. On the other hand if the internal representation $(b_1,\ldots, b_{N-1})$ is given, then the human readable base $10$ representation is given by:
\begin{lstlisting}
input b[N] : BOOL ; 
output x : UINT_N ; 
var lc1=0;
var e2=1;
for (var i = 0; i < N; i++) {
    b[i] <-- (in >> i) & 1;
    lc1 += b[i] * e2;
    e2 = e2+e2;
}
\end{lstlisting}



In computations like scalar multiplication of elliptic curve points its is often necessary to use a binary representation of elements from the base field type. It is therefore necesaary to have a way to transform field elements into their binary representation and vice versa in circuits.

To derive such a circuit over a prime field $\F_p$, let $m=|p_{base_2}|$ be the smallest number of bits necessary to represent the prime modulus $p$ itself. Then a bitstring $(b_0,\ldots,b_{m-1})\in \{0,1\}^m$ is a binary representation of a field element $x\in\F_p$, if and only if
$$
x = b_0\cdot 2^0 + b_1\cdot 2^1 + \ldots + b_m\cdot 2^{m-1}
$$ 
In this expression, addition and exponentiation is considered to be executed in $\F_p$, which is well defined, since all terms $2^j$ for $0\leq j \leq m$ are elements of $\F_p$. Note however that in contrast to the binary representation of counting numbers $n\in\N$, this representation is not unique in prime fields for odd prime numbers. 
\begin{example} Considering the prime field $\F_{13}$. To compute binary representations of elements from that field, we start with the binary representation of prime modulus $13$, which is $13_{base_2} = (1,0,1,1)$ since 
$13= 1\cdot 2^0 + 0\cdot 2^1 + 1\cdot 2^2 + 1\cdot 2^3$. So $m=4$ and we need up to $4$ bits to represent any element $x\in\F_{13}$.

To see that binary representations are not unique in general, consider the element $2\in \F_{13}$. It has the binary representations $2_{base_2}=(0,1,0,0)$ as well as $2_{base_2}=(1,1,1,1)$, since in $\F_{13}$ we have
$$
2 = \begin{cases}
0\cdot 2^0 + 1\cdot 2^1 + 0\cdot 2^2 + 0\cdot 2^3\\
1\cdot 2^0 + 1\cdot 2^1 + 1\cdot 2^2 + 1\cdot 2^3
\end{cases}
$$
\end{example}
Considering that the underlying prime field is fixed and the most significant bit of the prime modulus is $m$, the following circuit flattens equation XXX, assuming all inputs $b_1$, $\ldots$, $b_m$ are restricted to be either $0$ or $1$:
\begin{center}
\digraph[scale=0.3]{BINARYREP}{
	forcelabels=true;
	center=true;
	splines=ortho;
	nodesep= 2.0;

  subgraph cluster0 {
    n1 [shape=box, label="b_0"] ;
    n2 [shape=box, label="2^0"] ;
    n3 [label="*"] ;

    n1 -> n3 ;
    n2 -> n3  [xlabel="I_0"] ;
    color=white ;
  }

  subgraph cluster1 {
    n4 [shape=box, label="b_1"] ;
    n5 [shape=box, label="2^1"] ;
    n6 [label="*"] ;

    n4 -> n6 ;
    n5 -> n6  [xlabel="I_1  "] ;
    color=white ;
  }

  subgraph cluster2 {
    n7 [shape=box, label="b_2"] ;
    n8 [shape=box, label="2^2"] ;
    n9 [label="*"] ;

    n7 -> n9 ;
    n8 -> n9 [xlabel="I_2  "];
    color=white ;
  }

  subgraph cluster3 {
    n10 [shape=box, label="...", color=lightgrey] ;
    color=white ;
  }

  subgraph cluster4 {
    n11 [shape=box, label="b_(m-1)"] ;
    n12 [shape=box, label="2^(m-1)"] ;
    n13 [label="*"] ;

    n11 -> n13 ;
    n12 -> n13  [xlabel="I_(m-1)  "] ;
    color=white ;
  }

  subgraph cluster5 {
    n18 [shape=box, label="x"] ;
    n19 [shape=box, label="-1"] ;
    n20 [label="*"] ;

    n18 -> n20  [xlabel="I_m"] ;
    n19 -> n20 ;
    color=white ;
  }

  n14 [label="+"] ;
  n15 [label="+"] ;
  n16 [label="+", color=lightgrey] ;
  n17 [label="+"] ;
  n21 [label="+"] ;
  n22 [shape=0, label="0"] ;
  n3 -> n14 ;
  n6 -> n14 ; 
  n14 -> n15 ;
  n9 -> n15 ;
  n10 -> n16 [style=dashed, color=lightgrey] ;
  n15 -> n16 [style=dashed, color=lightgrey] ;
  n13 -> n17 ;
  n16 -> n17 [style=dashed, color=lightgrey] ;
  n20 -> n21 ;
  n17 -> n21 ;
  n21 -> n22  [xlabel="W_1=0  "] ;
}
\end{center}
Applying the general transformation rule into rank-1 constraint systems, we see that we actually only need a single constraint to enforce a binary representation of any field element. We get 
$$
(b_0\cdot 2^0 + b_1\cdot 2^1 + b_2\cdot 2^2 + \ldots + b_{m-1}\cdot 2^{m-1} -x)\cdot 1 = 0
$$
In designing more complex circuits from simple ones it is often conceptually as well as visually useful to collaps circuits into simple representative description. To do so, we write 
\begin{center}
\digraph[scale=0.6]{BINREPMETA}{
  forcelabels=true;
  center=true;
  splines=ortho;
  //<nodesep= 2.0;
  n1 [shape=box, label="BASE2"] ;
  n2 [shape=none, label="  "] ;
  n3 [shape=none, label="  "] ;
  n4 [shape=none, label="  "] ;
  n5 [shape=none, label="  "] ;
  n6 [shape=none, label="x"] ;
  n2 -> n1 ;
  n3 -> n1 ;
  n4 -> n1 [style=dashed];
  n5 -> n1 ;
  n6 -> n1 ;
}
\end{center}
indicating that the BASE2 circuit takes $m$ input, has no output and constraints the $x$ input to be the BLABLABLA
\begin{example} Considering the prime field $\F_{13}$, we want to enforce the binary representation of $7\in \F_{13}$. We know $m=4$ from example XX and we have to enforce a $4$-bit representation for $7$, which is $(1,1,1,0)$, since $7= 1\cdot 2^0 + 1\cdot 2^1 + 1\cdot 2^2 + 0\cdot 2^3$.

A valid circuit assignment is therefore given by $(I_0,I_1,I_2,I_3,I_4)=(1,1,1,0,7)$ and indeed we satify the required 5 constraints including the $4$ boolean constraints for $I_0$, $\ldots$, $I_3$ as 
\begin{align*}
1\cdot (1-1) &= 0 & \text{// boolean constraints}\\
1\cdot (1-1) &= 0 \\
1\cdot (1-1) &= 0 \\
0\cdot (1-0) &= 0  \\
(1 + 2 + 4 + 0 -7)\cdot 1 &= 0  & \text{// binary rep. constraint}
\end{align*}
\end{example}

\subsubsection{Range Proofs}
$x>5$...


\subsection{Cryptographic Primitives}
\subsubsection{Twisted Edwards curves}
Sometimes it required to do elliptic curve cryptography "inside of a circuit". This means that we have to implement the algebraic operations (addition, scalar multiplication) of an elliptic curve as a R1CS. To do this efficiently the curve that we want to implement must be defined over the same base field as the field that is used in the R1CS. 

% implmentations https://github.com/iden3/circomlib/blob/master/circuits/babyjub.circom

\begin{example}
So for example when we consider an R1CS over the field $\F_{13}$ as we did in example XXX, then we need a curve that is also defined over $\F_{13}$. Moreover it is advantegous to use a (twisted) Edwards curve inside a circuit, as the addition law contains no branching (See XXX). As we have seen in XXX our Baby-Jubjub curve is an Edwards curve defined over $\F_{13}$. So it is well suited for elliptic curve cryptography in our pend and paper examples
\end{example}

\paragraph{Twisted Edwards curves constraints} As we have seen in XXX, an Edwards curve over a finite field $F$ is the set of all pairs of points $(x,y)\in \F\times \F$, such that $x$ and $y$ satisfy the equation $a\cdot x^2+y^2= 1+d\cdot x^2y^2$. 

We can interpret this equation as a constraint on $x$ and $y$ and rewrite it as a R1CS by applying the flattenin technique from XXX.
$$
\begin{array}{lcr}
x \cdot x &=& x\_sq\\
y \cdot y &=& y\_sq\\
x\_sq \cdot y\_sq &=& xy\_sq\\
(a\cdot x\_sq+y\_sq)\cdot 1 &=& 1+d\cdot xy\_sq
\end{array}
$$
So we have the statement $w=(1,x,y,x\_sq, y\_sq, xy\_sq)$ and we need 4 constraints to enforce that $x$ and $y$ are points on the Edwards curve $x^2+y^2= 1+d\cdot x^2y^2$. Writing the constraint system in matrix form, we get:
\begingroup
    \fontsize{9pt}{9pt}\selectfont
$$
\begin{pmatrix}
0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & a & 1 & 0 
\end{pmatrix} \begin{pmatrix} 1 \\ x \\ y \\ x\_sq \\ y\_sq \\ xy\_sq \end{pmatrix}\odot
\begin{pmatrix}
0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 \\
1 & 0 & 0 & 0 & 0 & 0 
\end{pmatrix}  \begin{pmatrix} 1 \\ x \\ y \\ x\_sq \\ y\_sq \\ xy\_sq \end{pmatrix} =
\begin{pmatrix}
0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 & 0 & d 
\end{pmatrix} \begin{pmatrix} 1 \\ x \\ y \\ x\_sq \\ y\_sq \\ xy\_sq \end{pmatrix}
$$
\endgroup
EXERCISE: WRITE THE R1CS FOR WEIERSTRASS CURVE POINTS 
\begin{example}[Baby-JubJub]
Considering our pen and paper Baby JubJub curve over from XXX, we know that the curve is defined over $\F_{13}$ and that $(11,9)$ is a curve point, while $(2,3)$ is not a curve point. 

Starting with $(11,9)$, we can compute the statement $w=(1,11,9,4,3,12)$. Substituting this into the constraints we get
$$
\begin{array}{lcr}
11 \cdot 11 &=& 4\\
9 \cdot 9 &=& 3\\
4 \cdot 3 &=& 12\\
(1\cdot 4+3)\cdot 1 &=& 1+7\cdot 12
\end{array}
$$
which is true in $\F_{13}$. So our statement is indeed a valid assignment to the twisted Edwards curve constraining system.

Now considering the non valid point $(2,3)$, we can still come up with some kind of statement $w$ that will satisfy some of the constraints. But fixing $x=2$ and $y=3$, we can never satisfy all constraints. For example $w=(1,2,3,4,9,10)$ will satisfy the first three constraints, but the last constrain can not be satisfied. Or $w=(1,2,3,4,3,12)$ will satisfy the first and the last constrain, but not the others.
\end{example}
\paragraph{Twisted Edwards curves addition} As we have seen in XXX one the major advantages of working with (twisted) Edwards curves is the existence of an addition law, that contains no branching and is valid for all curve points. Moreover the neutral element is not "at infinity" but the actual curve poin $(0,1)$.

As we know from XXX, give two points $(x_1,y_1)$ and $(x_2,y_2)$ on a twisted Edwards curve their sum is given by
$$
(x_3,y_3) = \left(\frac{x_1y_2+y_1x_2}{1+d\cdot x_1x_2y_1y_2}, \frac{y_1y_2-a\cdot x_1x_2}{1-d\cdot x_1x_2y_1y_2}\right)
$$
% https://z.cash/technology/jubjub/
We can use the division circuit from XXX to flatten this equation into an algeraic circuit. Inputs to the circuit are then the two curve points $(x_1,y_1)$ abd $(x_2,y_2)$ as well as the the two denominators $denum_1 = 1+d\cdot x_1x_2y_1y_2$ as well as $denum_2= 1-d\cdot x_1x_2y_1y_2$. We get
\begin{center}
\digraph[scale=0.6]{EDWARDSADD}{
  forcelabels=true;
  center=true;
  splines=ortho;
  //nodesep= 2.0;
  
  subgraph clusterin {
    n1 [shape=box, label="x_1"] ;
    n2 [shape=box, label="x_2"] ;
    n3 [shape=box, label="y_1"] ;
    n4 [shape=box, label="y_2"] ;
      
    n22 [shape=box, label="denom_1"] ;
    n23 [shape=box, label="denom_2"] ;
  
    color=white ;
  }
  
  subgraph clusterout {
    n29 [shape=box, label="x_3"] ;
    n30 [shape=box, label="y_3"] ;
  
    color=white ;
  }

    n5 [shape=box, label="a"] ;
    n6 [shape=box, label="d"] ;
    n7 [shape=box, label="1"] ;
    n8 [shape=box, label="-1"] ;
    
    n9 [label="*"] ; // x_1*y_2
    n10 [label="*"] ; // x_1*x_2
    n11 [label="*"] ; // y_1*x_2
    n12 [label="*"] ; // y_1*y_2
    n13 [label="*"] ; // a*(x_1*x_2)
    n14 [label="*"] ; // -a*(x_1*x_2)
    n15 [label="+"] ; // x_1*y_2 + y_1*x_2
    n16 [label="+"] ; // y_1*y_2 - a*x_1*x_2
    n17 [label="*"] ; // (x_1*x_2)*(y_1*y_2)
    n18 [label="*"] ; // d*(x_1*x_2)*(y_1*y_2)
    n19 [label="*"] ; // -d*(x_1*x_2)*(y_1*y_2)
    n20 [label="+"] ; // 1 + d*(x_1*x_2)*(y_1*y_2)
    n21 [label="+"] ; // 1 - d*(x_1*x_2)*(y_1*y_2)
    
    n24 [label="*"] ; // (1 + d*(x_1*x_2)*(y_1*y_2))*denom_1 =1 
    n25 [label="*"] ; // (1 - d*(x_1*x_2)*(y_1*y_2))*denom_2 =1 
    n26 [shape=box, label="1"] ;
    n27 [label="*"] ; // denom_1*(x_1*y_2 + y_1*x_2) 
    n28 [label="*"] ; // denom_2*(y_1*y_2 - a*x_1*x_2) 
    
    n1 -> n9 [headlabel=" E_1"];
    n1 -> n10 [taillabel="E_1"];
    n2 -> {n10, n11} [taillabel="E_2"];
    n3 -> n11 [headlabel=" E_3"];
    n3 -> n12 [taillabel="E_3"];
    n4 -> n9 [taillabel="E_4"];
    n4 -> n12 [headlabel="  E_4"];
    n5 -> n13 ;
    n6 -> n18 ;
    n7 -> {n20, n21}
    n8 -> {n14, n19} ;
    n9 -> n15 [headlabel=" E_7"] ;
    n10 -> n13 [xlabel="E_8"] ;
    n10 -> n17 [xlabel="E_8"] ;
    n11 -> n15 [xlabel="E_9"] ;
    n12 -> n16 [taillabel="E_10 "] ;  
    n12 -> n17 [xlabel="  E_10"] ;   
    n13 -> n14 ;
    n14 -> n16 ;
    n15 -> n27 ;
    n16 -> n28 ; 
    n17 -> n18 [xlabel="E_11"] ;
    n18 -> {n19, n20} ;
    n19 -> n21 ;
    n20 -> n24 ;
    n21 -> n25 ;
    n22 -> {n24, n27} [xlabel="E_5"] ;
    n23 -> {n25, n28}  [xlabel="E_6"] ;
    n24 -> n26 [xlabel="E_12=1"] ;
    n25 -> n26 [xlabel="E_13=1"] ;
    
    n27 -> n29 [xlabel="E_14"] ;
    n28 -> n30 [xlabel="E_15"] ;
    
}
\end{center}
Using the general technique from XXX to derive the associated rank-1 constraint system, we get the following result:
\begin{align*}
E_1 \cdot E_4 & = E_7 \\
E_1 \cdot E_2 & = E_8 \\
E_2 \cdot E_3 & = E_9 \\
E_3 \cdot E_4 & = E_{10} \\
E_8 \cdot E_{10} & = E_{11} \\
E_5 \cdot (1+ d\cdot E_{11}) & = 1 \\
E_6 \cdot (1 - d\cdot E_{11}) & = 1 \\
E_5 \cdot (E_9 + E_7) & = E_{14} \\
E_6 \cdot (E_{10} - a\cdot E_8) & = E_{15}
\end{align*}

So we have the statement $w=(1,x_1,y_1,x_2,y_2,x_3,y_3,x_{12},y_{12},xy_{12},yx_{12},xy_{1212})$ and we need 7 constraints to enforce that $(x_1,y_1)+(x_2,y_2)=(x_3,y_3)$ 
\begin{example}[Baby-JubJub]
Considering our pen and paper Baby JubJub curve over from XXX. We recall from XXX that $(11,9)$ is a generator for the large prime order subgroup. We therefor already know from XXX that
$(11,9) + (7,8) = (11,9) + [3](11,9) = [4](11,9) = (2,9)$. So we compute a valid statement as 
$w=(1,11,9,7,8,2,9,12,7,10,11,6)$. Indeed
$$
\begin{array}{lcl}
11\cdot 7 &=& 12\\
9\cdot 8 &=& 7\\
11\cdot 8 &=& 10\\
9\cdot 7 &=& 11\\
10\cdot 11 &=& 6\\
2\cdot (1+7\cdot 6) &=& 10 + 11\\
9\cdot (1-7\cdot 6) &=& 7 -1\cdot 12
\end{array}
$$
\end{example}
There are optimizations for this using only 6 constraints, available:
% https://github.com/filecoin-project/zexe/blob/master/snark-gadgets/src/groups/curves/twisted_edwards/mod.rs#L129

\paragraph{Twisted Edwards curves inversion} Similar to elliptic curves in Weierstrass form, inversion is cheap on Edwards curve as the negative of a curve point $-(x,y)$ is given by $(-x,y)$. So a curve point $(x_2,y_2)$ is the additive inverse of another curve point $(x_1,y_1)$ precisely if the equation $(x_1,y_1) = (-x_2,y_2)$ holds. We can write this as
$$
\begin{array}{lcl}
x_1 \cdot 1 &=& -x_2 \\
y_1 \cdot 1 &=& y_2
\end{array}
$$
We therefor have a statement of the form $w=(1,x_1,y_1,x_2,y_2)$ and can write the constraints into a matrix equation as
$$
\begin{pmatrix}
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0
\end{pmatrix} \begin{pmatrix} 1 \\ x_1 \\ y_1 \\ x_2 \\ y_2 \end{pmatrix}\odot
\begin{pmatrix}
1 & 0 & 0 & 0 & 0\\
1 & 0 & 0 & 0 & 0
\end{pmatrix} \begin{pmatrix} 1 \\ x_1 \\ y_1 \\ x_2 \\ y_2 \end{pmatrix} =
\begin{pmatrix}
0 & 0 & 0 & -1 & 0\\
0 & 0 & 0 & 0 & 1
\end{pmatrix} \begin{pmatrix} 1 \\ x_1 \\ y_1 \\ x_2 \\ y_2 \end{pmatrix}
$$

In addition we need the following constraints:
$$
\begin{array}{lcl}
x_1 \cdot 1 &=& -x_2 \\
y_1 \cdot 1 &=& y_2
\end{array}
$$

\paragraph{Twisted Edwards curves scalar multiplication} 
% original circuit is here https://iden3-docs.readthedocs.io/en/latest/_downloads/33717d75ab84e11313cc0d8a090b636f/Baby-Jubjub.pdf

Although there are highly optimzed R1CS implementations for scal multiplication on elliptic curves, the basic idea is somewhat simple: Given an elliptic curve $E/\F_r$, a scalar $x\in \F_r$ with binary representation $(b_0,\ldots,b_m)$ and a curve point $P\in E/\F_r$, the scalar multiplication $[x]P$ can be written as
$$
[x]P = [b_0]P + [b_1]([2]P) + [b_2]([4]P) + \ldots + [b_m]([2^m] P)
$$
and since $b_j$ is either $0$ or $1$, $[b_j](kP)$ is either the neutral element of the curve or $[2^j]P$. However $[2^j]P$ can be computed inductively by curve point doubling, since $[2^j]P= [2]([2^{j-1}]P)$.

So scalar multiplication can be reduced to a loop of length $m$, where the original curve point is repeadedly douled and added to the result, whenever the appropriate bit in the scalar is equal to one.

So to enforce that a curve point $(x_2,y_2)$ is the scalar product $[k](x_1,y_1)$ of a scalar $x\in F_r$ and a curve point $(x_1,y_1)$, we need an R1CS the defines point doubling on the curve (XXX) and an R1CS that enforces the binary representation of $x$ (XXX). 

In case of twisted Edwards curve, we can use ordinary addition for doubling, as the constraints works for both cases (doublin is addition, where both arguments are equal). Moreover $[b](x,y)=(b\cdot x, b\cdot y)$ for boolean $b$. Hence flattening equation XXX gives
$$
\begin{array}{lclr}
b_0\cdot x_1 &=& x_{0,1} & // [b_0]P\\
b_0\cdot y_1 &=& y_{0,1}\\

\end{array}
$$
In addition we need to constrain $(b_0,\ldots, b_N)$ to be the binary representation of $x$ and we need to constrain each $b_j$ to be boolean.

As we can see a R1CS for scalar multiplication utilizes many R1CS that we have introduced before. For efficiency and readability it is therefore useful to apply the concept of a gadget (XXX). A pseudocode method to derive the associated R1CS could look like this:

%\begin{algorithmic}
%\Require $m$ Bitlength of modulus
%\Statement $w \gets [x,b[m],mid[m]]$
%\State $tmp \gets 0$
%\For{$j\gets 1,\ldots, m$}
%	\State \textbf{Constrain:} $b[j]\cdot (1-b[j]) == 0$
%	\State \textbf{Constrain:} $b[j] \cdot 2^j == mid[j]$
%	\State $tmp = tmp + mid[j]$
%\EndFor
%\State \textbf{Constrain:} $tmp \cdot 1 == x$
%\end{algorithmic}

%\begin{codebox}
%\Procname{$\proc{Insertion-Sort}(A)$}
%\li \For $j \gets 2$ \To $\id{length}[A]$
%\li     \Do$\id{key} \gets A[j]$
%\li         \Comment Insert $A[j]$ into the sorted sequence $A[1 \twodots j-1]$.
%\li         $i \gets j-1$\li         \While $i > 0$ and $A[i] > \id{key}$
%\li             \Do$A[i+1] \gets A[i]$
%\li                 $i \gets i-1$\End
%\li         $A[i+1] \gets \id{key}$\End
%\end{codebox}

\subsubsection{A Simple Pen and Paper Compiler Example}
% Set membership proof?


\subsection{Outlook on Real World Implementations}
many circuits can be found here:
% https://github.com/iden3/circomlib

Use the description of zdemir in Ana's podcast. 

