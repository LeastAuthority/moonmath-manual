\chapter{Algebra}
Todo: Def Subgroup,
% https://en.wikipedia.org/wiki/Subgroups_of_cyclic_groups
Fundamental theorem of cyclic groups.

We gave an introduction to the basic computational skills needed for a pen \& paper approach to SNARKS in the previous chapter. In this chapter we get a bit more abstract and clarify a lot of mathematical terminology and jargon.

When you read papers about cryptography or mathematical papers in general, you will frequently stumble across algebraic terms like \textit{groups}, \textit{fields},\textit{rings} and similar. To understand what is going on, it is necessary to get at least some understanding of these terms. In this chapter we therefore with a short introduction to those terms.

In a nutshell, algebraic types like groups or fields define sets that are analog to numbers to various extend, in the sense that you can add, subtract, multiply or divide on those sets.

We know many example of sets that fall under those categories, like the natural numbers, the integers, the rational or the real numbers. they are in some sense already the most fundamental examples.

\section{Groups} Groups are abstractions that capture the essence of mathematical phenomena, like addition and subtraction, multiplication and division, permutations, or symmetries.

To understand groups, remember back in school when we learned about addition and subtraction of integers (Forgetting about integer multiplication for a moment). We learned that we can always add two integers and that the result is guaranteed to be an integer again. We also learned how to deal with brackets, that nothing happens, when we add zero to any integer, that it doesn't matter in which order we add a given set of integers and that for every integer there is always another integer (the negative), such that when we add both together we get zero.

These conditions are the defining properties of a group and mathematicians have recognized that the exact same set of rules can be found in very different mathematical structures. It therefore makes sense to give a formulation of what a group should be, detached from any concrete example. This allows one to handle entities of very different mathematical origins in a flexible way, while retaining essential structural aspects of many objects in abstract algebra and beyond.

Distilling these rules to the smallest independent list of properties and making them abstract we arrive at the definition of a group:

A \textbf{group} $(\G,\cdot) $ is a set $ \G$, together with a map $ \cdot: \G \times \G \to \G $, called the group law, such that the following properties hold:
\begin{itemize}
\item (Existence of a neutral element) There is a $e\in\G$ for all $g\in\G$, such that $e\cdot g=g$ as well as $g\cdot e = g$.
\item (Existence of an inverse) For every $g\in\G$ there is a $g^{-1}\in\G$, such that $g\cdot g^{-1}=e$ as well as $g^{-1}\cdot g = e$.
\item (Associativity) For every $g_1,g_2,g_3\in\G$ the equation
$g_1\cdot(g_2\cdot g_3) = (g_1\cdot g_2)\cdot g_3$ holds.
\end{itemize}
Rephrasing the abstract definition in more layman's terms, a group is something, where we can do computations that resembles the behaviour of addition of integers. Therefore when the reader reads the term group they are advised to think of something where can combine some element with another element into a new element in a way that is reversible and where the order of combining many elements doesn't matter.
\begin{notation}
Let $(\mathbb{G}\cdot)$ be a finite group. If there is no risk of ambiguously we frequently drop the symbol $\cdot$ and simply write $\mathbb{G}$ as a notation for the group keeping the group law implicit.
\end{notation}
As we will see in what follows, groups are all over the place in cryptography and in SNARKS. In particular we will see in XXX, that the set of points on an elliptic curve define a group, which is the most important example in this book. To give some more familiar examples first:
\begin{example}[Integer Addition and Subtraction]
The set $(\Z,+)$ of integers together with integer addition is the archetypical example of a group, where the group law is traditionally written as $+$ (instead of $\cdot$). To compare integer addition against the abstract axioms of a group, we first see that the neutral element $e$ is the number $0$, since $a+0=a$ for all integers $a\in $ and that the inverse of a number is the negative, since $a+(-a)=0$, for all $a\in\Z$. In addition we know that $(a+b)+c=a+(b+c)$, so integers with addition are indeed a group in the abstract sense.
\end{example}
\begin{example}[The trivial group]
The most basic example of a group, is group with just one element $\{\bullet\}$ and the group law $\bullet\cdot \bullet=\bullet$.
\end{example}
%\begin{example}[Rotations]
%To give an example of a group that has effects in the real world, consider a dice. Then our group is the set of all possible ways to rotate the dice by 90 degrees along an imagined axis through two opposite faces. The group law is composition of rotations. So say hold the dice with two fingers at $1$ and $6$. $1$ is the face that points towards you and $5$ is the top face. Then you first rotate the dice along the $1$-$6$ axis by 90 degrees clockwise, such that now $3$ is the top face. Then you hold the dice at $5$ and
%\end{example}
\paragraph{Commutative Groups} When we look at the general definition of a group we see that it is somewhat different from what we know from integers. For integers we know, that it doesn't matter in which order we add two integers, as for example $4+2$ is the same as $2+4$. However we also know from example XXX, that this is not always the case in groups.

To capture the special case of a group where the order in which the group law is executed doesn't matter, the concept of so called a \textbf{commutative group} is introduced. To be more precise a group is called commutative if  $g_1\cdot g_2 = g_2 \cdot g_1$ holds for all $g_1,g_2\in\G$.
\begin{notation}
In case $(\G,\cdot)$ is a commutative group, we frequently use the so called \textit{additive notation} $(\G,+)$, that is we write $+$ instead of $\cdot$ for the group law and $-g:=g^{-1}$ for the inverse of an element $g\in\G$.
\end{notation}
\begin{example} Consider the group of integers with integer addition again.
Since $a+b=b+a$ for all integers, this group is the archetypical example of a commutative group. Since there are infinite many integers, $(\Z,+)$ is not a finite group.
\end{example}
\begin{example} Consider our definition of modulo $6$ residue classes $(\Z_6,+)$ as defined in the addition table from example XXX. As we see the residue class $0$ is the neutral element in modulo $6$ arithmetics and the inverse of a residue class $r$ is given by $6-r$, since $r+(6-r)=6$, which is congruent to $0$, since $\Zmod{6}{6}=0$. Moreover $(r_1+r_2)+r_3=r_1+(r_2+r_3)$ is inherited from integer arithmetic.

We therefore see that $(\Z_6,+)$ is a group and since addition table XX is symmetric, we see $r_1+r_2 = r_2+r_1$ which shows that $(\Z_6,+)$ is commutative.
\end{example}
The previous example provided us with an important example of commutative groups that are important in this book. Abstracting from this example and considering residue classes $(\Z_n,+)$ for arbitrary moduli $n$, it can be shown that $(\Z,+)$ is a commutative group with neutral element $0$ and additive inverse $n-r$ for any element $r\in\Z_n$. We call such a group the \textit{reminder class groups} of modulus $n$.

Of particular importance for pairing based cryptography in general and snarks in particular are so called \textit{pairing maps} on commutative groups. To be more precise let $\G_1$, $\G_2$ and $\G_3$ be three commutative groups. For historical reasons, we write the group law on $\G_1$ and $\G_2$ in additive notation and the group law on $\G_3$ in multiplicative notation. Then a \textbf{pairing map} is a function
\begin{equation}
e(\cdot,\cdot): \G_1 \times \G_2 \to \G_3
\end{equation}
that takes pairs $(g_1,g_2)$ (products) of elements from $\G_1$ and $\G_2$ and maps them somehow to elements from $\G_3$, such that the \textit{bilinearity} property holds: For all $g_1,g_1'\in \G_1$ and $g_2\in \G_2$ we have $e(g_1+ g_1',g_2)= e(g_1,g_2)\cdot e(g_1',g_2)$ and for all $g_1\in \G_1$ and $g_2, g_2'\in \G_2$ we have $e(g_1,g_2+ g_2')= e(g_1,g_2)\cdot e(g_1,g_2')$.

A pairing map is called \textit{non-degenerated}, if whenever the result of the pairing is the neutral element in $\G_3$, one of the input values must be the neutral element of $\G_1$ or $\G_2$. To be more precise $e(g_1,g_2)=e_{\G_3}$ implies $g_1=e_{\G_1}$ or $g_2=e_{\G_2}$.

So roughly speaking bilinearity means, that it doesn't matter if we first execute the group law on any side and then apply the bilinear map of if we first apply the bilinear map and then apply the group law. Moreover non-degeneray means that the result of the pairing is zero, only if at least one of the input values is zero.
\begin{example}Maybe the most basic example of a non-degenerate pairing is obtained, if we take $\G_1$, $\G_2$ and $\G_3$ all to be the group of integers with addition $(\Z,+)$. Then the following map
$$
e(\cdot,\cdot): \Z \times \Z \to \Z \; (a,b)\mapsto a\cdot b
$$
defines aa non-degenerate pairing. To see that observe, that bilinearity follows from the distributive law of integers, since for $a,b,c\in \Z$, we have $e(a+b,c)=(a+b)\cdot c = a\cdot c + b\cdot c = e(a,c)+ e(b,c)$ and the same reasoning is true for the second argument.

To the that $e(\cdot,\cdot)$ is non degenrate, assume that $e(a,b)=0$. Then a$\cdot b =0$ and this implies that $a$ or $b$ must be zero.
\end{example}

\begin{exercise} Consider example XXX again and let $\F_5^*$ be the set of all remainder classes from $\F_5$ without the class $0$. Then $\F_5^*=\{1,2,3,4\}$. Show that $(\F_5^*,\cdot)$ is a commutative group.
\end{exercise}
\begin{exercise} Generalizing the previous exercise, consider general moduli $n$ and let $\Z_n^*$ be the set of all remainder classes from $\Z_n$ without the class $0$. Then $\Z_n^*=\{1,2,\ldots,n-1\}$. Give a counter example to show that $(\Z^*_n,\cdot)$ is not a group in general.

Find a condition, such that $(\Z^*_n,\cdot)$ is a commutative group, compute the neutral element, give a closed form for the inverse of any element and proof the commutative group axioms.
\end{exercise}
\begin{exercise} Consider the remainder class groups $(\Z_n,+)$ for some modulus $n$. Show that the map
$$
e(\cdot,\cdot): \Z_n \times \Z_n \to \Z_n \; (a,b)\mapsto a\cdot b
$$
is bilinear. Why is it not a pairing in general and what condition must be imposed on $n$, such that the map is a pairing?
\end{exercise}
\paragraph{Finite groups} As we have seen in the previous examples, groups can either contain infinite many elements (as the integers) or finitely many elements as for example the remainder class groups $(\Z_n,+)$. To capture this distinction a group is called a \textit{finite group}, if the underlying set of elements is finite. In that case the number of elements of that group is called its \textbf{order}.
\begin{notation}
Let $\mathbb{G}$ be a finite group. Then we frquently write $ord(\mathbb{G})$ or  $|\mathbb{G}|$ for the order of $\mathbb{G}$.
\end{notation}
\begin{example}
Consider the remainder class groups $(\Z_6,+)$ and $(\F_5,+)$ from example XXX and example XXX and the group $(\F_5^*,\cdot)$ from exercise XX. We can easily see that the order of $(\Z_6,+)$ is $6$, the order of $(\F_5,+)$ is five and the order of $(\F_5^*,\cdot)$ is $4$.

To be more general, considering arbitrary moduli $n$, then we know from Euclidean division, that the order of the remainder class group $(\Z_n,+)$ is $n$.
\end{example}
\begin{exercise}The RSA crypto system is based on a modulus $n$ that is typically the product of two prime numbers of size $2048$-bits. What is (approximately) the order of the rainder class group $(\Z_n,+)$ in this case?
\end{exercise}
\paragraph{Generators} Of special interest, when working with groups are sets of elements that can generate the entire group, by applying the group law repeatedly to those elements or their inverses only.

Of course every group $\G$ has trivially a set of generators, when we just consider every element of the group to be in the generator set. So the more interesting question is to find the smallest set of generators. Of particular interest in this regard are groups that have a single generator, that is there exist an element $g\in\G$, such that every other element from $\G$ can be computed by repeated combination of $g$ and its inverse $g^{-1}$ only. Those groups are called \textbf{cyclic groups}.
\begin{example} The most basic example of a cyclic group are the integers $(\Z,+)$ with integer addition. To see that observe that $1$ is a generator of $\Z$, since every integer can be obtained by repeatedly add either $1$ or its inverse $-1$ to itself. For example
$-4$ is generated by $-1$, since $-4=-1+(-1)+(-1)+(-1)$.
\end{example}
\begin{example} Consider a modulus $n$ and the remainder class groups $(\Z_n,+)$ from example XXX. These groups are cyclic, with generator $1$, since every other element of that group can be constructed by repeatedly adding the remainder class $1$ to itself. Since $\Z_n$ is also finite, we know that $(\Z_n,+)$ is a finite cyclic group of order $n$.
\end{example}
\begin{example} Let $p\in\P$ be prime number and $(\F_p^*,\cdot)$ the finite group from exercise XXX. Then $(\F_p^*,\cdot)$ is cyclic and every element $g\in\F_q^*$ is a generator.
\end{example}
\paragraph{The discrete Logarithm problem}
In cryptography in general and in snark development in particular, we often do computations "in the exponent" of a generator. To see what this means, observe, that when
$\G$ is a cyclic group of order $n$ and $g\in \G$ is a generator of $\G$, then there is a map, called the \textbf{exponential map} with respect to the generator $g$
\begin{equation}
g^{(\cdot)}: \Z_n \to \G\; x \mapsto g^x
\end{equation}
where $g^x$ means "multiply $g$ $x$-times by itself and $g^0=e_{\G}$. This map has the remarkable property maps the additive group law of the remainder class group $(\Z_n,+)$ in a one-to-one correspondence to the group law of $\G$.

To see that first observe, that since $g^0:=e_{\G}$ by definition, the neutral element of $\Z_n$ is mapped to the neutral element of $\G$ and since $g^{x+y}=g^x\cdot g^y$, the map respects the group laws.

Since the exponential map respects the group law, it doesn't matter if we do our computation in $\Z_n$ before we write the result into the exponent of $g$ or afterwards. The result will be the same. This is what is usually meant by saying we do our computations "in the exponent".
\begin{example} Consider the multiplicative group $(\F_{5}^*,\cdot)$ from example XXX. We know that $\F_{5}^*$ is a cyclic group of order $4$ and that every element is a generator. Choose $3\in\F_5^*$, we then know that the map
$$
3^{(\cdot)}: \Z_4 \to \F_5^* \; x \mapsto 3^x
$$
respects the group law of addition in $\Z_4$ and the group law of multiplication in $\F_5^*$.
And indeed doing a computation like
\begin{align*}
3^{2+3-2} &=3^{3}\\
          & = 2
\end{align*}
in the exponent gives the same result as doing the same computation in $\F*_5$, that is
\begin{align*}
3^{2+3-2} &= 3^2 \cdot 3^3 \cdot 3^{-2}\\
          &= 4\cdot 2 \cdot (-3)^2\\
          &= 3\cdot 2^2\\
          &= 3\cdot 4 \\
          &= 2
\end{align*}
\end{example}
Since the exponential map is a one-to-one correspondence, that respects the group law, it can be shown that this map has an inverse
\begin{equation}
log_g(\cdot): \G \to \Z_n\; x \mapsto log_g(x)
\end{equation}
which is called the \textbf{discrete logarithm} map with respect to the base $g$. Discrete logarithms are highly importsnt in cryptography as there are groups, such that the exponential map and its inverse the discrete logarithm, are believed to be one way functions, that is while it is possible to compute the exponential map in polynomial time, computing the discrete log takes (sub)-exponential time. We will look at this and similar problems in more detail in the next section.


% TODO fundamental theorem of finite cyclic groups
% Cofactor clearing starts with aritrary group element and generates generators
% \begin{definition}[Cofactor Clearing]
%Since $BLS6-6(13)$ is a subgroup on our curve, it is not possible to leave the subgroup using the curves algebraic laws like scalar multiplication or addition. However in applications it often happens that random elements of the curve are generated, while what we really want are points in the subgroup. To get those points we can use cofactor clearing.
%\end{definition} 

\subsection{Cryptographic Groups} In this section, we will look at families of groups, which are believed to satisfy certain so called \textit{computational hardness assumptions}, the latter of which is a term to express the hypothesis that a particular problem cannot be solved efficiently (where efficiently typically means "in polynomial time of a given security parameter") in the groups of consideration. 
\begin{example} 
To highlight the concept of a computational hardness assumption, consider the group of integers $\Z$ from example XXX. One of the best known and most researched examples of computational hardness is the assumption that the factorization of integers into prime numbers as explained in XXX can not be solved by any algorithm in polynomial time with respect to the bit-length of the integer.

To be more precise the computational hardness assumption of integer factorization assumes that given any integer $z\in\Z$ with bit-length $b$, there is no integer $k$ and no algorithm with run time complexity $\mathcal{O}(b^k)$, that is able to find prime numbers $p_1, p_2,\ldots,p_j\in\Prim$, such that $z=p_1\cdot p_2\cdot \ldots\cdot p_j$. 

Generally speaking, this hardness assumption was proven to be false, since Shor's algorithm shows that integer factorization is at least efficiently possible on a quantum computer, since the run time complexity of this algorithm is $\mathcal{O}(b^3)$. However no such algorithm is known on a classical computer.

In the realm of classical computers however, we still have to call the non existence of such an algorithm an "assumption" because to date, there is no proof that it is actually impossible to find some. The problem is that it is hard to reason about algorithms that we don't know.

So despite the fact that there is currently no know algorithm that can factor integers efficiently on a classical computer, we can not exclude that such an algorithm might exist in principal and someone eventually will discover it in the future.

However what still makes the assumption plausible, despite the absense of any actual proof, is the fact that after decades of extensive search still no such algorithm has been found.
\end{example}
In what follows, we will describe a few computational hardness assumptions that arrise in the context of groups in cryptography, as we will need them throughout the book.
\paragraph{The discret logarithm assumption} The so called discrete logarithm problem is one of the most fundamental assumptions in cryptography. To define it, let $\G$ be a finite cyclic group of order $r$ and let $g$ be a generator of $\G$.  We know from XXX that there is a so called exponential map 
$g^{(\cdot)}: \Z_r \to \G:\; x\mapsto g^x$, which maps the residue classes from module $r$ arithmetic onto the group in a $1:1$ correspondence. The \textbf{discrete logarithm problem} is then the task to find inverses to this map, that is, to find a solution $x\in\Z_r$, to the equation
\begin{equation}
h = g^x
\end{equation}
for some given $h\in\G$. The \textbf{discrete logarithm assumption (DL-A)} is then the assumption that there exists no algorithm with run time polynimial in the "security parameter $log_2(r)$, that is able to compute some $x$ if ony $h$, $g$ and $g^x$ are given in $\G$. If this is the case for $\G$ we call $\G$ a \textit{DL-A group}.

Rephrasing the previous definition into simple words, DL-A groups are believed to have the property, that it is infeasible to compute some number $x$ that solves the equation $h=g^x$ for given $h$ and $g$, assuming that the size of the group $r$ is large enough.
\begin{example}[Public key cryptography]
One the most basic examples of an application for DL-A groups is in public key cryptography, where some pair $(\G,g)$ is publically agreed on, such that $\G$ is a finite cyclic group sufficiently large order $r$, where it is believed that the discrete logarithm assumption holds and $g$ is a generator of $\G$.

In this setting a secret key is nothing but some number $sk\in \Z_r$ and the associated public key $pk$ is the group element $pk=g^{sk}$. Since discrete logarithms are assumed to be hard it is therefore infisible for an attacker to compute the secret key from the public key, since it is believed to be hard to find solutions $x$ to the equation 
$$
pk = g^{x}
$$ 
\end{example}
As the previous example shows, it is an important practical problem to identify DL-A groups. Unfortunately it is easy to see, that it does not make sense to assume the hardness of the discrete logarithm problem in all finite cyclic groups. Counterexamples are common and easy to construct. 
\begin{example}[Modular arithmetics for Fermat's primes] It is widely believed that the discrete logarithm problem is hard in multiplicative groups $\Z_p^*$ of prime number modular arithmetics. However this is not true in general. To see that consider any so called Fermat's prime, which is a prime number $p\in\Prim$, such that $p=2^n+1$ for some number $n$.

We know from XXX, that in this case $\Z_p^* = \{1,2,\ldots, p-1\}$ is group with respect to integer multiplication in modular $p$ arithmetics and since $p=2^n+1$, the order of $\Z_p^*$ is $2^n$, which implies that the associated security parameter is given by $log_2(2^n)=n$.

We show that in this case $\Z_p^*$ is not a DL-A group, by constructing an algorithm, which is able compute some $x\in\Z_{2^n}$ for any given generator $g$ and arbitry element $h$ of $\F_p^*$, such that
$$
h = g^x
$$
holds and the run time complexity of the constrcted algorithm is $\mathcal{O}(n^2)$, which is quadratic in the security parameter $n=log_2(2^n)$.  

To define such an algorithm, lets assume that the generator $g$ is a public constant and that a group element $h$ is given. Our task is to compute $x$ efficiently. 

A first thing to note is that since $x$ is a number in modular $2^n$ aithmetic, we can write the binary representation of $x$ as
$$
x = c_0\cdot 2^0 + c_1\cdot 2^1 + \cdots + c_n \cdot 2^n
$$
with binary coefficients $c_j\in\{0,1\}$. In particular $x$ as an $n$-bit number, if interpreted as an integer. 

We then use this representation to construct an algorithm that computes the bits $c_j$ one after another, starting at $c_0$. To see how this can be achieved, observe that we can determine $c_0$ by raising the input $h$ to the power of $2^{n-1}$ in $\F_p^*$. We use the exponential laws and compute 
\begin{align*}
h^{2^{n-1}} & = \left(g^x\right)^{2^{n-1}}\\
            & = \left(g^{c_0\cdot 2^0 + c_1\cdot 2^1 + \ldots + c_n\cdot 2^n}\right)^{2^{n-1}}\\
            & = g^{c_0\cdot 2^{n-1}}\cdot g^{c_1\cdot 2^1\cdot 2^{n-1}} \cdot 
            g^{c_2\cdot 2^2\cdot 2^{n-1}} \cdots g^{c_n\cdot 2^n\cdot 2^{n-1}}\\
            & = g^{c_0 2^{n-1}}\cdot g^{c_1\cdot 2^0\cdot 2^{n}} \cdot
            g^{c_2\cdot 2^1\cdot 2^{n}} \cdots g^{c_n\cdot 2^{n-1}\cdot 2^{n}}
\end{align*}
Now since $g$ is a generator and $\F_p^*$ is cyclic of order $2^n$, we know $g^{2^n}=1$ and therefore $g^{k\cdot 2^n}= 1^k=1$. From this follows that all but the first factor in the last expressen are equal to $1$ and we can simplify the expression into
$$
h^{2^{n-1}} = g^{c_0 2^{n-1}}
$$ 
Now in case $c_0=0$, we get $h^{2^{n-1}} = g^0=1$ and in case $c_0=1$ we get 
$h^{2^{n-1}} = g^{2^{n-1}}\neq 1$ (To see that $g^{2^{n-1}}\neq 1$, recall that $g$ is a generator of $\F_p^*$ and hence is cyclic of order $2^n$, which implies $g^y\neq 1$ for all $y<2^n$).

So raising $h$ to the power of $2^{n-1}$ determines $c_0$ and we can apply the same reasoning to the coefficient $c_1$ by raising $h\cdot g^{-c_0\cdot 2^0}$ to the power of $2^{n-2}$. This approach can then be repeated until all the coefficients $c_j$ of $x$ are found.

Assuming that exponentiation in $\F_p^*$ can be done in logarithmic run time complexity $log(p)$, it follows that our algorithm has a run time complexity of 
$\mathcal{O}(log^2(p))=\mathcal{O}(n^2)$, since we have to execute $n$ exponentiations to determine the $n$ binary coefficients of $x$. 

From this follows that whenever $p$ is a Fermat's prime, the discrete logarithm assumption does not hold in $F_p^*$.
\end{example}
\paragraph{The decisional Diffi Hellman assumption}
To describe the decisional Diffie–Hellman assumption , let $\G$ be a finite cyclic group od order $r$ and let $g$ be a generator of $\G$. The DDH assumption then assumes that there is no algorithm that has a run time complexity polynomial in the security parameter $s= log(r)$, that is able to distiguish the so called DDH-tripple $(g^a,g^b, g^{ab})$ from any tripple $(g^a,g^b,g^c)$ for randomly and independently choosen parameters $a,b,c\in \Z_r$. If this is the case for $\G$ we call $\G$ a DDH-A group.

It is easy to see that DDH-A is a stronger assumption then DL-A, in the sense that the discrete logarithm assumption is neccessary for the dicisional Diffi Hellman assumption to hold, but not the other way around. 

To see why, assume that the discrete logarithm assumption does not hold. In that case given a genrator $g$ and a group element $h$, it is easy to compute some residue class $x\in\Z_p$ with $h=g^x$. Then the dicisional Diffi-Hellman assumption could not hold, since given some tripple ($ g^a , g^b , z )$, one could efficiently decide whether $z = g^{ab}$ by first computing the discrete logarithm $b$ of  $g^b$, then compute $g^{ab}= (g^a)^b$ and decide whether or not $z=g^{ab}$.

On the other hand, the following example shows, that there are groups where the discrete logarithm assumption holds but the decisional Diffi Hellman assumption does not hold:
\begin{example}[Efficiently computable pairings] Let $\G$ be a finite, cyclic group of order $r$ with generator $g$, such that the discrete logarithm assumtion holds and such that there is a pairing map $e(\cdot,\cdot): \G \times \G \to \G_T$ for some target group $\G_T$ that is computable in polynomial time of the parameter $log(r)$. 

In a setting like this it is easy to show that DDH-A can not hold, since given some tripple ($ g^a , g^b , z )$, it is possible to decide in polynomial times w.r.t $log(r)$ whether $z=g^{ab}$ or not. To see that check 
$$
e(g^a,g^b)=e(g,z)
$$Since the bilinierity properties of $e(\cdot,\cdot)$ imply $e(g^a,g^b)= e(g,g)^{ab}= e(g,g^{ab})$ and $e(g,y)=e(g,y')$ implies $y=y'$ due to the non degeneray property, the equality decides $z=e^{ab}$.
\end{example} 
It follows that DDH-A is indeed weaker then DL-A and groups with efficient pairings can not be DDH-A groups. As the following example shows, another important class of groups, where DDH-A does not hold are the multiplicative groups of prime number residue classes.
\begin{example}Let $p$ be a prime number and $\Z_p^*=\{1,2,\ldots,p-1\}$ the multiplicative group of modular $p$ arithmetics as in example XXX. As we have seen in XXX, this group is finite and cyclic of order $p-1$ and every element $g\neq 1$ is a generator.

To see that $\F_p^*$ can not be a DDH-A group recall from XXX that the Legendre symbol $\left(\frac{x}{p}\right)$ of any $x\in \F_p^*$ is efficiently computable by Euler's formular. But the  Legendre symbol of $g^{a}$ reveals if $a$ is even or odd. Given $g^{a}$, $g^{b}$ and $g^{ab}$, one can thus efficiently compute and compare the least significant bit of $a$, $b$ and $a b$, respectively, which provides a probabilistic method to distinguish $g^{ab}$ from a random group element $g^c$. 
\end{example}
\paragraph{The computational Diffi Hellman assumption}
To describe the computational Diffie-Hellman assumption, let $\G$ be a finite cyclic group od order $r$ and let $g$ be a generator of $\G$. The computational Diffi-Hellman assumption, then assumes that given randomly and independently choosen residue classes $a,b\in\Z_r$, it is not possible to compute $g^{ab}$ if only $g$, $g^a$ and $g^b$ (but not $a$ and $b$) are known. If this is the case for $\G$ we call $\G$ a CDH-A group.

In general it is not know if CDH-A is a stronger assumption then DL-A, or if both assumptions are equivalent. It is known that DL-A is necessary for CDH-A but the other direction is currently not well understood. In particular there are no groups known where DL-A holds but CDH-A does not hold.
% https://web.stanford.edu/class/cs259c/finalpapers/dlp-cdh.pdf

To see why the discrete logarithm assumption is necessary, assume that it does not hold. So given a genrator $g$ and a group element $h$, it is easy to compute some residue class $x\in\Z_p$ with $h=g^x$. In that case the computational Diffi-Hellman assumption can not hold, since given $g$, $g^a$ and $g^b$, one can efficiently compute $b$ and hence is able to compute $g^{ab}=(g^a)^b$ from this data.

The computational Diffi-Hellman assumption is a weaker assumption then the decisional Diffi Hellman assumption, which means that there are groups where CDH-A holds and DDH-A does not holdm while there can not be groups such that DDH-A holds but CDH-A does not hold. To see that assume that it is efficiently possible to compute $g^{ab}$ from $g$, $g^a$ and $g^b$. Then, given $(g^a,g^b,z)$ it is of course easy to decide if $z=g^{ab}$ or not. 

From the CDH-A various variations and specializations are known. For example the so called \textit{square computational Diffi Hellman assumption} asumes, that given $g$ and $g^x$ it is computationally hard to compute $g^{x^2}$ while the so called \textit{inverse computational Diffi Hellman assumption} asumes, that given $g$ and $g^x$ it is computationally hard to compute $g^{x^{-1}}$. 

\paragraph{Cofactor Clearing}
TODO: (theorem: every factor of order defines a subgroup...)
\subsection{Hashing to Groups}
% https://crypto.stackexchange.com/questions/78017/simple-hash-into-a-prime-field
\paragraph{Hash functions} Generally speaking, a hash function is any function that can be used to map data of arbitrary size to fixed-size values. Since binary strings of arbitrary length are a general way to represent arbitrary data, we can understand a general \textbf{hash function} as a map 
\begin{equation}
H: \{0,1\}^* \to \{0,1\}^k
\end{equation}
where $\{0,1\}^*$ represents the set of all binary strings of arbitrary but finite length and $\{0,1\}^k$ represents the set of all binary strings that have a length of exactly $k$ bits. So in our definition a hash function maps binary strings of arbitrary size onto binary strings of size exactly $k$. We call the images of $H$, that is the values returned by the hash function \textit{hash values}, \textit{digests}, or simply \textit{hashes}.

A hash function must be deterministic, that is inserting the same input $x$ into $H$, so image $H(x)$ must always be the same. In addition a hash function should be as uniform as possible, which means that it should map input values as evenly as possible over its output range. In mathematical terms every length $k$ string from $\{0,1\}^k$ should be generated with roughly the same probability.  
\begin{example}[$k$-truncation hash] One of the most basic hash functions 
$H_k:\{0,1\}^*\to \{0,1\}^k$ is given by simply truncating every binary string $s$ of size $s.len()> k$ to a string of size $k$ and by filling any string $s'$ of size $s'.len()<0$ with zeros. To make this hash function deterministic, we define that both truncation and filling should happen "on the left".

For example if $k=3$, $x_1=(0000101011101010011101010101)$ and $x_2=1$ then $H(x_1)=(101)$ and $H(x_2)=(001)$. It is easy to see that this hash function is deterministic and uniform.
\end{example}
Of particular interest are so called \textit{cryptographic} hash functions, which are hash functions that are also \textit{one-way functions}, which essentially means that given a string $y$ from $\{0,1\}^k$ its practically infeasible to find a string $x\in\{0,1\}^*$ such that $H(x)=y$ holds. This property is usually called \textit{preimage-resistence}. 

In addition it should be infeasible to find to strings $x_1,x_2 \in\{0,1\}^*$, such that $H(x_1)=H(x_2)$, which is called \textit{collision resistence}. It is important to note though, that collisions always exists, since a function $H: \{0,1\}^* \to \{0,1\}^k$ inevitable maps infinite many values onto the same hash. In fact, for any hash function with digests of length $k$, finding a preimage to a given digest can always be done using a brute force search in $2^k$ evaluation steps. It should just be practically impossible to compute those values and statistically very unlikely to generate two of them by chance. 

A third property of a cryptographic hash function is, that small changes in the input string like flipping a single bit, should generate hash values that look completely different from each other.

As cryptographically secure hash functions map tiny changes in input values onto large change in the output, implementation errors that change the outcome are usually easy to spot by comparing them to expected output values. The definition of cryptographically secure hash function are therefore usually acompanied by some test vectors of common inputs and expected digests. Since the empty string $''$ is the only string of length $0$ a common test vector is the expected digest of the empty string.
\begin{example}[$k$-truncation hash] Considering the $k$-truncation hash from example XXX. Since the empty string has length $0$ it follows that the digest of the empty string is string of length $k$ that only contains $0$'s. i.e 
$$
H_k('')= (000\ldots 000)
$$
It is pretty obvious from the definition of $H_k$ that this simple hash function is not a cryptographic hash function. In particular every digest is its onw preimage, since $H_k(y)=y$ for every string of size exactly $k$. Finding preimages is therefore easy. 

In addition it is easy to construct collusions as all strings of size $>k$ that share the same $k$-bits "on the right" are mapped to the same hash value.

Also this hash function is not very chaotic, as changing bits that are not part of the $k$ right most bits don't change the digest at all.  
\end{example}
Computing cryptographically secure hash function in pen and paper style is possible but tedious. Fortunately sage can import the \textit{PyCrypto} library, which is intended to provide a reliable and stable base for writing Python programs that require cryptographic functions. The following examples explains how to use PyCrypto in sage.
\begin{example}An example of a hash function that is generally believed to be a cryptographically secure hash function is the so called \textit{SHA256} hash, which in our notation is a function
$$
SHA256: \{0,1\}^* \to \{0,1\}^{256}
$$
that maps binary strings of arbitrary length onto binary strings of length $256$. To evaluate a proper implementation of the $SHA256$ hash function the giest of the empty string is supposed to be
$$
SHA256('')= e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
$$
For better human readability it is common practise to represent the digest of a string, not in its binary form but in a hexadecimal representation. We can use sage to compute $SHA256$ and freely transit between binary, hexadecimal and decimal representations. To do so we have to import PyCrypto and then load $SHA\_256$:
\begin{sagecommandline}
sage: import Crypto
sage: from Crypto.Hash import SHA256
sage: test = 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855'
sage: d = SHA256.new('')
sage: str = d.hexdigest()
sage: type(str)
sage: d = ZZ('0x'+ str) # conversion to integer type
sage: d.str(16) == str
sage: d.str(16) == test
sage: d.str(16)
sage: d.str(2)
sage: d.str(10)
\end{sagecommandline}
\end{example}

\paragraph{Hashing to cyclic groups} As we have seen in the previous paragraph general hash functions map binary strings of arbitrary length onto binary strings of length $k$ for some parameter $k$. In various cryptographic primitives it is however desireable to not simply hash to binary strings of fixed length but to hash into algebraic structures like groups, while keeping (some of) the properties like preimiage or collision resistence. 

Hash functions like this can be defined for various algebraic structures, but in a sense, the most fundamental ones are hash functions that map into groups, because they can usually be extended easily to map into other structures like rings or fields. 

To give a more precise definition, let $\G$ be a group and $\{0,1\}^*$ the set of all finite, binary strings, then a \textbf{hash-to-group} function is a deterministic map
\begin{equation}
H : \{0,1\}^* \to \G
\end{equation}
Common properties of hash functions, like uniformity are desireable but not always realized in actual real world instantiations of hash-to-group functions, so we skip those requirements for now and keep the definition very general.

As the following example shows hashing to finite cyclic groups can be trivially achieved for the price of some undesireable properties of the hash function:
\begin{example}[Naive cyclic group hash] Let $\G$ be a finite cyclic group. If the task is to implement a hash-to-$\G$ function, one immediate approach can be based on the observation that binary strings of size $k$, can be interpreted as integers $z\in\Z$ in the range $0\leq z < 2^k$. 

To be more precise, choose an ordinary hash function $H:\{0,1\}^*\to \{0,1\}^k$ for some parameter $k$ and a generator $g$ of $\G$. Then the expression
$$
z_{H(s)}= H(s)_0\cdot 2^0 + H(s)_1\cdot 2^1 + \ldots + H(s)_k \cdot 2^k
$$
is a positive integer, where $H(s)_j$ means the bit at the $j$-th position of $H(s)$. A hash-to-group function for the group $\G$ can then be defined as a concatenation of the exponential map $g^{(\cdot)}$ of $g$ with the interpretation of $H(s)$ as an integer: 
$$
H_{g} : \{0,1\}^* \to \G:\; s \mapsto g^{z_{H(s)}} 
$$
Constructing a hash-to-group function like this is easy to implement for cyclic groups and might be good enough in certain applications. It is however almost never adequate in cryptographic applications as discrete log relations might be constructible between two given hash value $H_g(s)$ and $H_g(t)$. 

To see that, assume that $\G$ is of order $r$ and that $z_{H(s)}$ has a multiplicative inverse in modular $r$ arithmetics. In that case we can compute $x=z_{H(t)}\cdot z_{H(s)}^{-1}$ in $\Z_r$ and have found a discrete log relation between the group hash values, that is we have found some $x$ with $H_g(t) = (H_g(s))^x$ since
\begin{align*}
H_g(t) & = (H_g(s))^x & \Leftrightarrow \\
g^{z_{H(t)}} & = g^{z_{H(s)}\cdot x} & \Leftrightarrow \\
g^{z_{H(t)}} & = g^{z_{H(t)}}
\end{align*}
\end{example}
Applications where discrete log relations between hash values are undesireable therefore need different approaches and many of those approaches start with a way to hash into the sets $\Z_r$ of modular $r$ arithmetics. 
\paragraph{Hashing to modular arithmetics} One of the most widely used applications of hash-into-group functions are hash functions that map into the set $\Z_r$ of modular $r$ arithmetics for some modulus $r$. Different approaches to construct such a function are known, but probably the most used once are based on the insight that the images of arbitrary hash functions can be interpreted as binary representations of integers as explained in example XXX.

From this interpretation follows that one simple method of hashing into $\Z_r$ is constructed by observing, that if $r$ is a modulus, with a bit-length of $k=r.nbits()$, then every binary string $(b_0,b_1,\ldots,b_{k-2})$ of length $k-1$ defines an integer $z$ in the rage $0\leq z < 2^{k-1}\leq r $, by defining
\begin{equation}
z = b_0\cdot 2^0 + b_1\cdot 2^1 + \ldots + b_{k-2}\cdot 2^{k-2}
\end{equation}
Now since $z<r$, we know that $z$ is guranteed to be in the set $\{0,1,\ldots,r-1\}$ and hence can be interpreted as an element of $\Z_r$. From this follows that if $H:\{0,1\}^*\to\{0,1\}^{k-1}$ is a hash function, then a hash-to-group function can be constructed by
\begin{equation}
H_{r.nbits()-1}: \{0,1\}^* \to \Z_r: \; s \mapsto 
H(s)_0\cdot 2^0 + H(s)_1\cdot 2^1 + \ldots + H(s)_{k-2}\cdot 2^{k-2}
\end{equation}
where $H(s)_j$ means the $j$'s bit of the image binary string $H(s)$ of the original binary hash function. 

A drawback of this hash function is that the distribution of the hash values in $\Z_r$ is not necessarily uniform. In fact if $r-2^{k-1}\neq 0$, then by design $H_{r.nbits()-1}$ will never hash onto values $z\geq 2^{k-1}$. Good moduli $r$ are therefore as close to $2^{k-1}$ as possible, why less good moduli are closer to $2^k$. In the worst case, that is $r=2^k-1$, it misses $2^{k-1}-1$, that is almost half of all elements, from $\Z_r$.
% TODO: DOUBLE CHECK THIS REASONING.

An advantage is that properties like preimage or collision resistence of the original hash function $H(\cdot)$ are preserved.
\begin{example} To give an implementation of the $H_{r.nbits()-1}$ hash function, we use a $5$-bit truncation of the $SHA256$ hash from example XXX and define a hash into $\Z_{16}$ by
$$
H_{16.nbits()-1}: \{0,1\}^* \to \Z_{16}:\; s\mapsto
SHA256(s)_0\cdot 2^0 + SHAH256(s)_1\cdot 2^1 + \ldots + SHA256(s)_4\cdot 2^4 
$$
Since $k=16.nbits()=5$ and $16-2^{k-1}=0$ this hash maps uniformly onto $\Z_{16}$. We can invoke sage to implement it e.g. like this:
\begin{sagecommandline}
sage: import Crypto 
sage: from Crypto.Hash import SHA256
sage: def Hash5(x):
....:     h = SHA256.new(x)
....:     d = h.hexdigest()
....:     d = ZZ(d, base=16)
....:     d = d.str(2)[-4:]
....:     return ZZ(d, base=2)
sage: Hash5('')
\end{sagecommandline}
We can then use sage to apply this function to a large set of input values in order to plot a visualization of the distribution over the set $\{0,\ldots,15\}$. Executing over $500$ input values gives:
\begin{sagesilent}
H1 = list_plot([Hash5(ZZ(i).str(2)) for i in range(500)])
\end{sagesilent}
\begin{center} 
\sageplot[scale=.5]{H1} 
\end{center}
To get an intuition of uniformity, we can count the number of times the hash function $H_{16.nbits()-1}$ maps onto each number in the set $\{0,1,\ldots,15\}$ in a loop of $100000$ hashes and compare that to the ideal uniform distribution, which would map exactly 6250 samples to each element. This gives the following result:
\begin{sagesilent}
arr = []
arr = [0 for i in range(16)]
for i in range(100000):
    arr[Hash5(ZZ(i).str(2))] +=1
H2 = list_plot(arr, ymin=0,ymax=10000)
\end{sagesilent}
\begin{center} 
\sageplot[scale=.5]{H2} 
\end{center}
The uniformity of the distribution problem becomes apparent if we want to construcht a similar hash function for $\Z_r$ for any $r$ in the range $17\leq r \leq 31$. In this case the definition of the hash function is exactly the same as for $\Z_{16}$ and hence the images will not exceed the value $16$. So for example in case of hashing to $\Z_{31}$ the hash function never maps to any value larger then $16$, leaving almost half of all numbers out of the image range.
\begin{sagesilent}
arr = []
arr = [0 for i in range(31)]
for i in range(100000):
    arr[Hash5(ZZ(i).str(2))] +=1
H3 = list_plot(arr, ymin=0,ymax=10000)
\end{sagesilent}
\begin{center} 
\sageplot[scale=.5]{H3} 
\end{center}
\end{example}
% We can do better thn this
 
The second widely used method of hashing into $\Z_r$ is constructed by observing, that if $r$ is a modulus, with a bit-length of $r.bits()=k_1$ and $H:\{0,1\}^*\to \{0,1\}^{k_2}$ is a hash function that produces digests of size $k_2$, with $k_2\geq k_1$, then a hash-to-group function can be constructed by interpreting the image of $H$ as binary representation of a integer and then take the modulus by $r$ to map into $\Z_r$. To be more precise 
\begin{equation}
H'_{mod_r}: \{0,1\}^* \to \Z_r: \; s \mapsto 
\Zmod{\left(H(s)_0\cdot 2^0 + H(s)_1\cdot 2^1 + \ldots + H(s)_{k_2}\cdot 2^{k_2}\right)}{n}
\end{equation}
where $H(s)_j$ means the $j$'s bit of the image binary string $H(s)$ of the original binary hash function. 

A drawback of this hash function is that computing the modulus requires some computational effort. In addition the distribution of the hash values in $\Z_r$ might not be even, depending on the difference $2^{k_2+1}-r$. An advantage is that potential properties like preimage or collision resistence of the original hash function $H(\cdot)$ are preserved and the distributen can be made almost uniform, with only neglectable bias, depending on what modulus $r$ and images size $k_2$ are choosen.
\begin{example} To give an implementation of the $H_{mod_r}$ hash function, we use  $k_2$-bit truncation of the $SHA256$ hash from example XXX and define a hashes into $\Z_{23}$ by
\begin{multline*}
H_{mod_{23},k_2}: \{0,1\}^* \to \Z_{23}:\; \\
s\mapsto
\Zmod{\left(SHA256(s)_0\cdot 2^0 + SHAH256(s)_1\cdot 2^1 + \ldots + SHA256(s)_{k_2}\cdot 2^{k_2}\right)}{23} 
\end{multline*}
We want to use various instantiations of $k_2$, to visualize the impact of truncation lenth on the distrubution of the hashes in $\Z_{23}$. We can invoke sage to implement it e.g. like this:
\begin{sagecommandline}
sage: import Crypto 
sage: from Crypto.Hash import SHA256
sage: Z23 = Integers(23)
sage: def Hash_mod23(x, k2):
....:     h = SHA256.new(x)
....:     d = h.hexdigest()
....:     d = ZZ(d, base=16)
....:     d = d.str(2)[-k2:]
....:     d = ZZ(d, base=2)
....:     return Z23(d)
\end{sagecommandline}
We can then use sage to apply this function to a large set of input values in order to plot visualizations of the distribution over the set $\{0,\ldots,22\}$ for various values of $k_2$ by counting the number of times it maps onto each number in a loop of $100000$ hashes. We get
\begin{sagesilent}
arr1 = []
arr1 = [0 for i in range(23)]
for i in range(100000):
    arr1[Hash_mod23(ZZ(i).str(2),5)] +=1
H3 = list_plot(arr1, ymin=0,ymax=10000,color='red', legend_label='k=5')
arr2 = []
arr2 = [0 for i in range(23)]
for i in range(100000):
    arr2[Hash_mod23(ZZ(i).str(2),7)] +=1
H4 = list_plot(arr2, ymin=0,ymax=10000,color='blue', legend_label='k=7')
arr3 = []
arr3 = [0 for i in range(23)]
for i in range(100000):
    arr3[Hash_mod23(ZZ(i).str(2),9)] +=1
H5 = list_plot(arr3, ymin=0,ymax=10000,color='yellow', legend_label='k=9')
arr4 = []
arr4 = [0 for i in range(23)]
for i in range(100000):
    arr4[Hash_mod23(ZZ(i).str(2),16)] +=1
H6 = list_plot(arr4, ymin=0,ymax=10000,color='black', legend_label='k=16')
\end{sagesilent}
\begin{center} 
\sageplot[scale=.6]{H3+H4+H5+H6} 
\end{center}
\end{example}
A third method that can sometimes be found in implementations is the so called \textit{try and increment method}. To underatand this method, we define an integer $z\in\Z$ from any hash value $H(s)$ as we did in the previous methods, that is we define $z = H(s)_0\cdot 2^0 + H(s)_1\cdot 2^1 + \ldots + H(s)_{k-1}\cdot 2^{k}$. 

Hashing into $\Z_r$ is then achievable by first computing $z$ and then try to see if $z\in\Z_r$. If this is the case than the hash is done and if not the string $s$ is modified in a deterministic way and the process is repeated until a sutable number $z$ is found. A suiteable, deterministic modification could be to concatenate the original string by some bit counter. A try and increment algorithm would then work like in algorithm XXX
\begin{algorithm}\caption{Hash-to-$\Z_n$}
\label{alg_projective_group_law}
\begin{algorithmic}[0]
\Require $r \in \Z$ with $r.nbits()=k$ and $s\in\{0,1\}^*$
\Procedure{Try-and-Increment}{$r,k,s$}
\State $c \gets 0$
\Repeat
\State $c\gets c+1$
\State $s' \gets s||c$
\State $z \gets H(s')_0\cdot 2^0 + H(s')_1\cdot 2^1 + \ldots + H(s')_{k}\cdot 2^{k}$
\Until{$z<r$}
\State \textbf{return} $x$
\EndProcedure
\Ensure $ z\in \Z_r$
\end{algorithmic}
\end{algorithm}

Depending on the parameters, this method can be very efficient. In fact, if $k$ is suficiently large and $r$ is close to $2^{k+1}$, the probability for $z<r$ is very high and the repeat loop will almost always be executed a single time only. A drawback is however that the probability to execute the loop multiple times is not zero. 

Once some hash function into modular arithmetics is found it can often be combined with additional techniques to hash into more general finite cyclif groups. The following paragraphs describes a few of thos methods widely adopted in snark development.
\paragraph{Pederson Hashes}
% T. P. Pedersen. “Non-interactive and information-theoretic secure verifiable secret shar- ing”. In: Annual International Cryptology Conference. Springer. 1991, pp. 129–140.
% https://fmouhart.epheme.re/Crypto-1617/TD08.pdf
The so called \textbf{Pedersen hash function} provides a way to map binary inputs of fixed size $k$ onto elements of finite cyclic groups, that avoids discrete log relations between the images as they occure in the naive approach XXX. Combining it with a classical hash function provides a hash function that maps strings of arbitrary lenth onto group elements. 

To be more pecise, let $j$ be an integer, $\G$ a finite cyclic group of order $r$ and $\{g_1, \ldots, g_j\} \subset \G$ a uniform randomly generated set of generators of $\G$. Then \textbf{Pedersen’s hash function} is defined as
\begin{equation}
H_{Ped} : \left(\Z_r\right)^j \to \G:\; (x_1,\ldots,x_j)\mapsto \Pi_{i=1}^j g_j^{x_j}
\end{equation}
It can be shown, that Pedersen’s  hash  function  is  collision-resistant under the assumption that $\G$ is a DL-A group. However it is important to note, that Pedersen hashes cannot be assumed to be pseudorandom and should therefore not be used where a hash function serves as an approximation of a random oracle.

From an implementation perspective, it is important to derive the set of generators $\{g_1,\ldots,g_j\}$ in such a way that they are as uniform and random as possible. In particular any known discrete log relation between two generators, that is, any known $x\in \Z_r$ with $g_h = (g_i)^x$ must be avoided.

To see how Pedersen hashes can be used to define an actual hash-to-group function according to our definition, we can use any of the hash-to-$\Z_r$ functions as we have derived them in XXX. 
%\begin{example} In this example, let $\G$ be a finite cyclic group of order $r$ and $\{g_1,\ldots,g_j\}$ a set of uniformly and randomly sampled set of generators of $\G$. To instantiate a Pedersen hash in combination with a hash-to-group function $H_{mod_r}$ as defined in XXX, let $k$ be the bit-length of the group order $r$ and $H:\{0,1\}^*\to \{0,1\}^{j\cdot k}$ some general hash function.

\paragraph{MimC Hashes}
% https://eprint.iacr.org/2016/492
\paragraph{Pseudo Random Functions in DDH-A groups}
% https://fmouhart.epheme.re/Crypto-1617/TD08.pdf 
As noted in XXX, Pederson's hash function does not have the properties a random function and should therefore not be instantiated as such. To look at a construction that serves as random oracle function in groups where the dicisional Diffi-Hellman construction is assumed to hold true let $\G$ be a DDH-A group of order $r$ with generator $g$ and $\{a_0,a_1,\ldots,a_k\}\subset \Z_{r}^*$ a uniform randomly generated set of numbers invertible in modular $r$ arithmetics. Then a pseudo-random function is given by
\begin{equation}
F_{rand}: \{0,1\}^{k+1} \to \G:\; (b_0,\ldots,b_k)\mapsto g^{b_0\cdot \Pi_{i=1}^k a_i^{b_i}}
\end{equation}
Of course if $H:\{0,1\}^*\to \{0,1\}^{k+1}$ is a random oracle, then the concation of $F_{rand}$ and $H$, defines a random oracle 
\begin{equation}
H_{rand,\G}:\{0,1\}^* \to \G:\; s\mapsto F_{rand}(H(s))
\end{equation}

%\begin{example}[p\&{}p-$\F_{13}$-mod-hash]
%Consider our pen\&paper hash function from XXX. We want to use this hash function, to define a $16$-bounded hash function that maps into the prime field $\F_{13}$. We define:
%$$\mathcal{H}_{mod}^{13}: \{0,1\}^{16}\to \F_{13}: S \mapsto \Zmod{\mathcal{H}_{PaP}(S)}{13}$$
%Considering the string $S=(1110011101110011)$ from example XXX again we know $\mathcal{H}_{PaP}(S)=(1110)$ and since $(1110)_{10}=14$ and $\Zmod{14}{13}=1$ we get $\mathcal{H}_{mod}^{13}(S)=1$.
%\end{example}

%\begin{example}[p\&{}p-$\F_{13}$-drop-hash]We can consider the same pen\&paper hash function from XXX and define another hash into $\F_{13}$, by deleting the first leading bit from the hash. The result is then a $3$-digit number and therefore guaranteed to be smaller then $13$, since $13$ is equal to $(1101)$ in base $2$.

%Considering the string $S=(1110011101110011)$ from example XXX again we know $\mathcal{H}_{PaP}(S)=(1110)$ and stripping of the leading bit we get $(110)_{10}=6$ as our hash value.

%As we can see this hash function has the drawback of an uneven distribution in $\F_{13}$. In fact this hash function is unable to map to values from $\{8,9,10,11,12\}$ as those numbers have a $1$-bit in position $4$. However as we will see in XXX, this hash is cheaper to implement as a circuit as no expensive modulus operation has to be used.
%\end{example}

\section{Commutative Rings}
Thinking of integers again, we know, that there are actually two operations addition and multiplication and as we know addition defines a group structure on the set of integers. However multiplication does not define a group structure as we know that integers in general don't have multiplicative inverses.

Combinations like this are captured by the concept of a so called \textit{commutative ring with unit}. To be more precise, a commutative ring with unit $ (R, +, \cdot, 1) $ is a set $R$, provided with two maps $ +: R \cdot R \to R $ and $ \cdot: R \cdot R \to R $, called \textit{addition} and \textit{multiplication}, such that the following conditions hold:
\begin{itemize}
\item $ \left (R, + \right) $ is a commutative group, where the neutral element is denoted  with $ 0 $.
\item (Commuativity of the multiplication) We have $r_1\cdot r_2 = r_2\cdot r_1$ for all $r_1, r_2\in R$.
\item (Existence of a unit) There is an element $1\in R$, such that $1\cdot g$ holds for all $g\in R$,
\item (Associativity) For every $g_1,g_2,g_3\in\G$ the equation
$g_1\cdot(g_2\cdot g_3) = (g_1\cdot g_2)\cdot g_3$ holds.
\item (Distributivity) For all $ g_1, g_2, g_3 \in R $ the distributive laws
$ g_1 \cdot \left (g_2 + g_3 \right) = g_1 \cdot g_2 + g_1 \cdot g_3$ holds.
\end{itemize}
\begin{example}[The Ring of Integers] The set $\Z$ of integers with the usual addition and multiplication is the archetypical example of a commutative ring with unit $1$.
\end{example}
\begin{example}[Underlying commutative group of a ring] Every commutative ring with unit $(R,+,\cdot,1)$ gives rise to group, if we just forget about the multiplication
\end{example}
The following example is more interesting. The motivated reader is encouraged to think through this example, not so much because we need this in what follows, but more so as it helps to detach the reader from familiar styles of computation.
\begin{example} Let $S:=\{\bullet,\star,\odot,\otimes\}$ be a set that contains four elements and let addition and multiplication on $S$ be defined as follows:
\begin{center}
  \begin{tabular}{c | c c c c c c}
    $\cup$ & $\bullet$ & $\star$ & $\odot$ & $\otimes$ \\\hline
    $\bullet$ & $\bullet$ & $\star$ & $\odot$ & $\otimes$ \\
    $\star$ & $\star$ & $\odot$ & $\otimes$ & $\bullet$ \\
    $\odot$ & $\odot$ & $\otimes$ & $\bullet$ & $\star$ \\
    $\otimes$ & $\otimes$ & $\bullet$ & $\star$ & $\odot$ \\
  \end{tabular} \quad \quad \quad \quad
  \begin{tabular}{c | c c c c c c}
$ \circ $ & $\bullet$ & $\star$ & $\odot$ & $\otimes$ & \\\hline
        $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ &\\
        $\star$ & $\bullet$ & $\star$ & $\odot$ & $\otimes$ &\\
        $\odot$ & $\bullet$ & $\odot$ & $\bullet$ & $\odot$ &\\
        $\otimes$ & $\bullet$ & $\otimes$ & $\odot$ & $\star$ &\\
  \end{tabular}
\end{center}
Then $(S,\cup,\circ)$ is a ring with unit $\star$ and zero $\bullet$. It therefore makes sense to ask for solutions to equations like this one:
Find $x\in S$ such that
$$
\otimes \circ (x \cup \odot ) = \star
$$
To see how such a "moonmath equation" can be solved, we have to keep in mind, that rings behaves mostly like normal number when it comes to bracketing and computation rules. The only differences are the symbols and the actual way to add and multiply. With this we solve the equation for $x$ in the "usual way"
\begin{align*}
\otimes \circ (x \cup \odot ) &= \star & \text{ \# aply the distributive law}\\
\otimes \circ x \cup \otimes \circ \odot  &= \star &\# \otimes \circ \odot = \odot\\
\otimes \circ x \cup \odot  &= \star & \text{\# concatenate the $\cup$ inverse of $\odot$ to both sides}\\
\otimes \circ x \cup \odot \cup -\odot  &= \star \cup -\odot & \# \odot \cup -\odot = \bullet\\
\otimes \circ x \cup \bullet &= \star \cup -\odot & \text{\# $\bullet$ is the $\cup$ neutral element}\\
\otimes \circ x &= \star \cup -\odot & \text{\# for $\cup$ we have $-\odot = \odot$} \\
\otimes \circ x &= \star \cup \odot &\# \star \cup \odot = \otimes \\
\otimes \circ x &= \otimes  &\text{\# concatenate the $\circ$ inverse of $\otimes$ to both sides}\\
(\otimes)^{-1}\circ \otimes \circ x &= (\otimes)^{-1}\circ \otimes & \text{\# multiply with the multiplicative inverse}\\
\star \circ x &= \star\\
x &= \star
\end{align*}
So even despite this equation looked really alien on the surface, computation was basically exactly the way "normal" equation like for fractional numbers are done.

Note however that in a ring, things can be very different, then most are used to, whenever a multiplicative inverse would be needed to solve an equation in the usual way. For example the equation
$$
\odot \circ x = \otimes
$$
can not be solved for $x$ in the usual way, since there is no multiplicative inverse for $\odot$ in our ring. And in fact looking at the multiplication table we see that no such $x$ exits. On another example the equation
$$
\odot \circ x = \odot
$$
can has not a single solution but two $x\in\{\star, \otimes\}$. Having no or two solutions is certainly not something to expect from types like $\mathbb{Q}$.
\end{example}
\begin{example} Considering polynomials again, we note from their definition, that what we have called the type $R$ of the coefficients, must in fact be a commutative ring with unit, since we need addition, multiplication, commutativity and the existence of a unit for $R[x]$ to have the properties we expect.

Now considering $R$ to be a ring, addition and multiplication of polynomials as defined in XXX, actually makes $R[x]$ into a commutative ring with unit, too, where the polynomial $1$ is the multiplicative unit.
\end{example}
\begin{example} Let $n$ be a modulus and $(\Z_n,+,\cdot)$ the set of all remainder classes of integers modulo $n$, with the projection of integer addition and multiplication as defined in XXX. It can be shown that $(\Z_n,+,\cdot)$ is a commutative ring with unit $1$.
\end{example}
Considering the exponential map from XXX again, let $\G$ be a finite cyclic group of order $n$ with generator $g\in\G$. Then the ring structure of $(\Z_n,+,\cdot)$ is mapped onto the group structure of $\G$ in the following way:
\begin{align*}
g^{x+y} &= g^x\cdot g^y & \text{for all } x,y\in\Z_n\\
g^{x\cdot y} &= \left( g^x\right)^y & \text{for all } x,y\in\Z_n
\end{align*}
This of particular interest in cryptographic and snarks, as it allows for the evaluation of polynomials with coefficients in $\Z_n$ to be evaluated "in the exponent". To be more precise let $p\in \Z_n[x]$ be a polynomial with $p(x)=a_m\cdot x^m+a_{m-1}x^{m-1}+\ldots + a_1x +a_0$. Then the previously defined exponential laws XXX imply that
\begin{align*}
g^{p(x)} & = g^{a_m\cdot x^m+a_{m-1}x^{m-1}+\ldots + a_1x +a_0}\\
         & = \left(g^{x^m}\right)^{a_m}\cdot \left(g^{x^{m-1}}\right)^{a_{m-1}}\cdot \ldots\cdot \left(g^{x}\right)^{a_1}\cdot g^{a_0}
\end{align*}
and hence to evaluate $p$ at some point $s$ in the exponent, we can insert $s$ into the right hand side of the last equation and evaluate the product.

As we will see this is a key insight to understand many snark protocols like e.g. Groth16 or XXX.
\begin{example} To give an example for the evaluation of a polynomial in the exponent of a finite cyclic group, consider the exponential map
$$
3^{(\cdot)}: \Z_4 \to \F_5^* \; x \mapsto 3^x
$$
from example XXX. Choosing the polynomial $p(x)= 2x^2 +3x +1$ from $\Z_4[x]$, we can evaluate the polynomial at say $x=2$ in the exponent of $3$ in two different ways. On the one hand side we can evaluate $p$ at $2$ and then write the result into the exponent, which gives
\begin{align*}
3^{p(2)} &=3^{2\cdot 2^2+3\cdot 2 +1}\\
          & = 3^{2\cdot 0 +2 +1}\\
          & = 3^{3}\\
          & = 2
\end{align*}
and on the other hand we can use the right hand side of equation to evaluate $p$ at $2$ in the exponent of $3$, which gives:
\begin{align*}
3^{p(2)} &= \left(3^{2^2}\right)^2 \cdot \left(3^{2}\right)^3\cdot 3^1\\
         &= \left(3^{0}\right)^2 \cdot 3^3\cdot 3\\
         &= 1^2 \cdot 2 \cdot 3\\
         &= 2 \cdot 3\\
         &= 2
\end{align*}
\end{example}
\paragraph{Hashing to Commutative Rings} As we have seen in XXX various constructions for hashing-to-groups are known and used in applications. As commutative rings are abelian groups, when we simply forget about the multiplicative structure, hash-to-group constructions can be applied for hashing into commutative rings, too. This is possible in general as the codomain of a general hash function $\{0,1\}^*$ is just the set of binary strings of arbitrary but finite lenth, which hash no algebraic structure that the hash function must respect.

\section{Fields}
In this chapter we started with the definition of a group, which we the expended into the definition of a commutative ring with unit. Those rings generalize the behaviour of integers. In this section we will look at the special case of commutative rings, where every element, other than the neutral element of addition, has a multiplicative inverse. Those structures behave very much like the rational numbers $\mathbb{Q}$, which are in a sense an extension of the ring of integers, that is constructed by just including newly defined multiplicative inverses (the fractions) to the integers.

Now considering the definition of a ring XXX again, we define a \textbf{field} $ (\F, +, \cdot) $ to be a set $ \F$, together with two maps $ +: \F \cdot \F \to \F $ and $ \cdot: \F \cdot \F \to \F $, called \textit{addition} and \textit{multiplication}, such that the following conditions holds
\begin{itemize}
\item $ \left (\F, + \right) $ is a commutative group, where the neutral element is denoted by $ 0 $.
\item $ \left (\F \setminus \left \{0 \right \}, \cdot \right) $ is a commutative group, where the neutral element is denoted by $ 1 $.
\item (Distributivity) For all $ g_1, g_2, g_3 \in \F $ the distributive law
$g_1 \cdot \left (g_2 + g_3 \right) = g_1 \cdot g_2 + g_1 \cdot g_3$ holds.
\end{itemize}
If a field is iven and the definition of its addition and multiplication is not ambiguous, we will often simple write $\F$ instead of $(\F,+,\cdot)$ to describe it. We moreover write $\F^*$ to describe the multiplicative group of the field, that is the set of elements, except the neutral element of addition, with the multiplication as group law.

The \textbf{characteristic} $char(\F)$ of a field $ \F $ is the smallest natural number $ n \geq 1 $, for which the $ n $ -fold sum of $ 1 $ equals zero, i.e. for which $ \sum_{i = 1} ^ n 1 = 0 $. If such a $ n> 0 $ exists, the field is also called to have a \textit{finite characteristic}. If, on the other hand, every finite sum of $1$ is not equal to zero, then the field is defined to have characteristic $ 0 $.
\begin{example}[Field of rational numbers] Probably the best known example of a field is the set of rational numbers $\mathbb{Q}$ together with the usual definition of addition, subtraction, multiplication and division. Since there is no counting number $n\in \N$, such that $\sum_{j=0}^n 1 =0$ in the rational numbers, the characteristic $char(\mathbb{Q})$ of the field $\mathbb{Q}$ is zero. In sage rational numbers are called like this
\begin{sagecommandline}
sage: QQ
sage: QQ(1/5) # Get an element from the field of rational numbers
sage: QQ(1/5) / QQ(3) # Division
\end{sagecommandline}
\end{example}
\begin{example}[Field with two elements] It can be shown that in any field, the neutral element $0$ of addition must be different from the neutral element $1$ of multiplication, that is we always have $0\neq 1$ in a field. From this follows that the smallest field must contain at least two elements and as the following addition and multiplication tables show, there is indeed a field with two elements, which is usually called $\F_2$:

Let $\F_2:=\{0,1 \}$ be a set that contains two elements and let addition and multiplication on $\F_2$ be defined as follows:
\begin{center}
  \begin{tabular}{c | c c c}
    + & 0 & 1 \\\hline
    0 & 0 & 1\\
    1 & 1 & 0 \\
  \end{tabular} \quad \quad \quad \quad
  \begin{tabular}{c | c c c}
$\cdot$ & 0 & 1 \\\hline
      0 & 0 & 0 \\
      1 & 0 & 1 \\
  \end{tabular}
\end{center}
Since $1+1=0$ in the field $\F_2$, we know that the characteristic of $\F_2$ is there, that is we have $char(\F_2)=0$.

For reasons we will understand better in XXX, sage defines this field as a so called Galois field with 2 elements. It is called like this:
\begin{sagecommandline}
sage: F2 = GF(2)
sage: F2(1) # Get an element from GF(2)
sage: F2(1) + F2(1) # Addition
sage: F2(1) / F2(1) # Division
\end{sagecommandline}
\end{example}
\begin{example}
Both the real numbers $\mathbb{R}$ as well as the complex numbers $\mathbb{C}$ are well known examples of fields.
\end{example}
\begin{exercise}
Consider our remainder class ring $(\F_5,+,\cdot)$ and show that it is a field. What is the characteristic of $\F_5$?
\end{exercise}
\paragraph{Prime fields}
As we have seen in the various examples of the previous sections, modular arithmetics behaves in many ways similar to ordinary arithmetics of integers, which is due to the fact that remainder class sets $\Z_n$ are commutative rings with units.

However at the same time we have seen in XXX, that, whenever the modulus is a prime number, every remainder class other then the zero class, has a modular multiplicative inverse. This is an important observation, since it immediately implies, that in case of a prime number, the remainder class set $\Z_n$ is not just a ring but actually a \textit{field}. Moreover since $\sum_{j=0}^n 1 = 0$ in $\Z_n$, we know that those fields have finite characteristic $n$

To distinguish this important case from arbitrary reminder class rings, we write  $ (\F_p, +, \cdot) $ for the field of all remainder classes for a prime number modulus $p \in \Prim$ and call it the \textbf{prime field} of characteristic $p$.

Prime fields are the foundation for many of the contemporary algebra based cryptographic systems, as they have many desirable properties. One of them is, that since these sets are finite and a prime field of characteristic $p$ can be represented on a computer in roughly $log_2(p)$ amount of space, no precision problems occur, that are for example unavoidable for computer representations of rational numbers or even the integers, because those sets are infinite.

Since prime fields are special cases of remainder class rings, all computations remain the same. Addition and multiplication can be computed by first doing normal integer addition and multiplication and then take the remainder modulus $p$. Subtraction and division can be computed by addition or multiplication with the additive or the multiplicative inverse, respectively. The additive inverse $-x$ of a field element $x\in\F_p$ is given by $p-x$ and the multiplicative inverse of $x\neq 0$ is given by $x^{p-2}$, or can be computed using the extended Euclidean algorithm.

Note however that these computations might not be the fastest to implement on a computer. They are however useful in this book as they are easy to compute for small prime numbers.
\begin{example}
The smallest field is the field $\F_2$ of characteristic $2$ as we have seen it in example XXX. It is the prime field of the prime number $2$.
\end{example}
\begin{example}
To summarize the basic aspects of computation in prime fields, lets consider the prime field $\F_5$ and simplify the following expression
$$\left(\frac{2}{3} - 2\right)\cdot 2 $$
A first thing to note is that since $\F_5$ is a field all rules like bracketing (distributivity), summing ect. are identical to the rules we learned in school when we where dealing with rational, real or complex numbers. We get
\begin{align*}
\left(\frac{2}{3} - 2\right)\cdot 2 &=
 \frac{2}{3}\cdot 2 - 2\cdot 2 & \text{\# distributive law}\\
 &= \frac{2\cdot 2}{3} - 2\cdot 2 & \Zmod{4}{5}=4 \\
 &= \frac{4}{3} - 4 & \text{\# multiplicative inverse of 3 is } \Zmod{3^{5-2}}{5}=2\\
 &= 4\cdot 2 - 4 & \text{\# additive inverse of 4 is } 5-4=1\\
 &= 4\cdot 2 +1 & \Zmod{8}{5}=3\\
 &= 3 +1 & \Zmod{4}{5}=4\\
 &= 4
\end{align*}
In this computation we computed the multiplicative inverse of $3$ using the identity
$x^{-1}=x^{p-2}$ in a prime field. This impractical for large prime numbers. Recall that another way of computing the multiplicative inverse is the Extended Euclidean algorithm.  To see that again, the task is to compute $x^{-1}\cdot 3 + t \cdot 5 =1$, but $t$ is actually irrelevant. We get
\begin{center}
  \begin{tabular}{c | c c l}
    k & $ r_k $ & $ x^{-1}_k $ & $ t_k = \Zdiv{(r_k-s_k \cdot a)}{b} $ \\\hline
    0 & 3 & 1 & $\cdot$\ \\
    1 & 5 & 0 & $\cdot$ \\
    2 & 3 & 1 & $\cdot$ \\
    3 & 2 &-1 & $\cdot$ \\
    4 & 1 & 2  & $\cdot$ \\
  \end{tabular}
\end{center}
So the multiplicative inverse of $3$ in $\Z_5$ is $2$ and indeed if compute $3\cdot 2$ we get $1$ in $\F_5$.
\end{example}
\paragraph{Square Roots}
In this part we deal with square numbers also called \textit{quadratic residues} and \textit{square roots} in prime fields. This is of particular importance in our studies on elliptic curves as only square numbers can actually be points on an elliptic curve.

To make the intuition of quadratic residues and roots precise, let $p \in \Prim $ be a prime number and $\F_p $ its associate prime field. Then a number $x\in \F_p$ is called a \textbf{square root} of another number $y\in\F_p$, if $x$ is a solution to the equation
\begin{equation}
x^2 = y
\end{equation}
In this case $y$ is called a \textbf{quadratic residue}. On the other hand, if $y$ is given and the quadratic equation has no $x$ solution, we call $ y $ as \textbf{quadratic non-residue}. For any $ y \in \F_p $ we write
\begin{equation}
\sqrt{y}: = \{x \in \F_p \; | \; x^2 = y \}
\end{equation}
for the set of all square roots of $ y $ in the prime field $ \F_n $. (If $ y $ is a quadratic non-residue, then $ \sqrt{y} = \emptyset $ and if $ y = 0 $, then $ \sqrt{y} = \{0 \} $)

So roughly speaking, quadratic residues are numbers such that we can take the square root from them and quadratic non-residues are numbers that don't have square roots. The situation therefore parallels the know case of integers, where some integers like $4$ or $9$ have square roots and others like $2$ or $3$ don't (as integers).

It can be shown that in any prime field every non zero element has either no square root or two of them. We adopt the convention to call the smaller one (when interpreted as an integer) as the \textbf{positive} square root and the larger one as the \textbf{negative}. This makes sense, as the larger one can always be computed as the modulus minus the smaller one, which is the definition of the negative in prime fields.


\begin{example} [Quadratic (Non)-Residues and roots in $ \F_5 $] Let us consider our example prime field $\F_5$ again. All square numbers can be found on the main diagonal of the multiplication table XXX. As you can see, in $ \Z_5 $ only the numbers $ 0 $, $ 1 $ and $ 4 $ have square roots and we get $ \sqrt{0} = \{0 \} $, $ \sqrt{1} = \{1,4 \} $, $ \sqrt{2} = \emptyset $, $ \sqrt{3} = \emptyset $ and $ \sqrt{4} = \{2,3 \} $. The numbers $0$, $1$ and $4$ are therefore quadratic residues, while the numbers $2$ and $3$ are quadratic non-residues.
\end{example}
In order to describe whether an element of a prime field is a square number  or not, the so called Legendre Symbol can sometimes be found in the literature, why we will recapitulate it here:

Let $ p \in \Prim $ be a prime number and $ y \in \F_p $ an element from the associated prime field. Then the so-called \textit{Legendre symbol} of $ y $ is defined as follows:
\begin{equation}
\label{eq: Legendre-symbol}
\left (\frac{y}{p} \right): =
\begin{cases}
1 & \text{if $ y $ has square roots} \\
-1 & \text{if $ y $ has no square roots} \\
0 & \text{if $ y = 0 $}
\end{cases}
\end{equation}
\begin{example}
Look at the quadratic residues and non residues in $\F_5$ from example XXX again, we can deduce the following Legendre symbols, from example XXX.
$$
\begin{array}{ccccc}
\left (\frac{0}{5} \right) = 0, &
\left (\frac{1}{5} \right) = 1, &
\left (\frac{2}{5} \right) = -1, &
\left (\frac{3}{5} \right) = -1, &
\left (\frac{4}{5} \right) = 1 \;.
\end{array}
$$
\end{example}
The legendre symbol gives a criterion to decide whether or not an element from a prime field has a quadratic root or not. This however is not just of theoretic use, as the following so called \textit{Euler criterion} gives a compact way to actually compute the Legendre symbol. To see that, let $ p \in \Prim_{\geq 3} $ be an odd
Prime number and $ y \in \F_p $. Then the Legendre symbol can be computed as
\begin{equation}
\label{eq: Euler_criterium}
\left (\frac{y}{p} \right) = y^{\frac{p-1}{2}} \;.
\end{equation}
\begin{example}
Look at the quadratic residues and non residues in $\F_5$ from example XXX again, we can compute the following Legendre symbols using the Euler criterium:
\begin{align*}
\left (\frac{0}{5} \right) &= 0^{\frac{5-1}{2}}= 0^2=0\\
\left (\frac{1}{5} \right) &= 1^{\frac{5-1}{2}}= 1^2=1\\
\left (\frac{2}{5} \right) &= 2^{\frac{5-1}{2}}= 2^2=4 = -1\\
\left (\frac{3}{5} \right) &= 3^{\frac{5-1}{2}}= 3^2=4 =-1\\
\left (\frac{4}{5} \right) &= 4^{\frac{5-1}{2}}= 4^2=1
\end{align*}
\end{example}
\begin{exercise} Consider the prime field $\F_{13}$. Find the set of all pairs $(x,y)\in \F_{13}\times \F_{13}$ that satisfy the equation
$$
x^2+y^2 = 1 + 7\cdot x^2\cdot y^2
$$
\end{exercise}
% I think this isn't needed. Will just leave it here in case this changes
%
%So the question remains how to actually compute square roots in prime field. The following algorithms give a solution
%\begin{definition}[Tonelli-Shanks algorithm]
%\label{def: Tonelli-Shanks}
%Let $ p $ be an odd prime number $ p \in \Prim _{\geq 3} $ and $ y $ a quadratic residue in $ \Z_p $. Then the so-called Tonneli \cite{TA} and Shanks \cite{SD} algorithm computes the two square roots of $ y $. It is defined as follows:
%\begin{enumerate}
%\item Find $ Q, S \in \Z $ with $ p-1 = Q \cdot 2 ^ S $ such that $ Q $ is odd.
%\item Find an arbitrary quadratic non-remainder $ z \in \Z_p $.
%\item
%\begin{algorithmic}
%\State $ \begin{array}{ccccc}
%M: = S, & c: = z ^ Q, & t: = y ^ Q, & R: = y ^{\frac{Q + 1}{2}}, & M, c, t, R \in \Z_p
%\end{array} $
%\While{$ t \neq 1 $}
%\State Find the smallest $ i $ with $ 0 <i <M $ and $ t ^{2 ^ i} = 1 $
%\State $ b: = c ^{2 ^{M-i-1}} $
%\State $ \begin{array}{ccccc}
%M: = i, & c: = b ^ 2, & t: = tb ^ 2, & R: = R \cdot b
%\end{array} $
%\EndWhile
%\end{algorithmic}
%The results are then the square roots $ r_1: = R $ and $ r_2: = p-R $ of $y$ in $\F_p$.
%\end{enumerate}
%\end{definition}

%\begin{remark}The algorithm (\ref{def: Tonelli-Shanks}) works in prime fields for any odd prime numbers. From a practical point of view, however, it is efficient only if the prime number is congruent to $ 1 $ modulo $ 4 $, since in the other case the formula from the proposition \ref{theorem: square_roots}, which can be calculated more quickly, can be used.\end{remark}
\paragraph{Exponentiation} TO APPEAR...
\paragraph{Hashing into Prime fields} 
An important problem in snark development is the ability to hash to (various subsets) of elliptic curves. As we will see in XXX those curves are often defined over prime fields and hashing to a curve then might start with hashing to the prime field. It is therefore of importance to understand who to hash into prime fields.

To understand it, note that in XXX we have looked at a few constructions of how to hash into the residue class rings $\Z_n$ for arbitrary $n>1$. As prime fiels are just special instances of those rings, all hashing into $\Z_n$ functions can be used for hashing into prime fields, too.
\paragraph{Extension Fields}
% references https://blog.plover.com/math/se/finite-fields.html
We defined prime fields in the previous section. They are the basic building blocks for cryptography in general and snarks in particular.

However as we will see in XX so called \textit{pairing based} snark systems are crucially dependent on group pairings XXX defined over the group of rational points of elliptic curves. For those pairings to be non-trivial the elliptic curve must not only be defined over a prime field but over a so called \textit{extension field} of a given prime field.

We therefore have to understand field extensions. To understand them first observe the field $\F'$ is called an \textit{extension} of a field $\F$, if $\F$ is a subfield of $\F'$, that is $\F$ is a subset of $\F'$ and restricting the addition and multiplication laws of $\F'$ to the subset $\F$ recovers the appropriate laws of $\F$.

Now it can be shown, that whenever $p\in \Prim$ is a prime and $m\in\N$ a natural number, then there is a field $\F_{p^m}$ with characteristic $p$ and $p^m$ elements, such that $\F_{p^m}$ is an extension field of the prime field $\F_p$.

Similar to how prime fields $\F_p$ are generated by starting with the ring of integers and then divide by a prime number $p$ and keep the remainder, prime field extensions $\F_{p^m}$ are generated by starting with the ring $\F_p[x]$ of polynomials and then divide them by an irreducible polynomial of degree $m$ and keep the remainder.

To be more precise let $P\in F_p[x]$ be an irreducible polynomial of degree $m$ with coefficients from the given prime field $\F_p$. Then the underlying set $\F_{p^m}$ of the extension field is given by the set of all polynomials with a degree less then $m$:
\begin{equation}
\F_{p^m}:=\{a_{m-1} x^{m-1}+a_{k-2}x^{k-2}+\ldots+a_1 x+a_0\;|\; a_i\in \F_p\}
\end{equation}
which can be shown to be the set of all remainders when dividing any polynomial $Q\in \F_p[x]$ by $P$. So elements of the extension field are polynomials of degree less than $m$. This is analog to how $\F_p$ is the set of all remainders, when dividing integers by $p$.

Addition in then inheritec from $\F_p[x]$, which means that addition on $\F_{p^m}$ is defined as normal addition of polynomials. To be more precise, we have
\begin{equation}
+:\; \F_{p^m}\times \F_{p^m} \to \F_{p^m}\; ,\; (\sum_{j=0}^m a_j x^j,\sum_{j=0}^m b_j x^j)\mapsto \sum_{j=0}^m (a_j+b_j) x^j
\end{equation}
and we can see that the neutral element is (the polynomial) $0$ and that the additive inverse is given by the polynomial with all negative coefficients.

Multiplication in inheritec from $\F_p[x]$, too, but we have to divide the result by our modulus polynomial $P$, whenever the degree of the resulting polynomial is equal or greater to $m$. To be more precise, we have
\begin{equation}
\cdot\; \F_{p^m}\times \F_{p^m} \to \F_{p^m}\; ,\; (\sum_{j=0}^m a_j x^j,\sum_{j=0}^m b_j x^j)\mapsto \Zmod{\left(\sum _{n = 0} ^{2m} \sum _{i = 0} ^{n}{a} _{i }{{b} _{n-i}}{x} ^{n}\right)}{P}
\end{equation}
and we can see that the neutral element is (the polynomial) $1$. It is however not obvious from this definition how the multiplicative inverse looks.

We can easily see from the definition of $\F_{p^m}$ that the field is of characteristic $p$, since the multiplicative neutral element $1$ is equivalent to the multiplicative element $1$ from the underlying prime field and hence $\sum_{j=0}^p 1=0$. Moreover $\F_{p^m}$ is finite and contains $p^m$ many elements, since elements are polynomials of degree $<m$ and every coefficient $a_j$ can have $p$ different values. In addition we see that the prime field $\F_p$ is a subfield of $\F_{p^m}$ that occurs, when we restrict the elements of $\F_{p}$ to polynomials of degree zero.

One key point is that the construction of $\F_{p^m}$ depends on the choice of an irreducible polynomial and in fact different choices will give different multiplication tables, since the remainders from dividing a product by $P$ will be different..

It can however be shown, that the fields for different choices of $P$ are isomorphic, which means that there is a one to one identification between all of them and hence from an abstract point of view they are the same thing. From an implementations point of view however some choices are better, because they allow for faster computations.

To summerize we have seen that when a prime field $\F_p$ is given then any field 
$\F_{p^m}$ constructed in the above manner is a field extension of $\F_p$. To be more general a field $\F_{p^{m_2}}$ is a field extension of a field $\F_{p^{m_1}}$, if and only if $m_1$ divides $m_2$ and from this we can deduces, that for any given fixed prime number, there are nested sequences of fields
\begin{equation}
\F_p \subset \F_{p^{m_1}} \subset \cdots \subset \F_{p^{m_k}}
\end{equation}
whenever the power $m_j$ divides the power $m_{j+1}$, such that $\F_{p^{m_j}}$ is a subfield of $\F_{p^{m_{j+1}}}$.

To get a more intuitive picture of that the following example will construct an extension field of the prime field $\F_3$ and we can see how $\F_3$ sits inside that extension field. 
\begin{example}[The Extension field $\F_{3^2}$]In (XXX) we have constructed the prime field $\F_3$. In this example we apply the definition (XXX) of a field extension to construct $\F_{3^2}$. We start by choosing an irreducibe polynomial of degree $2$ with coefficients in $\F_3$. We try
$P(t)=t^2+1$. Maybe the fastest way to show that $P$ is indeed irreducible is to just insert all elements from $\F_3$ to see if the result is never zero. WE compute
\begin{align*}
P(0) = 0^2+1 &= 1\\
P(1) = 1^2+1 &= 2\\
P(2) = 2^2+1 &=  1+1  = 2
\end{align*}
This implies, that $P$ is irreducible. The set $\F_{3^2}$ then contains all polynomials of degrees lower then two with coefficients in $\F_3$, which is precisely
$$
\F_{3^2}=\{0,1,2,t,t+1,t+2,2t,2t+1,2t+2\}
$$
So our extension field contains $9$ elements as expected. Addition is  defined as addition of polynomials. For example $(t+2) + (2t+2)= (1+2)t +(2+2)= 1$. Doing this computation for all elements give the following addition table
\begin{center}
  \begin{tabular}{c | c c c c c c c c c}
    + & 0    & 1    & 2    & t    & t+1  & t+2  & 2t   & 2t+1 & 2t+2 \\\hline
    0 & 0    & 1    & 2    & t    & t+1  & t+2  & 2t   & 2t+1 & 2t+2 \\
    1 & 1    & 2    & 0    & t+1  & t+2  & t    & 2t+1 & 2t+2 & 2t   \\
    2 & 2    & 0    & 1    & r+2  & t    & t+1  & 2t+2 & 2t   & 2t+1 \\
    t & t    & t+1  & t+2  & 2t   & 2t+1 & 2t+2 & 0    & 1    & 2    \\
  t+1 & t+1  & t+2  & t    & 2t+1 & 2t+2 & 2t   & 1    & 2    & 0    \\
  t+2 & t+2  & t    & t+1  & 2t+2 & 2t   & 2t+1 & 2    & 0    & 1    \\
   2t & 2t   & 2t+1 & 2t+2 & 0    & 1    & 2    & t    & t+1  & t+2  \\
 2t+1 & 2t+1 & 2t+2 & 2t   & 1    & 2    & 0    & t+1  & t+2  & t    \\
 2t+2 & 2t+2 & 2t   & 2t+1 & 2    & 0    & 1    & t+2  & t    & t+1
  \end{tabular}
\end{center}
As we can see, the group $(\F_3,+)$ is a subgroup of the group $(\F_{3^2},+)$, obtained by only considering the first three rows and columns of this table.

As it was the case in previous examples, we can use the table to deduce the negative of any element from $\F_{3^2}$. For example in $\F_{3^2}$ we have $-(2t+1)= t+2$, since $(2t+1) + (t+2)=0$

Multiplication needs a bit more computation, as we first have to multiply the polynomials and whenever the result has a degree $\geq 2$, we have to divide it by $P$ and keep the remainder. To see how this works compute the product of $t+2$ and $2t+2$ in $\F_{3^2}$
\begin{align*}
(t+2) \cdot (2t+2) &= \Zmod{(2t^2 + 2t + t + 1)}{(t^2+1)} \\
                   &= \Zmod{(2t^2+1)}{(t^2+1)} & \#\; 2t^2+1:t^2+1= 2 + \frac{2}{t^2+1} \\
                   &= 2
\end{align*}
So the product of $t+2$ and $2t+2$ in $\F_{3^2}$ is $2$. Doing this computation for all elements give the following multiplication table:
\begin{center}
  \begin{tabular}{c | c c c c c c c c c}
$\cdot$ & 0    & 1    & 2    & t    & t+1  & t+2  & 2t   & 2t+1 & 2t+2 \\\hline
      0 & 0    & 0    & 0    & 0    & 0    & 0    & 0    & 0    & 0 \\
      1 & 0    & 1    & 2    & t    & t+1  & t+2  & 2t   & 2t+1 & 2t+2\\
      2 & 0    & 2    & 1    & 2t   & 2t+2 & 2t+1 & t    & t+2  & t+1 \\
      t & 0    & t    & 2t   & 2    & t+2  & 2t+2 & 1    & t+1  & 2t+1  \\
    t+1 & 0    & t+1  & 2t+2 & t+2  & 2t   & 1    & 2t+1 & 2    & t   \\
    t+2 & 0    & t+2  & 2t+1 & 2t+2 & 1    & t    & t+1  & 2t   & 2    \\
     2t & 0    & 2t   & t    & 1    & 2t+1 & t+1  & 2  & 2t+2 & t+2\\
   2t+1 & 0    & 2t+1 & t+2  & t+1  & 2    & 2t   & 2t+2 & t    & 1    \\
   2t+2 & 0    & 2t+2 & t+1  & 2t+1 & t    & 2    & t+2  & 1     & 2t
  \end{tabular}
\end{center}
As it was the case in previous examples, we can use the table to deduce the multiplicative inverse of any non-zero element from $\F_{3^2}$. For example in $\F_{3^2}$ we have $(2t+1)^{-1}= 2t+2 $, since $(2t+1) \cdot (2t+2)=1$.

From the multiplication table we can also see, that the only quadratic residues in $\F_{3^2}$ are the set $\{0,1,2, t, 2t\}$, with
$\sqrt{0}=\{0\}$, $\sqrt{1}=\{1,2\}$, $\sqrt{2}=\{t, 2t\}$, $\sqrt{t}=\{t+2,2t+1\}$ and $\sqrt{2t}=\{t+1,2t+2\}$.

Since $\F_{3^2}$ is a field, we can solve equations as we would for other fields, like the rational numbers. To see that lets find all $x\in\F_{3^2}$ that solve the quadratic equation $(t+1)(x^2 + (2t+2)) = 2$. So we compute:
\begin{align*}
(t+1)(x^2 + (2t+2))    &= 2 &\text{\# 2 distributive law}\\
(t+1)x^2 + (t+1)(2t+2) &= 2 \\
(t+1)x^2 + (t)         &= 2 &\text{\# 2 add the additive inverse of $t$}\\
(t+1)x^2 + (t) + (2t)  &= (2) + (2t) \\
(t+1)x^2               &= 2t+2 & \text{\# multiply with the multiplicative invers of $t+1$}\\
(t+2)(t+1)x^2          &=(t+2)(2t+2) & \text{\# multiply with the multiplicative invers of $t+1$}\\
x^2                    &= 2 & \text{\# 2 is quadratic residue. Take the roots.}\\
x &\in \{t, 2t\}
\end{align*}
Computations in extension fields are arguably on the edge of what can reasonbly be done with pen and paper. Fortunately sage provides us with a simple way to do the computations.
\begin{sagecommandline}
sage: Z3 = GF(3) # prime field
sage: Z3t.<t> = Z3[] # polynomials over Z3
sage: P = Z3t(t^2+1)
sage: P.is_irreducible()
sage: F3_2.<t> = GF(3^2, name='t', modulus=P)
sage: F3_2
sage: F3_2(t+2)*F3_2(2*t+2) == F3_2(2)
sage: F3_2(2*t+2)^(-1) # multiplicative inverse
sage: # verify our solution to (t+1)(x^2 + (2t+2)) = 2
sage: F3_2(t+1)*(F3_2(t)**2 + F3_2(2*t+2)) == F3_2(2)
sage: F3_2(t+1)*(F3_2(2*t)**2 + F3_2(2*t+2)) == F3_2(2)
\end{sagecommandline}
\end{example}
\begin{exercise}
Consider the extension field $\F_{3^2}$ from the previous example and find all pairs of elements $(x,y)\in\F_{3^2}$, such that
$$
y^2 = x^3 + 4
$$
\end{exercise}
\begin{exercise} Show that the polynomial $P=x^3+x+1$ from $\F_5[x]$ is irreducible. Then consider the extension field $\F_{5^3}$ defined relative to $P$. Compute the multiplicative inverse of $(2t^2+4)\in\F_{5^3}$ using the extended Euklidean algorithm. Then find all $x\in\F_{5^3}$ that solve the  equation
$$(2t^2+4)(x-(t^2+4t+2))= (2t+3)$$
\end{exercise}
\paragraph{Hashing into extension fields} In XXX we have seen how to hash into prime fields. As elements of extension fields can be seen as polynomials over prime fields, hashing into extension fields is therefore possible, if every coefficient of the polynimial is hashed independently. 
\section{Projective Planes}
Projective planes are a certain type of geometry defined over some given field, that in a sense extend the concept of the ordinary Euclidean plane by including "points at infinity".

Such an inclusion of infinity points makes them particularly useful in the description of elliptic curves, as the description of such a curve in an ordinary plane needs an additional symbol "the point at infinity" to give the set of points on the curve the structure of a group. Translating the curve into projective geometry, then includes this "point at infinity" more naturally into the set of all points on a projective plane.

To understand the idea for the construction of projective planes, note that in
an ordinary Euclidean plane, two lines either intersect in a single point, or are parallel. In the latter case both lines are either the same, that is they intersect in all points, or do not intersect at all. A projective plane can then be thought of as an ordinary plane, but equipped with additional "points at infinity" such that two different lines always intersect in a single point. Parallel lines intersect "at infinity".

To be more precise, let $\F$ be a field, $\F^3:=\F\times \F\times F$ the set of all three tuples over $\F$ and $x\in \F^3$ with $x=(X,Y,Z)$. Then there is exactly one \textit{line} in $\F^3$ that intersects both $(0,0,0)$ and $x$. This line is given by
\begin{equation}
[X:Y:Z] := \{(k\cdot X,k\cdot Y, k\cdot Z)\;|\; k\in\F\}
\end{equation}
A \textit{point} in the \textbf{projective plane} over $\F$ is then defined as such a \textit{line} and the projective plane is the set of all such points, that is
\begin{equation}
\F\mathbb{P}^2:=\{[X:Y:Z]\;|\; (X,Y,Z)\in \F^3\text{ with } (X,Y,Z)\neq (0,0,0)\}
\end{equation}
It can be shown that a projective plane over a finite field $\F_{p^m}$ contains $p^{2m}+p^m+1$ many elements.

To understand why $[X:Y:Z]$ is called a line, consider the situation, where the underlying field $\F$ are the real numbers $\mathbb{R}$. Then $\mathbb{R}^3$ can be seen as the three dimensional space and $[X:Y:Z]$ is then an ordinary line in this 3-dimensional space that intersects zero and the point with coordinates $X$, $Y$ and $Z$.

The key observation here is, that points in the projective plane, are lines in the $3$-dimensional space $\F^3$, also for finite fields, the terms space and line share very little visual similarity with their counterparts over the real numbers.

It follows from this that points $[X:Y:Z]\in \F\mathbb{P}^2$ are not simply described by fixed coordinates $(X,Y,Z)$, but by \textit{sets of coordinates} rather, where two different coordinates $(X_1,Y_1,Z_1)$ and $(X_2,Y_2,Z_2)$, with describe the same point, if and only if there is some field element $k$, such that $(X_1,Y_1,Z_1) = (k\cdot X_2,k\cdot Y_2,k\cdot Z_2)$. Point $[X:Y:Z]$ are called \textbf{projective coordinates}.

\begin{notation}[Projective coordinates]
%https://math.mit.edu/classes/18.783/2017/Lecture1.pdf
Projective coordinates of the form $[X:Y:1]$ are descriptions of so called \textbf{affine points} and projective coordinates of the form $[X:Y:0]$ are descriptions of so called \textbf{points at infinity}. In particular the projective coordinate $[1:0:0]$ describes the so called \textbf{line at infinity}.
\end{notation}
\begin{example} Consider the field $\F_3$ from example XXX. As this field only contains, three elements it takes not to much effort to construct its associated projective plane $\F_3\mathbb{P}^2$, as we knwo that it only contain $13$ elemts.

To find $\F_3\mathbb{P}^2$, we have to compute the set of all lines in $\F_3\times \F_3\times \F_3$ that intersect $(0,0,0)$. Since those lines are parameterized by tuples $(x_1,x_2,x_3)$. We compute:
\begin{align*}
[0:0:1] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(0,0,1), (0,0,2)\}\\
[0:0:2] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(0,0,2), (0,0,1)\}
          = [0:0:1]\\
[0:1:0] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(0,1,0), (0,2,0)\}\\
[0:1:1] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(0,1,1), (0,2,2)\}\\
[0:1:2] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(0,1,2), (0,2,1)\}\\
[0:2:0] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(0,2,0), (0,1,0)\}
          = [0:1:0]\\
[0:2:1] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(0,2,1), (0,1,2)\}
          = [0:1:2]\\
[0:2:2] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(0,2,2), (0,1,1)\}
          = [0:1:1]\\
[1:0:0] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(1,0,0), (2,0,0)\}\\
[1:0:1] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(1,0,1), (2,0,2)\}\\
[1:0:2] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(1,0,2), (2,0,1)\}\\
[1:1:0] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(1,1,0), (2,2,0)\}\\
[1:1:1] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(1,1,1), (2,2,2)\}\\
[1:1:2]&= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(1,1,2), (2,2,1)\}\\
[1:2:0] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(1,2,0), (2,1,0)\}\\
[1:2:1] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(1,2,1), (2,1,2)\}\\
[1:2:2] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(1,2,2), (2,1,1)\}\\
[2:0:0] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(2,0,0), (1,0,0)\}
          = [1:0:0]\\
[2:0:1] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(2,0,1), (1,0,2)\}
          = [1:0:2]\\
[2:0:2] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(2,0,2), (1,0,1)\}
          = [1:0:1]\\
[2:1:0] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(2,1,0), (1,2,0)\}
          = [1:2:0]\\
[2:1:1] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(2,1,1), (1,2,2)\}
          = [1:2:2]\\
[2:1:2] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(2,1,2), (1,2,1)\}
          = [1:2:1]\\
[2:2:0] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(2,2,0), (1,1,0)\}
          = [1:1:0]\\
[2:2:1] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(2,2,1), (1,1,2)\}
          = [1:1:2]\\
[2:2:2] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(2,2,2), (1,1,1)\}
          = [1:1:1]
\end{align*}
Those lines define the $13$ points in the projective plane $\F_3\mathbb{P}$ as follows
\begin{multline*}
\F_3\mathbb{P} = \{ [0:0:1], [0:1:0], [0:1:1], [0:1:2], [1:0:0], [1:0:1], \\ [1:0:2], [1:1:0], [1:1:1], [1:1:2], [1:2:0], [1:2:1], [1:2:2]\}
\end{multline*}
This projective plane contains $9$ affine points, three points at infinity and one line at infinity.

To understand the ambiguity in projective coordinates a bit better, lets consider the point $[1:2:2]$. As this point in the projective plane is a line in $\F_3^3$, it has the projective coordinates $(1,2,2)$ as well as $(2,1,1)$, since the former coordinate give the latter, when multiplied in $\F_3$ by the factor $2$. In addition note, that for the same reasons the points $[1:2:2]$ and $[2:1:1]$ are the same, since their underlying sets are equal.
\end{example}
\begin{exercise}
Construct the so called \textit{Fano plane}, that is the projective plane over the finite field $\F_2$.
\end{exercise}
