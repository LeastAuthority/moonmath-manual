\chapter{Algebra}
\sme{Def Subgroup,
% https://en.wikipedia.org/wiki/Subgroups_of_cyclic_groups
Fundamental theorem of cyclic groups.}

In the previous chapter, we gave an introduction to the basic computational skills needed for a pen-and-paper approach to SNARKs. In this chapter, we get a bit more abstract and clarify a lot of mathematical terminology and jargon.

When you read papers about cryptography, or mathematical papers in general, you will frequently come across algebraic terms like \term{groups}, \term{fields}, \term{rings} and similar. To be able to follow these papers, it is necessary to get at least some understanding of such terms. In this chapter, we therefore provide a short introduction to these terms.

In a nutshell, algebraic types like groups or fields define sets that are analogous to numbers in various aspects, in the sense that you can add, subtract, multiply or divide on those sets.

We know many examples of sets that fall under those categories, such as natural numbers, integers,  rational or the real numbers. In some sense, these are the most fundamental examples of such sets.

\section{Groups}\label{sec:groups}
 Groups are abstractions that capture the essence of mathematical phenomena, like addition and subtraction, multiplication and division, permutations, or symmetries.

To understand groups, let us think back to when we learned about addition and subtraction of integers in school (putting integer multiplication aside for the moment). We learned that we can always add two integers, and that the result is guaranteed to be an integer again. We also learned that adding zero to any integer means that ``nothing happens'', that it doesn't matter in which order we add a given set of integers, that operations within brackets should be computed before operations outside brackets and that, for every integer, there is always another integer  such that when we add both together we get zero (the negative).

These conditions are the defining properties of a group, and mathematicians have recognized that the exact same set of rules can be found in very different mathematical structures. It therefore makes sense to give a formal definition of what a group should be, detached from any concrete example. This lets us handle entities of very different mathematical origins in a flexible way, while retaining essential structural aspects of many objects in abstract algebra and beyond.

Distilling these rules to the smallest independent list of properties and making them abstract, we arrive at the definition of a group:

A \term{group} $(\G,\cdot) $ is a set $ \G$, together with a map $ \cdot$. The map, also denoted as $\G \times \G \to \G $ and called the group law, combines two elements of the set $ \G$ into a third one such that the following properties hold:
\begin{itemize}
\item \hilight{Existence of a neutral element}: There is a $e\in\G$ for all $g\in\G$, such that $e\cdot g=g$ as well as $g\cdot e = g$.
\item \hilight{Existence of an inverse}: For every $g\in\G$ there is a $g^{-1}\in\G$, such that $g\cdot g^{-1}=e$ as well as $g^{-1}\cdot g = e$.
\item \hilight{Associativity}: For every $g_1,g_2,g_3\in\G$ the equation
$g_1\cdot(g_2\cdot g_3) = (g_1\cdot g_2)\cdot g_3$ holds.
\end{itemize}
Rephrasing the abstract definition in layman's terms, a group is something where we can do computations in a way that resembles the behaviour of the addition of integers. Specifically, this means we can combine some element with another element into a new element in a way that is reversible and where the order of combining elements doesn't matter.
\begin{notation}
Let $(\mathbb{G}\cdot)$ be a finite group. If there is no risk of ambiguity, we frequently drop the symbol $\cdot$ and simply write $\mathbb{G}$ as a notation for the group keeping the group law implicit.
\end{notation}
As we will see in XXX\sme{add reference when available}, groups are heavily used in cryptography and in SNARKs. But let us look at some more familiar examples fist:
\begin{example}[Integer Addition and Subtraction]
The set $(\Z,+)$ of integers together with integer addition is the archetypical example of a group, where the group law is traditionally written as $+$ (instead of $\cdot$). To compare integer addition against the abstract axioms of a group, we first see that the neutral element $e$ is the number $0$, since $a+0=a$ for all integers $a\in \Z$. Furthermore, the inverse of a number is its negative counterpart, since $a+(-a)=0$, for all $a\in\Z$. In addition, we know that $(a+b)+c=a+(b+c)$, so integers with addition are indeed a group in the abstract sense.
\end{example}
\begin{example}[The trivial group]
The most basic example of a group is group with just one element $\{\bullet\}$ and the group law $\bullet\cdot \bullet=\bullet$.
\end{example}
%\begin{example}[Rotations]
%To give an example of a group that has effects in the real world, consider a dice. Then our group is the set of all possible ways to rotate the dice by 90 degrees along an imagined axis through two opposite faces. The group law is composition of rotations. So say hold the dice with two fingers at $1$ and $6$. $1$ is the face that points towards you and $5$ is the top face. Then you first rotate the dice along the $1$-$6$ axis by 90 degrees clockwise, such that now $3$ is the top face. Then you hold the dice at $5$ and
%\end{example}
\paragraph{Commutative Groups} When we look at the general definition of a group, we see that it is somewhat different from what we know from integers. We know that the order in which we add two integers doesn't matter, as, for example, $4+2$ is the same as $2+4$. However we also know from example XXX\sme{add reference when available} that this is not always the case in groups.

To capture the special case of a group where the order in which the group law is executed doesn't matter, the concept of so-called a \term{commutative group} is introduced. To be more precise, a group is called commutative if  $g_1\cdot g_2 = g_2 \cdot g_1$ holds for all $g_1,g_2\in\G$.
\begin{notation}
In case $(\G,\cdot)$ is a commutative group, we frequently use the so-called \term{additive notation} $(\G,+)$, that is, we write $+$ instead of $\cdot$ for the group law and $-g:=g^{-1}$ for the inverse of an element $g\in\G$.
\end{notation}
\begin{example} Consider the group of integers with integer addition again.
Since $a+b=b+a$ for all integers, this group is the archetypical example of a commutative group. Since there are infinitely many integers, $(\Z,+)$ is not a finite group.
\end{example}
\begin{example} Consider our definition of modulo $6$ residue classes $(\Z_6,+)$ as defined in the addition table from example \ref{def_residue_ring_z_6}. As we can see, the residue class $0$ is the neutral element in modulo $6$ arithmetics, and the inverse of a residue class $r$ is given by $6-r$, since $r+(6-r)=6$, which is congruent to $0$, since $\Zmod{6}{6}=0$. Moreover $(r_1+r_2)+r_3=r_1+(r_2+r_3)$ is inherited from integer arithmetic.

We therefore see that $(\Z_6,+)$ is a group, and, since the addition table in example \ref{def_residue_ring_z_6} is symmetrical, we see $r_1+r_2 = r_2+r_1$ which shows that $(\Z_6,+)$ is commutative.
\end{example}
The previous example of a commutative groups is a very important one for this book. Abstracting from this example and considering residue classes $(\Z_n,+)$ for arbitrary moduli $n$, it can be shown that $(\Z,+)$ is a commutative group with the neutral element $0$ and the additive inverse $n-r$ for any element $r\in\Z_n$. We call such a group the \term{remainder class group} of modulus $n$.

Of particular importance for pairing-based cryptography in general and SNARKs in particular are so-called \term{pairing maps} on commutative groups. To be more precise let $\G_1$, $\G_2$ and $\G_3$ be three commutative groups. For historical reasons, we write the group law on $\G_1$ and $\G_2$ in additive notation and the group law on $\G_3$ in multiplicative notation. Then a \textbf{pairing map} is a function
\begin{equation}
e(\cdot,\cdot): \G_1 \times \G_2 \to \G_3
\end{equation}
that takes pairs $(g_1,g_2)$ (products) of elements from $\G_1$ and $\G_2$ and maps them somehow to elements from $\G_3$, such that the \textit{bilinearity} property holds: For all $g_1,g_1'\in \G_1$ and $g_2\in \G_2$ we have $e(g_1+ g_1',g_2)= e(g_1,g_2)\cdot e(g_1',g_2)$ and for all $g_1\in \G_1$ and $g_2, g_2'\in \G_2$ we have $e(g_1,g_2+ g_2')= e(g_1,g_2)\cdot e(g_1,g_2')$.

A pairing map is called \textit{non-degenerated}, if whenever the result of the pairing is the neutral element in $\G_3$, one of the input values must be the neutral element of $\G_1$ or $\G_2$. To be more precise $e(g_1,g_2)=e_{\G_3}$ implies $g_1=e_{\G_1}$ or $g_2=e_{\G_2}$.

So, roughly speaking, bilinearity means that it doesn't matter if we first execute the group law on any side and then apply the bilinear map, or if we first apply the bilinear map and then apply the group law. Moreover, non-degeneracy means that the result of the pairing is zero if and only if at least one of the input values is zero.
\begin{example}Maybe the most basic example of a non-degenerate pairing is obtained if we take $\G_1$, $\G_2$ and $\G_3$ all to be the groups of integers with addition $(\Z,+)$. Then the following map defines aa non-degenerate pairing:
$$
e(\cdot,\cdot): \Z \times \Z \to \Z \; (a,b)\mapsto a\cdot b
$$
Note that bilinearity follows from the distributive law of integers, since for $a,b,c\in \Z$, we have $e(a+b,c)=(a+b)\cdot c = a\cdot c + b\cdot c = e(a,c)+ e(b,c)$ and the same reasoning is true for the second argument.

To the see that $e(\cdot,\cdot)$ is non-degenrate, assume that $e(a,b)=0$. Then a$\cdot b =0$ implies that $a$ or $b$ must be zero.
\end{example}

\begin{exercise}\label{fstar} Consider example \sme{add reference}XXX again and let $\F_5^*$ be the set of all remainder classes from $\F_5$ without the class $0$. Then $\F_5^*=\{1,2,3,4\}$. Show that $(\F_5^*,\cdot)$ is a commutative group.
\end{exercise}
\begin{exercise} Generalizing the previous exercise, consider general moduli $n$ and let $\Z_n^*$ be the set of all remainder classes from $\Z_n$ without the class $0$. Then $\Z_n^*=\{1,2,\ldots,n-1\}$. Give a counter example to show that $(\Z^*_n,\cdot)$ is not a group in general.

Find a condition such that $(\Z^*_n,\cdot)$ is a commutative group, compute the neutral element, give a closed form for the inverse of any element and proof the commutative group axioms.
\end{exercise}
\begin{exercise} Consider the remainder class groups $(\Z_n,+)$ for some modulus $n$. Show that the map
$$
e(\cdot,\cdot): \Z_n \times \Z_n \to \Z_n \; (a,b)\mapsto a\cdot b
$$
is bilinear. Why is it not a pairing in general and what condition must be imposed on $n$, such that the map is a pairing?
\end{exercise}
\paragraph{Finite groups} As we have seen in the previous examples, groups can either contain infinitely many elements (such as integers) or finitely many elements (as for example the remainder class groups $(\Z_n,+)$). To capture this distinction, a group is called a \term{finite group} if the underlying set of elements is finite. In that case, the number of elements of that group is called its \term{order}.
\begin{notation}
Let $\mathbb{G}$ be a finite group. Then we frquently write $ord(\mathbb{G})$ or  $|\mathbb{G}|$ for the order of $\mathbb{G}$.
\end{notation}
\begin{example}\label{Zn}
Consider the remainder class groups $(\Z_6,+)$ from example \ref{def_residue_ring_z_6}\sme{check references to previous examples} and $(\F_5,+)$  from example \ref{primfield_z_5}, and the group $(\F_5^*,\cdot)$ from exercise \ref{fstar}. We can easily see that the order of $(\Z_6,+)$ is $6$, the order of $(\F_5,+)$ is 5 and the order of $(\F_5^*,\cdot)$ is $4$.

To be more general, considering arbitrary moduli $n$, we know from Euclidean division that the order of the remainder class group $(\Z_n,+)$ is $n$.
\end{example}
\begin{exercise}The \uterm{RSA crypto system} is based on a modulus $n$ that is typically the product of two prime numbers of \uterm{size $2048$-bits}. What is the approximate order of the \uterm{rainder class group} $(\Z_n,+)$ in this case?
\end{exercise}
\paragraph{Generators} These are sets of elements that can be used to generate the entire group by applying the group law repeatedly to these elements or their inverses only. Generators are of particular interest when working with groups.

Of course, every group $\G$ has a trivial set of generators, when we just consider every element of the group to be in the generator set. The more interesting question is to find the smallest possible set of generators for a given group. Of particular interest in this regard are groups that have a single generator, that is, there exists an element $g\in\G$ such that every other element from $\G$ can be computed by the repeated combination of $g$ and its inverse $g^{-1}$ only. Groups with a single generator are called \term{cyclic groups}.
\begin{example} The most basic example of a cyclic group is the group of integers $(\Z,+)$ with integer addition. $1$ is a single generator of $\Z$, since every integer can be obtained by repeatedly adding either $1$ or its inverse $-1$ to itself. For example
$-4$ is generated by $-1$, since $-4=-1+(-1)+(-1)+(-1)$.
\end{example}
\begin{example} Consider a modulus $n$ and the remainder class groups $(\Z_n,+)$ from example \sme{check reference}\ref{Zn}. These groups are cyclic, with generator $1$, since every other element of that group can be constructed by repeatedly adding the remainder class $1$ to itself. Since $\Z_n$ is also finite, we know that $(\Z_n,+)$ is a finite cyclic group of order $n$.
\end{example}
\begin{example} Let $p\in\P$ be prime number and $(\F_p^*,\cdot)$ the finite group from exercise XXX\sme{add reference}. Then $(\F_p^*,\cdot)$ is cyclic and every element $g\in\F_q^*$ is a generator.
\end{example}
\paragraph{The discrete Logarithm problem}
In cryptography in general, and in SNARK development in particular, we often do computations``in the exponent" of a generator. To see what this means, observe that when
$\G$ is a cyclic group of order $n$ and $g\in \G$ is a generator of $\G$, then there is a map, called the \textbf{exponential map} with respect to the generator $g$
\begin{equation}\label{exponentialmap}
g^{(\cdot)}: \Z_n \to \G\; x \mapsto g^x
\end{equation}
where $g^x$ means "multiply $g$ $x$-times by itself and $g^0=e_{\G}$. This map has the remarkable property that it maps the additive group law of the remainder class group $(\Z_n,+)$ in a one-to-one correspondence to the group law of $\G$.

To see this, first observe that, since $g^0:=e_{\G}$ by definition, the neutral element of $\Z_n$ is mapped to the neutral element of $\G$ and since $g^{x+y}=g^x\cdot g^y$, the map respects the group laws.

Since the exponential map respects the group law, it doesn't matter if we do our computation in $\Z_n$ before we write the result into the exponent of $g$ or afterwards. The result will be the same. This is what is usually meant by saying we do our computations "in the exponent".
\begin{example} Consider the multiplicative group $(\F_{5}^*,\cdot)$ from example \ref{fstar}\sme{check reference}. We know that $\F_{5}^*$ is a cyclic group of order $4$ and that every element is a generator. Choose $3\in\F_5^*$, we then know that the map
$$
3^{(\cdot)}: \Z_4 \to \F_5^* \; x \mapsto 3^x
$$
respects the group law of addition in $\Z_4$ and the group law of multiplication in $\F_5^*$.
And indeed doing a computation like
\begin{align*}
3^{2+3-2} &=3^{3}\\
          & = 2
\end{align*}
in the exponent gives the same result as doing the same computation in $\F*_5$, that is
\begin{align*}
3^{2+3-2} &= 3^2 \cdot 3^3 \cdot 3^{-2}\\
          &= 4\cdot 2 \cdot (-3)^2\\
          &= 3\cdot 2^2\\
          &= 3\cdot 4 \\
          &= 2
\end{align*}
\end{example}
Since the exponential map is a one-to-one correspondence that respects the group law, it can be shown that this map has an inverse which is called the \term{discrete logarithm} map with respect to the base $g$:
\begin{equation}
log_g(\cdot): \G \to \Z_n\; x \mapsto log_g(x)
\end{equation}
Discrete logarithms are highly important in cryptography as there are groups such that the exponential map and its inverse, the discrete logarithm, are believed to be one-way functions, that is, while it is possible to compute the exponential map in \uterm{polynomial time}, computing the discrete log takes (sub)-\uterm{exponential time}. We have discussed this briefly following example \ref{eq: primenumber_sequence} in the previous chapter, and will look at this and similar problems in more detail in the next section.


\sme{TODO: Fundamental theorem of finite cyclic groups}
% Cofactor clearing starts with aritrary group element and generates generators
% \begin{definition}[Cofactor Clearing]
%Since $BLS6-6(13)$ is a subgroup on our curve, it is not possible to leave the subgroup using the curves algebraic laws like scalar multiplication or addition. However in applications it often happens that random elements of the curve are generated, while what we really want are points in the subgroup. To get those points we can use cofactor clearing.
%\end{definition} 

\subsection{Cryptographic Groups} In this section, we will look at families of groups, which are believed to satisfy certain so-called \term{computational hardness assumptions}, that is, the hypothesis that a particular problem cannot be solved efficiently (where efficiently typically means ``in polynomial time of a given security parameter") in the groups under consideration. 
\begin{example} 
To highlight the concept of the computational hardness assumption, consider the group of integers $\Z$ from example \sme{check reference}\ref{eq: primenumber_sequence}. One of the best known and most researched examples of computational hardness is the assumption that the factorization of integers into prime numbers cannot be solved by any algorithm in polynomial time with respect to the bit-length of the integer.

To be more precise, the computational hardness assumption of integer factorization assumes that, given any integer $z\in\Z$ with bit-length $b$, there is no integer $k$ and no algorithm with the \uterm{run-time complexity} of $\mathcal{O}(b^k)$ that is able to find the prime numbers $p_1, p_2,\ldots,p_j\in\Prim$, such that $z=p_1\cdot p_2\cdot \ldots\cdot p_j$. 

This hardness assumption was proven to be false, since Shor's algorithm\sme{add reference} shows that integer factorization is at least \sme{S: what does "efficiently" mean here?}efficiently possible on a quantum computer, since the running time complexity of this algorithm is $\mathcal{O}(b^3)$. However, no such algorithm is known on a classical computer.

In the realm of classical computers, however, we still have to call the non-existence of such an algorithm an ``assumption" because, to date, there is no proof that it is actually impossible to find one. The problem is that it is hard to reason about algorithms that we don't know.

So, despite the fact that there is currently no known algorithm that can factor integers efficiently on a classical computer, we cannot exclude that such an algorithm might exist in principle, and that someone eventually will discover it in the future.

However, what still makes the assumption plausible, despite the absence of any actual proof, is the fact that, after decades of extensive search, still no such algorithm has been found.
\end{example}
In what follows, we will describe a few \uterm{computational hardness assumptions} that arise in the context of groups in cryptography, because we will refer to them throughout the book.
\paragraph{The discrete logarithm assumption} The so-called discrete logarithm problem is one of the most fundamental assumptions in cryptography. To define it, let $\G$ be a finite cyclic group of order $r$ and let $g$ be a generator of $\G$.  We know from \ref{exponentialmap}\sme{check reference} that there is a so-called exponential map 
$g^{(\cdot)}: \Z_r \to \G:\; x\mapsto g^x$, which maps the residue classes from module $r$ arithmetic onto the group in a $1:1$ correspondence. The \textbf{discrete logarithm problem} is then the task of finding inverses to this map, that is, to find a solution $x\in\Z_r$ to the following equation for some given $h\in\G$:
\begin{equation}
h = g^x
\end{equation}
In other words, the \term{discrete logarithm assumption (DL-A)} is the assumption that there exists no algorithm with running time polynomial in the``security parameter $log_2(r)$, that is able to compute some $x$ if only $h$, $g$ and $g^x$ are given in $\G$. If this is the case for $\G$, we call $\G$ a \term{DL-A group}.

Rephrasing the previous definition into simple words, DL-A groups are believed to have the property that it is infeasible to compute some number $x$ that solves the equation $h=g^x$ for a given $h$ and $g$, assuming that the size of the group $r$ is large enough.
\begin{example}[Public key cryptography]
One the most basic examples of an application for DL-A groups is in public key cryptography, where some pair $(\G,g)$ is publicly agreed on, such that $\G$ is a finite cyclic group of sufficiently large order $r$, where it is believed that the discrete logarithm assumption holds, and $g$ is a generator of $\G$.

In this setting, a secret key is nothing but some number $sk\in \Z_r$ and the associated public key $pk$ is the group element $pk=g^{sk}$. Since discrete logarithms are assumed to be hard, it is infeasible for an attacker to compute the secret key from the public key, since it is believed to be hard to find solutions $x$ to the equation 
$$
pk = g^{x}
$$ 
\end{example}
As the previous example shows, identifying DL-A groups is an important practical problem. Unfortunately, it is easy to see that it does not make sense to assume the hardness of the discrete logarithm problem in all finite cyclic groups: Counterexamples are common and easy to construct. 
\begin{example}[Modular arithmetics for Fermat's primes] It is widely believed that the discrete logarithm problem is hard in multiplicative groups $\Z_p^*$ of prime number modular arithmetics. However, this is not true in general. To see that, consider any so-called Fermat's prime, which is a prime number $p\in\Prim$, such that $p=2^n+1$ for some number $n$.

We know from XXX\sme{add reference} that in this case $\Z_p^* = \{1,2,\ldots, p-1\}$ is a group with respect to integer multiplication in modular $p$ arithmetics and since $p=2^n+1$, the order of $\Z_p^*$ is $2^n$, which implies that the associated security parameter is given by $log_2(2^n)=n$.

We show that, in this case, $\Z_p^*$ is not a DL-A group, by constructing an algorithm, which is able compute some $x\in\Z_{2^n}$ for any given generator $g$ and arbitrary element $h$ of $\F_p^*$, such that
$$
h = g^x
$$
holds and the running time complexity of the constructed algorithm is $\mathcal{O}(n^2)$, which is quadratic in the security parameter $n=log_2(2^n)$.  

To define such an algorithm, let us assume that the generator $g$ is a public constant and that a group element $h$ is given. Our task is to compute $x$ efficiently. 

The first thing to note is that, since $x$ is a number in modular $2^n$ arithmetic, we can write the binary representation of $x$ as
$$
x = c_0\cdot 2^0 + c_1\cdot 2^1 + \cdots + c_n \cdot 2^n
$$
with binary coefficients $c_j\in\{0,1\}$. In particular, $x$ is an $n$-bit number if interpreted as an integer.\sme{explain last sentence more}

We then use this representation to construct an algorithm that computes the bits $c_j$ one after another, starting at $c_0$. To see how this can be achieved, observe that we can determine $c_0$ by raising the input $h$ to the power of $2^{n-1}$ in $\F_p^*$. We use the exponential laws and compute 
\begin{align*}
h^{2^{n-1}} & = \left(g^x\right)^{2^{n-1}}\\
            & = \left(g^{c_0\cdot 2^0 + c_1\cdot 2^1 + \ldots + c_n\cdot 2^n}\right)^{2^{n-1}}\\
            & = g^{c_0\cdot 2^{n-1}}\cdot g^{c_1\cdot 2^1\cdot 2^{n-1}} \cdot 
            g^{c_2\cdot 2^2\cdot 2^{n-1}} \cdots g^{c_n\cdot 2^n\cdot 2^{n-1}}\\
            & = g^{c_0 2^{n-1}}\cdot g^{c_1\cdot 2^0\cdot 2^{n}} \cdot
            g^{c_2\cdot 2^1\cdot 2^{n}} \cdots g^{c_n\cdot 2^{n-1}\cdot 2^{n}}
\end{align*}
Now, since $g$ is a generator and $\F_p^*$ is cyclic of order $2^n$, we know $g^{2^n}=1$ and therefore $g^{k\cdot 2^n}= 1^k=1$. From this, it follows that all but the first factor in the last expression are equal to $1$ and we can simplify the expression into
$$
h^{2^{n-1}} = g^{c_0 2^{n-1}}
$$ 
Now, in case $c_0=0$, we get $h^{2^{n-1}} = g^0=1$ and in case $c_0=1$, we get 
$h^{2^{n-1}} = g^{2^{n-1}}\neq 1$ (To see that $g^{2^{n-1}}\neq 1$, recall that $g$ is a generator of $\F_p^*$ and hence is cyclic of order $2^n$, which implies $g^y\neq 1$ for all $y<2^n$).

So raising $h$ to the power of $2^{n-1}$ determines $c_0$, and we can apply the same reasoning to the coefficient $c_1$ by raising $h\cdot g^{-c_0\cdot 2^0}$ to the power of $2^{n-2}$. This approach can then be repeated until all the coefficients $c_j$ of $x$ are found.

Assuming that exponentiation in $\F_p^*$ can be done in logarithmic running time complexity $log(p)$, it follows that our algorithm has a running time complexity of 
$\mathcal{O}(log^2(p))=\mathcal{O}(n^2)$, since we have to execute $n$ exponentiations to determine the $n$ binary coefficients of $x$. 

From this, it follows that whenever $p$ is a Fermat's prime, the discrete logarithm assumption does not hold in $F_p^*$.
\end{example}
\paragraph{The decisional Diffie--Hellman assumption}
To describe the decisional Diffie--Hellman assumption, let $\G$ be a finite cyclic group of order $r$ and let $g$ be a generator of $\G$. The DDH assumption then assumes that there is no algorithm that has a polynomial running time complexity in the security parameter $s= log(r)$ that is able to distiguish the so-called DDH- triple $(g^a,g^b, g^{ab})$ from any triple $(g^a,g^b,g^c)$ for randomly and independently  chosen parameters $a,b,c\in \Z_r$. If this is the case for $\G$, we call $\G$ a \term{DDH-A group}.

It is easy to see that DDH-A is a stronger assumption than DL-A, in the sense that the discrete logarithm assumption is necessary for the decisional  Diffie--Hellman assumption to hold, but not the other way around. 

To see why, assume that the discrete logarithm assumption does not hold. In that case, given a generator $g$ and a group element $h$, it is easy to compute some residue class $x\in\Z_p$ with $h=g^x$. Then the decisional Diffie--Hellman assumption cannot hold, since given some triple ($ g^a , g^b , z )$, one could efficiently decide whether $z = g^{ab}$ by first computing the discrete logarithm $b$ of  $g^b$, then computing $g^{ab}= (g^a)^b$ and deciding whether or not $z=g^{ab}$.

On the other hand, the following example shows that there are groups where the discrete logarithm assumption holds but the decisional  Diffie--Hellman assumption does not hold:
\begin{example}[Efficiently computable pairings] Let $\G$ be a finite, cyclic group of order $r$ with generator $g$, such that the discrete logarithm assumtion holds and such that there is a pairing map $e(\cdot,\cdot): \G \times \G \to \G_T$ for some target group $\G_T$ that is computable in polynomial time of the parameter $log(r)$. 

In a setting like this, it is easy to show that DDH-A cannot hold, since given some  triple ($ g^a , g^b , z )$, it is possible to decide in polynomial time w.r.t $log(r)$ whether $z=g^{ab}$ or not. To see that check 
$$
e(g^a,g^b)=e(g,z)
$$Since the bilinierity properties of $e(\cdot,\cdot)$ imply $e(g^a,g^b)= e(g,g)^{ab}= e(g,g^{ab})$ and $e(g,y)=e(g,y')$ implies $y=y'$ due to the non degeneray property, the equality decides $z=e^{ab}$.
\end{example} 
It follows that DDH-A is indeed weaker than DL-A and groups with efficient pairings cannot be DDH-A groups. The following example shows another important class of groups where DDH-A does not hold: multiplicative groups of prime number residue classes.
\begin{example}Let $p$ be a prime number and $\Z_p^*=\{1,2,\ldots,p-1\}$ the multiplicative group of modular $p$ arithmetics as in example XXX\sme{add reference}. As we have seen in XXX, this group is finite and cyclic of order $p-1$ and every element $g\neq 1$ is a generator.

To see that $\F_p^*$ cannot be a DDH-A group, recall from XXX that the \uterm{Legendre symbol} $\left(\frac{x}{p}\right)$ of any $x\in \F_p^*$ is efficiently computable by \uterm{Euler's formular}.\sme{These are only explained later in the text, `\label{eq: Legendre-symbol}`} But the  Legendre symbol of $g^{a}$ reveals if $a$ is even or odd. Given $g^{a}$, $g^{b}$ and $g^{ab}$, one can thus efficiently compute and compare the least significant bit of $a$, $b$ and $a b$, respectively, which provides a probabilistic method to distinguish $g^{ab}$ from a random group element $g^c$. 
\end{example}
\paragraph{The computational  Diffie--Hellman assumption}
To describe the computational Diffie-Hellman assumption, let $\G$ be a finite cyclic group of order $r$ and let $g$ be a generator of $\G$. The computational Diffie--Hellman assumption assumes that, given randomly and independently  chosen residue classes $a,b\in\Z_r$, it is not possible to compute $g^{ab}$ if only $g$, $g^a$ and $g^b$ (but not $a$ and $b$) are known. If this is the case for $\G$, we call $\G$ a CDH-A group.

In general, it is not know if CDH-A is a stronger assumption then DL-A, or if both assumptions are equivalent. It is known that DL-A is necessary for CDH-A but the other direction is currently not well understood. In particular, there are no groups known where DL-A holds but CDH-A does not hold \citep{Fifield12theequivalence}.

To see why the discrete logarithm assumption is necessary, assume that it does not hold. So, given a generator $g$ and a group element $h$, it is easy to compute some residue class $x\in\Z_p$ with $h=g^x$. In that case, the computational Diffie--Hellman assumption cannot hold, since, given $g$, $g^a$ and $g^b$, one can efficiently compute $b$ and hence is able to compute $g^{ab}=(g^a)^b$ from this data.

The computational Diffie--Hellman assumption is a weaker assumption than the decisional  Diffie--Hellman assumption, which means that there are groups where CDH-A holds and DDH-A does not hold, while there cannot be groups such that DDH-A holds but CDH-A does not hold. To see that, assume that it is efficiently possible to compute $g^{ab}$ from $g$, $g^a$ and $g^b$. Then, given $(g^a,g^b,z)$ it is easy to decide if $z=g^{ab}$ or not. 

Several variations and special cases of the CDH-A exist. For example, the \term{square computational  Diffie--Hellman assumption} assumes that, given $g$ and $g^x$, it is computationally hard to compute $g^{x^2}$. The \term{inverse computational  Diffie--Hellman assumption} assumes that, given $g$ and $g^x$, it is computationally hard to compute $g^{x^{-1}}$. 

\paragraph{Cofactor Clearing}
\sme{TODO: theorem: every factor of order defines a subgroup...}
\subsection{Hashing to Groups}
% https://crypto.stackexchange.com/questions/78017/simple-hash-into-a-prime-field
\paragraph{Hash functions} Generally speaking, a hash function is any function that can be used to map data of arbitrary size to fixed-size values. Since binary strings of arbitrary length are a general way to represent arbitrary data, we can understand a general \textbf{hash function} as a map 
\begin{equation}
H: \{0,1\}^* \to \{0,1\}^k
\end{equation}
where $\{0,1\}^*$ represents the set of all binary strings of arbitrary but finite length and $\{0,1\}^k$ represents the set of all binary strings that have a length of exactly $k$ bits. So, in our definition, a hash function maps binary strings of arbitrary size onto binary strings of size exactly $k$. We call the images of $H$, that is, the values returned by the hash function \term{hash values}, \term{digests}, or simply \term{hashes}.

A hash function must be deterministic, that is, when we insert the same input $x$ into $H$, the image $H(x)$ must always be the same. In addition, a hash function should be as uniform as possible, which means that it should map input values as evenly as possible over its output range. In mathematical terms, every string of length $k$  from $\{0,1\}^k$ should be generated with roughly the same probability.  
\begin{example}[$k$-truncation hash]\label{ex:k-truncation-hash} One of the most basic hash functions 
$H_k:\{0,1\}^*\to \{0,1\}^k$ is given by simply truncating every binary string $s$ of size $s.len()> k$ to a string of size $k$ and by filling any string $s'$ of size $s'.len()<0$ with zeros. To make this hash function deterministic, we define that both truncation and filling should happen``on the left".

For example if $k=3$, $x_1=(0000101011101010011101010101)$ and $x_2=1$ then $H(x_1)=(101)$ and $H(x_2)=(001)$. It is easy to see that this hash function is deterministic and uniform.
\end{example}
Of particular interest are so-called \term{cryptographic} hash functions, which are hash functions that are also \term{one-way functions}, which essentially means that, given a string $y$ from $\{0,1\}^k$ it is practically infeasible to find a string $x\in\{0,1\}^*$ such that $H(x)=y$ holds. This property is usually called \term{preimage-resistance}. 

In addition, it should be infeasible to find two strings $x_1,x_2 \in\{0,1\}^*$, such that $H(x_1)=H(x_2)$, which is called \term{collision resistance}. It is important to note, though, that collisions always exist, since a function $H: \{0,1\}^* \to \{0,1\}^k$ inevitable maps infinitly many values onto the same hash. In fact, for any hash function with digests of length $k$, finding a preimage to a given digest can always be done using a brute force search in $2^k$ evaluation steps. It should just be practically impossible to compute those values and statistically very unlikely to generate two of them by chance. 

A third property of a cryptographic hash function is that small changes in the input string, like changing a single bit, should generate hash values that look completely different from each other.\sme{Is there a term for this property?}

As cryptographically secure hash functions map tiny changes in input values onto large change in the output, implementation errors that change the outcome are usually easy to spot by comparing them to expected output values. The definitions of cryptographically secure hash functions are therefore usually accompanied by some test vectors of common inputs and expected digests. Since the empty string $''$ is the only string of length $0$, a common test vector is the expected digest of the empty string.
\begin{example}[$k$-truncation hash] Considering the $k$-truncation hash from example \ref{ex:k-truncation-hash}. Since the empty string has length $0$ it follows that the digest of the empty string is string of length $k$ that only contains $0$'s: 
$$
H_k('')= (000\ldots 000)
$$
It is pretty obvious from the definition of $H_k$ that this simple hash function is not a cryptographic hash function. In particular, every digest is its own preimage, since $H_k(y)=y$ for every string of size exactly $k$. Finding preimages is therefore easy. 

In addition, it is easy to construct collusions as all strings of size $>k$ that share the same $k$-bits``on the right" are mapped to the same hash value.

Also, this hash function is not very chaotic, as changing bits that are not part of the $k$ right-most bits don't change the digest at all.  
\end{example}
Computing cryptographically secure hash functions in pen-and-paper style is possible but tedious. Fortunately, Sage can import the \term{PyCrypto} library, which is intended to provide a reliable and stable base for writing Python programs that require cryptographic functions. The following examples explain how to use PyCrypto in Sage.
\begin{example}\label{ex:SHA256}An example of a hash function that is generally believed to be a cryptographically secure hash function is the so-called \term{SHA256} hash, which in our notation is a function
$$
SHA256: \{0,1\}^* \to \{0,1\}^{256}
$$
that maps binary strings of arbitrary length onto binary strings of length $256$. To evaluate a proper implementation of the $SHA256$ hash function, the digest of the empty string is supposed to be
$$
SHA256('')= e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
$$
For better human readability, it is common practise to represent the digest of a string in a hexadecimal representation rather than in its binary form. We can use Sage to compute $SHA256$ and freely transit between binary, hexadecimal and decimal representations. To do so, we have to import PyCrypto and then load $SHA\_256$:\tbds{Check Sage code wrt local setup}
\begin{sagecommandline}
sage: import Crypto
sage: from Crypto.Hash import SHA256
sage: test = 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855'
sage: d = SHA256.new('')
sage: str = d.hexdigest()
sage: type(str)
sage: d = ZZ('0x'+ str) # conversion to integer type
sage: d.str(16) == str
sage: d.str(16) == test
sage: d.str(16)
sage: d.str(2)
sage: d.str(10)
\end{sagecommandline}
\end{example}

\paragraph{Hashing to cyclic groups} As we have seen in the previous paragraph, general hash functions map binary strings of arbitrary length onto binary strings of length $k$ for some parameter $k$. However, it is desirable in various cryptographic primitives to not simply hash to binary strings of fixed length but to hash into algebraic structures like groups, while keeping (some of) the properties like preimage resistance or collision resistance. 

Hash functions like this can be defined for various algebraic structures, but, in a sense, the most fundamental ones are hash functions that map into groups, because they are usually easily extended to map into other structures like rings or fields. 

To give a more precise definition, let $\G$ be a group and $\{0,1\}^*$ the set of all finite, binary strings, then a \term{hash-to-group} function is a deterministic map
\begin{equation}
H : \{0,1\}^* \to \G
\end{equation}
Common properties of hash functions, like uniformity, are desirable but not always realized in actual real world instantiations of hash-to-group functions, so we skip those requirements for now and keep the definition very general.

As the following example shows, hashing to finite cyclic groups can be trivially achieved for the price of some undesirable properties of the hash function:
\begin{example}[Naive cyclic group hash] Let $\G$ be a finite cyclic group. If the task is to implement a hash-to-$\G$ function, one immediate approach can be based on the observation that binary strings of size $k$, can be interpreted as integers $z\in\Z$ in the range $0\leq z < 2^k$. 

To be more precise, choose an ordinary hash function $H:\{0,1\}^*\to \{0,1\}^k$ for some parameter $k$ and a generator $g$ of $\G$. Then the expression
$$
z_{H(s)}= H(s)_0\cdot 2^0 + H(s)_1\cdot 2^1 + \ldots + H(s)_k \cdot 2^k
$$
is a positive integer, where $H(s)_j$ means the bit at the $j$-th position of $H(s)$. A hash-to-group function for the group $\G$ can then be defined as a concatenation of the exponential map $g^{(\cdot)}$ of $g$ with the interpretation of $H(s)$ as an integer: 
$$
H_{g} : \{0,1\}^* \to \G:\; s \mapsto g^{z_{H(s)}} 
$$
Constructing a hash-to-group function like this is easy to implement for cyclic groups and might be good enough in certain applications. It is however, almost never adequate in cryptographic applications, as discrete log relations might be constructible between two given hash value $H_g(s)$ and $H_g(t)$. 

To see that, assume that $\G$ is of order $r$ and that $z_{H(s)}$ has a multiplicative inverse in modular $r$ arithmetics. In that case, we can compute $x=z_{H(t)}\cdot z_{H(s)}^{-1}$ in $\Z_r$ and have found a discrete log relation between the group hash values, that is, we have found some $x$ with $H_g(t) = (H_g(s))^x$ since
\begin{align*}
H_g(t) & = (H_g(s))^x & \Leftrightarrow \\
g^{z_{H(t)}} & = g^{z_{H(s)}\cdot x} & \Leftrightarrow \\
g^{z_{H(t)}} & = g^{z_{H(t)}}
\end{align*}
\end{example}
Applications where discrete log relations between hash values are undesirable therefore need different approaches, and many of those approaches start with a way to hash into the sets $\Z_r$ of modular $r$ arithmetics. 
\paragraph{Hashing to modular arithmetics} One of the most widely used applications of hash-into-group functions are hash functions that map into the set $\Z_r$ of modular $r$ arithmetics for some modulus $r$. Different approaches to construct such a function are known, but probably the most widely used ones are based on the insight that the images of arbitrary hash functions can be interpreted as binary representations of integers as explained in example XXX\sme{add reference}.

It follows from this interpretation that one simple method of hashing into $\Z_r$ is constructed by observing that if $r$ is a modulus with a bit-length of $k=r.nbits()$, then every binary string $(b_0,b_1,\ldots,b_{k-2})$ of length $k-1$ defines an integer $z$ in the rage $0\leq z < 2^{k-1}\leq r $, by defining
\begin{equation}
z = b_0\cdot 2^0 + b_1\cdot 2^1 + \ldots + b_{k-2}\cdot 2^{k-2}
\end{equation}
Now since $z<r$, we know that $z$ is guranteed to be in the set $\{0,1,\ldots,r-1\}$ and hence can be interpreted as an element of $\Z_r$. From this, it follows that if $H:\{0,1\}^*\to\{0,1\}^{k-1}$ is a hash function, then a hash-to-group function can be constructed by
\begin{equation}
H_{r.nbits()-1}: \{0,1\}^* \to \Z_r: \; s \mapsto 
H(s)_0\cdot 2^0 + H(s)_1\cdot 2^1 + \ldots + H(s)_{k-2}\cdot 2^{k-2}
\end{equation}
where $H(s)_j$ means the $j$-th bit of the image binary string $H(s)$ of the original binary hash function. 

A drawback of this hash function is that the distribution of the hash values in $\Z_r$ is not necessarily uniform. In fact, if $r-2^{k-1}\neq 0$, then by design $H_{r.nbits()-1}$ will never hash onto values $z\geq 2^{k-1}$. Good moduli $r$ are therefore as close to $2^{k-1}$ as possible, while less good moduli are closer to $2^k$. In the worst case, that is, $r=2^k-1$, it misses $2^{k-1}-1$, that is almost half of all elements, from $\Z_r$.
\sme{TODO: DOUBLE CHECK THIS REASONING.}

An advantage is that properties like preimage resistance or collision resistance of the original hash function $H(\cdot)$ are preserved.
\begin{example} To give an implementation of the $H_{r.nbits()-1}$ hash function, we use a $5$-bit truncation of the $SHA256$ hash from example \ref{ex:SHA256} and define a hash into $\Z_{16}$ by
$$
H_{16.nbits()-1}: \{0,1\}^* \to \Z_{16}:\; s\mapsto
SHA256(s)_0\cdot 2^0 + SHAH256(s)_1\cdot 2^1 + \ldots + SHA256(s)_4\cdot 2^4 
$$
Since $k=16.nbits()=5$ and $16-2^{k-1}=0$, this hash maps uniformly onto $\Z_{16}$. We can invoke Sage to implement it e.g. like this:\tbds{Check Sage code wrt local setup}
\begin{sagecommandline}
sage: import Crypto 
sage: from Crypto.Hash import SHA256
sage: def Hash5(x):
....:     h = SHA256.new(x)
....:     d = h.hexdigest()
....:     d = ZZ(d, base=16)
....:     d = d.str(2)[-4:]
....:     return ZZ(d, base=2)
sage: Hash5('')
\end{sagecommandline}
We can then use sage to apply this function to a large set of input values in order to plot a visualization of the distribution over the set $\{0,\ldots,15\}$. Executing over $500$ input values gives:
\begin{sagesilent}
H1 = list_plot([Hash5(ZZ(k).str(2).encode('utf-8')) for k in range(500)])
\end{sagesilent}
\begin{center} 
\sageplot[scale=.5]{H1} 
\end{center}
To get an intuition of uniformity, we can count the number of times the hash function $H_{16.nbits()-1}$ maps onto each number in the set $\{0,1,\ldots,15\}$ in a loop of $100000$ hashes and compare that to the ideal uniform distribution, which would map exactly 6250 samples to each element. This gives the following result:
\begin{sagesilent}
arr = []
arr = [0 for i in range(16)]
for i in range(100000):
    arr[Hash5(ZZ(i).str(2).encode('utf-8'))] +=1
H2 = list_plot(arr, ymin=0,ymax=10000)
\end{sagesilent}
\begin{center} 
\sageplot[scale=.5]{H2} 
\end{center}
The uniformity of the distribution problem becomes apparent if we want to construcht a similar hash function for $\Z_r$ for any $r$ in the range $17\leq r \leq 31$. In this case, the definition of the hash function is exactly the same as for $\Z_{16}$ and hence the images will not exceed the value $16$. So, for example, in the case of hashing to $\Z_{31}$, the hash function never maps to any value larger than $16$, leaving almost half of all numbers out of the image range.
\begin{sagesilent}
arr = []
arr = [0 for i in range(31)]
for i in range(100000):
    arr[Hash5(ZZ(i).str(2).encode('utf-8'))] +=1
H3 = list_plot(arr, ymin=0,ymax=10000)
\end{sagesilent}
\begin{center} 
\sageplot[scale=.5]{H3} 
\end{center}
\end{example}
\sme{Mirco: We can do better than this}
 
The second widely used method of hashing into $\Z_r$ is constructed by observing that if $r$ is a modulus with a bit-length of $r.bits()=k_1$ and $H:\{0,1\}^*\to \{0,1\}^{k_2}$ is a hash function that produces digests of size $k_2$, with $k_2\geq k_1$, then a hash-to-group function can be constructed by interpreting the image of $H$ as a binary representation of a integer and then taking the modulus by $r$ to map into $\Z_r$. To be more precise 
\begin{equation}
H'_{mod_r}: \{0,1\}^* \to \Z_r: \; s \mapsto 
\Zmod{\left(H(s)_0\cdot 2^0 + H(s)_1\cdot 2^1 + \ldots + H(s)_{k_2}\cdot 2^{k_2}\right)}{n}
\end{equation}
where $H(s)_j$ means the $j$'th bit of the image binary string $H(s)$ of the original binary hash function. 

A drawback of this hash function is that computing the modulus requires some computational effort. In addition, the distribution of the hash values in $\Z_r$ might not be even, depending on the difference $2^{k_2+1}-r$. An advantage is that potential properties like preimage resistance or collision resistance of the original hash function $H(\cdot)$ are preserved and the distribution can be made almost uniform, with only negligible bias, depending on what modulus $r$ and images size $k_2$ are  chosen.
\begin{example} To give an implementation of the $H_{mod_r}$ hash function, we use  $k_2$-bit truncation of the $SHA256$ hash from example \ref{ex:SHA256} and define a hash into $\Z_{23}$ as follows:
\begin{multline*}
H_{mod_{23},k_2}: \{0,1\}^* \to \Z_{23}:\; \\
s\mapsto
\Zmod{\left(SHA256(s)_0\cdot 2^0 + SHAH256(s)_1\cdot 2^1 + \ldots + SHA256(s)_{k_2}\cdot 2^{k_2}\right)}{23} 
\end{multline*}
We want to use various instantiations of $k_2$ to visualize the impact of truncation length on the distribution of the hashes in $\Z_{23}$. We can invoke sage to implement it e.g. like this:
\begin{sagecommandline}
sage: import Crypto 
sage: from Crypto.Hash import SHA256
sage: Z23 = Integers(23)
sage: def Hash_mod23(x, k2):
....:     h = SHA256.new(x.encode('utf-8'))
....:     d = h.hexdigest()
....:     d = ZZ(d, base=16)
....:     d = d.str(2)[-k2:]
....:     d = ZZ(d, base=2)
....:     return Z23(d)
\end{sagecommandline}
We can then use Sage to apply this function to a large set of input values in order to plot visualizations of the distribution over the set $\{0,\ldots,22\}$ for various values of $k_2$ by counting the number of times it maps onto each number in a loop of $100000$ hashes. We get
\begin{sagesilent}
arr1 = []
arr1 = [0 for i in range(23)]
for i in range(100000):
    arr1[Hash_mod23(ZZ(i).str(2),5)] +=1
H3 = list_plot(arr1, ymin=0,ymax=10000,color='red', legend_label='k=5')
arr2 = []
arr2 = [0 for i in range(23)]
for i in range(100000):
    arr2[Hash_mod23(ZZ(i).str(2),7)] +=1
H4 = list_plot(arr2, ymin=0,ymax=10000,color='blue', legend_label='k=7')
arr3 = []
arr3 = [0 for i in range(23)]
for i in range(100000):
    arr3[Hash_mod23(ZZ(i).str(2),9)] +=1
H5 = list_plot(arr3, ymin=0,ymax=10000,color='yellow', legend_label='k=9')
arr4 = []
arr4 = [0 for i in range(23)]
for i in range(100000):
    arr4[Hash_mod23(ZZ(i).str(2),16)] +=1
H6 = list_plot(arr4, ymin=0,ymax=10000,color='black', legend_label='k=16')
\end{sagesilent}
\begin{center} 
\sageplot[scale=.6]{H3+H4+H5+H6} 
\end{center}
\end{example}
A third method that can sometimes be found in implementations is the so-called \term{``try-and-increment'' method}. To understand this method, we define an integer $z\in\Z$ from any hash value $H(s)$ as we did in the previous methods, that is, we define $z = H(s)_0\cdot 2^0 + H(s)_1\cdot 2^1 + \ldots + H(s)_{k-1}\cdot 2^{k}$. 

Hashing into $\Z_r$ is then achievable by first computing $z$, and then trying to see if $z\in\Z_r$. If it is, then the hash is done; if not, the string $s$ is modified in a deterministic way and the process is repeated until a suitable number $z$ is found. A suitable, deterministic modification could be to concatenate the original string by some bit counter. A ``try-and-increment'' algorithm would then work like in algorithm \ref{alg_try_and_increment}.\sme{check reference}
\begin{algorithm}\caption{Hash-to-$\Z_n$}
\label{alg_try_and_increment}
\begin{algorithmic}[0]
\Require $r \in \Z$ with $r.nbits()=k$ and $s\in\{0,1\}^*$
\Procedure{Try-and-Increment}{$r,k,s$}
\State $c \gets 0$
\Repeat
\State $s' \gets s||c\_bits()$
\State $z \gets H(s')_0\cdot 2^0 + H(s')_1\cdot 2^1 + \ldots + H(s')_{k}\cdot 2^{k}$
\State $c\gets c+1$
\Until{$z<r$}
\State \textbf{return} $x$
\EndProcedure
\Ensure $ z\in \Z_r$
\end{algorithmic}
\end{algorithm}

Depending on the parameters, this method can be very efficient. In fact, if $k$ is suficiently large and $r$ is close to $2^{k+1}$, the probability for $z<r$ is very high and the repeat loop will almost always be executed a single time only. A drawback is, however, that the probability of having to execute the loop multiple times is not zero. 

Once some hash function into modular arithmetics is found, it can often be combined with additional techniques to hash into more general finite cyclic groups. The following paragraphs describes a few of those methods widely adopted in SNARK development.
\paragraph{Pedersen Hashes}

The so-called \term{Pedersen hash function} \citep{Pedersen92} provides a way to map binary inputs of fixed size $k$ onto elements of finite cyclic groups that avoids discrete log relations between the images as they occur in the naive approach XXX\sme{add reference}. Combining it with a classical hash function provides a hash function that maps strings of arbitrary length onto group elements. 

To be more precise, let $j$ be an integer, $\G$ a finite cyclic group of order $r$ and $\{g_1, \ldots, g_j\} \subset \G$ a uniform randomly generated set of generators of $\G$. Then \term{Pedersens hash function} is defined as
\begin{equation}
H_{Ped} : \left(\Z_r\right)^j \to \G:\; (x_1,\ldots,x_j)\mapsto \Pi_{i=1}^j g_j^{x_j}
\end{equation}
It can be shown that Pedersens  hash  function  is  collision-resistant under the assumption that $\G$ is a DL-A group. However, it is important to note that Pedersen hashes cannot be assumed to be \term{pseudorandom} and should therefore not be used where a hash function serves as an approximation of a random \term{oracle}.

From an implementation perspective, it is important to derive the set of generators $\{g_1,\ldots,g_j\}$ in such a way that they are as uniform and random as possible. In particular, any known discrete log relation between two generators, that is, any known $x\in \Z_r$ with $g_h = (g_i)^x$ must be avoided.

To see how Pedersen hashes can be used to define an actual hash-to-group function according to our definition, we can use any of the hash-to-$\Z_r$ functions as we have derived them in XXX\sme{add reference}. 
%\begin{example} In this example, let $\G$ be a finite cyclic group of order $r$ and $\{g_1,\ldots,g_j\}$ a set of uniformly and randomly sampled set of generators of $\G$. To instantiate a Pedersen hash in combination with a hash-to-group function $H_{mod_r}$ as defined in XXX, let $k$ be the bit-length of the group order $r$ and $H:\{0,1\}^*\to \{0,1\}^{j\cdot k}$ some general hash function.

\paragraph{MimC Hashes}
\citep{cryptoeprint:2016:492}

\paragraph{Pseudo Random Functions in DDH-A groups}
% https://fmouhart.epheme.re/Crypto-1617/TD08.pdf 

As noted in XXX, Pederson's hash function does not have the properties a random function and should therefore not be instantiated as such. To look at a construction that serves as a random oracle function in groups where the decisional Diffie--Hellman construction is assumed to hold true, let $\G$ be a DDH-A group of order $r$ with generator $g$ and $\{a_0,a_1,\ldots,a_k\}\subset \Z_{r}^*$ a uniform randomly generated set of numbers invertible in modular $r$ arithmetics. Then a pseudo-random function is given by
\begin{equation}
F_{rand}: \{0,1\}^{k+1} \to \G:\; (b_0,\ldots,b_k)\mapsto g^{b_0\cdot \Pi_{i=1}^k a_i^{b_i}}
\end{equation}
Of course, if $H:\{0,1\}^*\to \{0,1\}^{k+1}$ is a random oracle, then the concatenation of $F_{rand}$ and $H$, defines a random oracle 
\begin{equation}
H_{rand,\G}:\{0,1\}^* \to \G:\; s\mapsto F_{rand}(H(s))
\end{equation}

%\begin{example}[p\&{}p-$\F_{13}$-mod-hash]
%Consider our pen\&paper hash function from XXX. We want to use this hash function, to define a $16$-bounded hash function that maps into the prime field $\F_{13}$. We define:
%$$\mathcal{H}_{mod}^{13}: \{0,1\}^{16}\to \F_{13}: S \mapsto \Zmod{\mathcal{H}_{PaP}(S)}{13}$$
%Considering the string $S=(1110011101110011)$ from example XXX again we know $\mathcal{H}_{PaP}(S)=(1110)$ and since $(1110)_{10}=14$ and $\Zmod{14}{13}=1$ we get $\mathcal{H}_{mod}^{13}(S)=1$.
%\end{example}

%\begin{example}[p\&{}p-$\F_{13}$-drop-hash]We can consider the same pen\&paper hash function from XXX and define another hash into $\F_{13}$, by deleting the first leading bit from the hash. The result is then a $3$-digit number and therefore guaranteed to be smaller then $13$, since $13$ is equal to $(1101)$ in base $2$.

%Considering the string $S=(1110011101110011)$ from example XXX again we know $\mathcal{H}_{PaP}(S)=(1110)$ and stripping of the leading bit we get $(110)_{10}=6$ as our hash value.

%As we can see this hash function has the drawback of an uneven distribution in $\F_{13}$. In fact this hash function is unable to map to values from $\{8,9,10,11,12\}$ as those numbers have a $1$-bit in position $4$. However as we will see in XXX, this hash is cheaper to implement as a circuit as no expensive modulus operation has to be used.
%\end{example}

\section{Commutative Rings}\label{sec:rings}
Thinking of back to operations on integers, we know that there are two of these: addition and multiplication. As we have seen, addition defines a group structure on the set of integers. However, multiplication does not define a group structure, given that integers in general don't have multiplicative inverses.

Configurations like this constitute a so-called \term{commutative ring with unit}. To be more precise, a commutative ring with unit $ (R, +, \cdot, 1) $ is a set $R$ provided with two maps $ +: R \cdot R \to R $ and $ \cdot: R \cdot R \to R $, called \term{addition} and \term{multiplication}, such that the following conditions hold:
\begin{itemize}
\item $ \left (R, + \right) $ is a commutative group, where the neutral element is denoted  with $ 0 $.
\item \hilight{Commutativity of  multiplication}: $r_1\cdot r_2 = r_2\cdot r_1$ for all $r_1, r_2\in R$.
\item \hilight{Existence of a unit}: There is an element $1\in R$, such that $1\cdot g$ holds for all $g\in R$,
\item \hilight{Associativity}: For every $g_1,g_2,g_3\in\G$ the equation
$g_1\cdot(g_2\cdot g_3) = (g_1\cdot g_2)\cdot g_3$ holds.
\item \hilight{Distributivity}: For all $ g_1, g_2, g_3 \in R $ the distributive laws
$ g_1 \cdot \left (g_2 + g_3 \right) = g_1 \cdot g_2 + g_1 \cdot g_3$ holds.
\end{itemize}
\begin{example}[The ring of integers] The set $\Z$ of integers with the usual addition and multiplication is the archetypical example of a commutative ring with unit $1$.
\end{example}
\begin{example}[Underlying commutative group of a ring] Every commutative ring with unit $(R,+,\cdot,1)$ gives rise to a group, if we disregard multiplication.
\end{example}
The following example is more interesting. The motivated reader is encouraged to think through this example, not so much because we need this in what follows, but more so as it helps to detach the reader from familiar styles of computation.
\begin{example} Let $S:=\{\bullet,\star,\odot,\otimes\}$ be a set that contains four elements and let addition and multiplication on $S$ be defined as follows:
\begin{center}
  \begin{tabular}{c | c c c c c c}
    $\cup$ & $\bullet$ & $\star$ & $\odot$ & $\otimes$ \\\hline
    $\bullet$ & $\bullet$ & $\star$ & $\odot$ & $\otimes$ \\
    $\star$ & $\star$ & $\odot$ & $\otimes$ & $\bullet$ \\
    $\odot$ & $\odot$ & $\otimes$ & $\bullet$ & $\star$ \\
    $\otimes$ & $\otimes$ & $\bullet$ & $\star$ & $\odot$ \\
  \end{tabular} \quad \quad \quad \quad
  \begin{tabular}{c | c c c c c c}
$ \circ $ & $\bullet$ & $\star$ & $\odot$ & $\otimes$ & \\\hline
        $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ &\\
        $\star$ & $\bullet$ & $\star$ & $\odot$ & $\otimes$ &\\
        $\odot$ & $\bullet$ & $\odot$ & $\bullet$ & $\odot$ &\\
        $\otimes$ & $\bullet$ & $\otimes$ & $\odot$ & $\star$ &\\
  \end{tabular}
\end{center}
Then $(S,\cup,\circ)$ is a ring with unit $\star$ and zero $\bullet$. It therefore makes sense to ask for solutions to equations like this one:
Find $x\in S$ such that
$$
\otimes \circ (x \cup \odot ) = \star
$$
To see how such a "moonmath equation" can be solved, we have to keep in mind that rings behaves mostly like normal number when it comes to bracketing and computation rules. The only differences are the symbols and the actual way to add and multiply. With this in mind, we solve the equation for $x$ in the``usual way'':
\begin{align*}
\otimes \circ (x \cup \odot ) &= \star & \text{ \# aply the distributive law}\\
\otimes \circ x \cup \otimes \circ \odot  &= \star &\# \otimes \circ \odot = \odot\\
\otimes \circ x \cup \odot  &= \star & \text{\# concatenate the $\cup$ inverse of $\odot$ to both sides}\\
\otimes \circ x \cup \odot \cup -\odot  &= \star \cup -\odot & \# \odot \cup -\odot = \bullet\\
\otimes \circ x \cup \bullet &= \star \cup -\odot & \text{\# $\bullet$ is the $\cup$ neutral element}\\
\otimes \circ x &= \star \cup -\odot & \text{\# for $\cup$ we have $-\odot = \odot$} \\
\otimes \circ x &= \star \cup \odot &\# \star \cup \odot = \otimes \\
\otimes \circ x &= \otimes  &\text{\# concatenate the $\circ$ inverse of $\otimes$ to both sides}\\
(\otimes)^{-1}\circ \otimes \circ x &= (\otimes)^{-1}\circ \otimes & \text{\# multiply with the multiplicative inverse}\\
\star \circ x &= \star\\
x &= \star
\end{align*}
So even though this equation looked really alien at first glance, we could solve it basically exactly the way we solve ``normal'' equations, like we do for fractional numbers, for example.

Note, however, that in a ring, things can be very different than most of us are used to whenever a multiplicative inverse would be needed to solve an equation in the usual way. For example the equation
$$
\odot \circ x = \otimes
$$
cannot be solved for $x$ in the usual way, since there is no multiplicative inverse for $\odot$ in our ring. We can confirm this by looking at the multiplication table to see that no such $x$ exits. 

As another example, the equation
$$
\odot \circ x = \odot
$$
does not have a single solution but two: $x\in\{\star, \otimes\}$. Having no solution or two solutions is certainly not something to expect from types like $\mathbb{Q}$.
\end{example}
\begin{example} Considering polynomials again, we note from their definition that what we have called the type $R$ of the coefficients must in fact be a commutative ring with a unit, since we need addition, multiplication, commutativity and the existence of a unit for $R[x]$ to have the properties we expect.

Considering $R$ to be a ring with addition and multiplication of polynomials as defined in XXX\sme{add reference} actually makes $R[x]$ into a commutative ring with a unit, too, where the polynomial $1$ is the multiplicative unit.
\end{example}
\begin{example} Let $n$ be a modulus and $(\Z_n,+,\cdot)$ the set of all remainder classes of integers modulo $n$, with the projection of integer addition and multiplication as defined in XXX. It can be shown that $(\Z_n,+,\cdot)$ is a commutative ring with unit $1$.
\end{example}
Considering the exponential map from page \pageref{exponentialmap} \sme{check reference} again, let $\G$ be a finite cyclic group of order $n$ with generator $g\in\G$. Then the ring structure of $(\Z_n,+,\cdot)$ is mapped onto the group structure of $\G$ in the following way:
\begin{align*}
g^{x+y} &= g^x\cdot g^y & \text{for all } x,y\in\Z_n\\
g^{x\cdot y} &= \left( g^x\right)^y & \text{for all } x,y\in\Z_n
\end{align*}
This of particular interest in cryptography and SNARKs, as it allows for the evaluation of polynomials with coefficients in $\Z_n$ to be evaluated ``in the exponent''. To be more precise, let $p\in \Z_n[x]$ be a polynomial with $p(x)=a_m\cdot x^m+a_{m-1}x^{m-1}+\ldots + a_1x +a_0$. Then the previously defined exponential laws \sme{add reference}XXX imply that
\begin{align*}
g^{p(x)} & = g^{a_m\cdot x^m+a_{m-1}x^{m-1}+\ldots + a_1x +a_0}\\
         & = \left(g^{x^m}\right)^{a_m}\cdot \left(g^{x^{m-1}}\right)^{a_{m-1}}\cdot \ldots\cdot \left(g^{x}\right)^{a_1}\cdot g^{a_0}
\end{align*}
and hence to evaluate $p$ at some point $s$ in the exponent, we can insert $s$ into the right hand side of the last equation and evaluate the product.

As we will see, this is a key insight to understand many SNARK protocols like e.g. Groth16 \citep{Groth16} or XXX\sme{add more examples protocols of SNARK}.
\begin{example} To give an example of the evaluation of a polynomial in the exponent of a finite cyclic group, consider the exponential map from example XXX\sme{add reference}:
$$
3^{(\cdot)}: \Z_4 \to \F_5^* \; x \mapsto 3^x
$$
 Choosing the polynomial $p(x)= 2x^2 +3x +1$ from $\Z_4[x]$, we can evaluate the polynomial at say $x=2$ in the exponent of $3$ in two different ways. On the one hand, we can evaluate $p$ at $2$ and then write the result into the exponent, which \tbds{gives} the following:
\begin{align*}
3^{p(2)} &=3^{2\cdot 2^2+3\cdot 2 +1}\\
          & = 3^{2\cdot 0 +2 +1}\\
          & = 3^{3}\\
          & = 2
\end{align*}
On the other hand, we can use the right hand side of the equation to evaluate $p$ at $2$ in the exponent of $3$, which \tbds{gives} the following:
\begin{align*}
3^{p(2)} &= \left(3^{2^2}\right)^2 \cdot \left(3^{2}\right)^3\cdot 3^1\\
         &= \left(3^{0}\right)^2 \cdot 3^3\cdot 3\\
         &= 1^2 \cdot 2 \cdot 3\\
         &= 2 \cdot 3\\
         &= 2
\end{align*}
\end{example}
\paragraph{Hashing to Commutative Rings} As we have seen in XXX\sme{add reference}, various constructions for hashing-to-groups are known and used in applications. As commutative rings are \uterm{Abelian groups}, when we disregard the multiplicative structure, hash-to-group constructions can be applied for hashing into commutative rings, too. This is possible in general, as the \uterm{codomain} of a general hash function $\{0,1\}^*$ is just the set of binary strings of arbitrary but finite length, which has no algebraic structure that the hash function must respect.

\section{Fields}\label{sec:fields}
We started this chapter with the definition of a group (section \ref{sec:groups}), which we then expanded into the definition of a commutative ring with a unit (section \ref{sec:rings}). Such rings generalize the behaviour of integers. In this section, we will look at the special case of commutative rings, where every element, other than the neutral element of addition, has a multiplicative inverse. Those structures behave very much like the rational numbers $\mathbb{Q}$, which are in a sense an extension of the ring of integers, that is, constructed by including newly defined multiplicative inverses (fractions) to the integers.

Now, considering the definition of a ring XXX\tbdsmarg{Add numbering to definitions} again, we define a \term{field} $ (\F, +, \cdot) $ to be a set $ \F$, together with two maps $ +: \F \cdot \F \to \F $ and $ \cdot: \F \cdot \F \to \F $, called \textit{addition} and \textit{multiplication}, such that the following conditions hold:
\begin{itemize}
\item $ \left (\F, + \right) $ is a commutative group, where the neutral element is denoted by $ 0 $.
\item $ \left (\F \setminus \left \{0 \right \}, \cdot \right) $ is a commutative group, where the neutral element is denoted by $ 1 $.
\item (Distributivity) For all $ g_1, g_2, g_3 \in \F $ the distributive law
$g_1 \cdot \left (g_2 + g_3 \right) = g_1 \cdot g_2 + g_1 \cdot g_3$ holds.
\end{itemize}
If a field is given and the definition of its addition and multiplication is not ambiguous, we will often simple write $\F$ instead of $(\F,+,\cdot)$ to denote the field. We moreover write $\F^*$ to describe the multiplicative group of the field, that is, the set of elements with the multiplication as group law excluding the neutral element of addition.

The \term{characteristic} of a field $ \F $, represented as $char(\F)$, is the smallest natural number $ n \geq 1 $, for which the $ n $ -fold sum of $ 1 $ equals zero, i.e. for which $ \sum_{i = 1} ^ n 1 = 0 $. If such a $ n> 0 $ exists, the field is also-called to have a \term{finite characteristic}. If, on the other hand, every finite sum of $1$ is such that it is not equal to zero, then the field is defined to have characteristic $ 0 $.\sme{Check change of wording} \smelong{S: Tried to disambiguate the scope of negation between 1. ``It is true of every finite sum of $1$ that it is not equal to 0''  and 2. ``It is not true of every finite sum of $1$ that it is  equal to 0'' From the example below, it looks like 1. is the intended meaning here, correct?}
\begin{example}[Field of rational numbers] Probably the best known example of a field is the set of rational numbers $\mathbb{Q}$ together with the usual definition of addition, subtraction, multiplication and division. Since there is no counting number $n\in \N$, such that $\sum_{j=0}^n 1 =0$ in the set of rational numbers, the characteristic $char(\mathbb{Q})$ of the field $\mathbb{Q}$ is zero. In Sage rational numbers are called as follows:
\begin{sagecommandline}
sage: QQ
sage: QQ(1/5) # Get an element from the field of rational numbers
sage: QQ(1/5) / QQ(3) # Division
\end{sagecommandline}
\end{example}
\begin{example}[Field with two elements]\label{ex:field-2-elements} It can be shown that in any field, the neutral element $0$ of addition must be different from the neutral element $1$ of multiplication, that is, $0\neq 1$ always holds in a field. From this, it follows that the smallest field must contain at least two elements. As the following addition and multiplication tables show, there is indeed a field with two elements, which is usually called $\F_2$:

Let $\F_2:=\{0,1 \}$ be a set that contains two elements and let addition and multiplication on $\F_2$ be defined as follows:
\begin{center}
  \begin{tabular}{c | c c c}
    + & 0 & 1 \\\hline
    0 & 0 & 1\\
    1 & 1 & 0 \\
  \end{tabular} \quad \quad \quad \quad
  \begin{tabular}{c | c c c}
$\cdot$ & 0 & 1 \\\hline
      0 & 0 & 0 \\
      1 & 0 & 1 \\
  \end{tabular}
\end{center}
Since $1+1=0$ in the field $\F_2$, we know that the characteristic of $\F_2$ is there, that is, we have $char(\F_2)=0$.

For reasons we will understand better in XXX\sme{add reference}, Sage defines this field as a so-called Galois field with 2 elements. You can call it in Sage as follows:
\begin{sagecommandline}
sage: F2 = GF(2)
sage: F2(1) # Get an element from GF(2)
sage: F2(1) + F2(1) # Addition
sage: F2(1) / F2(1) # Division
\end{sagecommandline}
\end{example}
\begin{example}
Both the real numbers $\mathbb{R}$ as well as the complex numbers $\mathbb{C}$ are well known examples of fields.
\end{example}
\begin{exercise}
Consider our remainder class ring $(\F_5,+,\cdot)$ and show that it is a field. What is the characteristic of $\F_5$?
\end{exercise}
\paragraph{Prime fields}
As we have seen in the various examples of the previous sections, modular arithmetics behaves similarly to ordinary arithmetics of integers  in many ways. This is due to the fact that remainder class sets $\Z_n$ are commutative rings with units.

However, we have also seen in XXX\sme{add reference} that, whenever the modulus is a prime number, every remainder class other than the zero class has a modular multiplicative inverse. This is an important observation, since it immediately implies that in case of a prime number, the remainder class set $\Z_n$ is not just a ring but actually a \term{field}. Moreover, since $\sum_{j=0}^n 1 = 0$ in $\Z_n$, we know that those fields have the finite characteristic $n$.

To distinguish this important case from arbitrary remainder class rings, we write  $ (\F_p, +, \cdot) $ for the field of all remainder classes for a prime number modulus $p \in \Prim$ and call it the \term{prime field} of characteristic $p$.

Prime fields are the foundation for many of the contemporary algebra-based cryptographic systems, as they have many desirable properties. One of them is that, since these sets are finite and a prime field of characteristic, $p$ can be represented on a computer in roughly $log_2(p)$ amount of space without precision problems that are unavoidable for computer representations of infinite sets, e.g. rational numbers or integers.

Since prime fields are special cases of remainder class rings, all computations remain the same. Addition and multiplication can be computed by first doing normal integer addition and multiplication, and then taking the remainder modulus $p$. Subtraction and division can be computed by adding or multiplying with the additive or the multiplicative inverse, respectively. The additive inverse $-x$ of a field element $x\in\F_p$ is given by $p-x$, and the multiplicative inverse of $x\neq 0$ is given by $x^{p-2}$, or can be computed using the Extended Euclidean Algorithm.

Note that these computations might not be the fastest to implement on a computer. They are, however, useful in this book, as they are easy to compute for small prime numbers.
\begin{example}
The smallest field is the field $\F_2$ of characteristic $2$ as we have seen in example \ref{ex:field-2-elements}. It is the prime field of the prime number $2$.\sme{Why are we repeating this example here again?}
\end{example}
\begin{example}
To summarize the basic aspects of computation in prime fields, let us consider the prime field $\F_5$ and simplify the following expression
$$\left(\frac{2}{3} - 2\right)\cdot 2 $$
A first thing to note is that since $\F_5$ is a field all rules are identical to the rules we learned in school when we where dealing with rational, real or complex numbers. This means we can use e.g.  bracketing (distributivity) or addition as usual:
\begin{align*}
\left(\frac{2}{3} - 2\right)\cdot 2 &=
 \frac{2}{3}\cdot 2 - 2\cdot 2 & \text{\# distributive law}\\
 &= \frac{2\cdot 2}{3} - 2\cdot 2 & \Zmod{4}{5}=4 \\
 &= \frac{4}{3} - 4 & \text{\# multiplicative inverse of 3 is } \Zmod{3^{5-2}}{5}=2\\
 &= 4\cdot 2 - 4 & \text{\# additive inverse of 4 is } 5-4=1\\
 &= 4\cdot 2 +1 & \Zmod{8}{5}=3\\
 &= 3 +1 & \Zmod{4}{5}=4\\
 &= 4
\end{align*}
In this computation we computed the multiplicative inverse of $3$ using the identity
$x^{-1}=x^{p-2}$ in a prime field. This is impractical for large prime numbers. Recall that another way of computing the multiplicative inverse is the Extended Euclidean Algorithm (see \ref{eq: erw_Eukl_algo} on page \pageref{eq: erw_Eukl_algo}).  To refresh our memories, the task is to compute $x^{-1}\cdot 3 + t \cdot 5 =1$, but $t$ is actually irrelevant. We get
\begin{center}
  \begin{tabular}{c | c c l}
    k & $ r_k $ & $ x^{-1}_k $ & $ t_k = \Zdiv{(r_k-s_k \cdot a)}{b} $ \\\hline
    0 & 3 & 1 & $\cdot$\ \\
    1 & 5 & 0 & $\cdot$ \\
    2 & 3 & 1 & $\cdot$ \\
    3 & 2 &-1 & $\cdot$ \\
    4 & 1 & 2  & $\cdot$ \\
  \end{tabular}
\end{center}
So the multiplicative inverse of $3$ in $\Z_5$ is $2$ and indeed if compute $3\cdot 2$ we get $1$ in $\F_5$.
\end{example}
\paragraph{Square Roots}
In this part, we deal with square numbers, also called \term{quadratic residues} and \term{square roots} in prime fields. This is of particular importance in our studies on elliptic curves, as only square numbers can actually be points on an elliptic curve.\sme{S: are we introducing elliptic curves in section 1 or 2?}

To make the intuition of quadratic residues and roots precise, let $p \in \Prim $ be a prime number and $\F_p $ its associated prime field. Then a number $x\in \F_p$ is called a \textbf{square root} of another number $y\in\F_p$, if $x$ is a solution to the equation
\begin{equation}
x^2 = y
\end{equation}
In this case, $y$ is called a \term{quadratic residue}. On the other hand, if $y$ is given and the quadratic equation has no solution $x$ , we call $ y $ a \term{quadratic non-residue}. For any $ y \in \F_p $, we write
\begin{equation}
\sqrt{y}: = \{x \in \F_p \; | \; x^2 = y \}
\end{equation}
for the set of all square roots of $ y $ in the prime field $ \F_n $. (If $ y $ is a quadratic non-residue, then $ \sqrt{y} = \emptyset $ (an empty set), and if $ y = 0 $, then $ \sqrt{y} = \{0 \} $)

Informally speaking, quadratic residues are numbers such that we can take the square root of them, while quadratic non-residues are numbers that don't have square roots. The situation therefore parallels the know case of integers, where some integers like $4$ or $9$ have square roots and others like $2$ or $3$ don't (as integers).

It can be shown that in any prime field every non zero element has either no square root or two of them. We adopt the convention to call the smaller one (when interpreted as an integer) as the \textbf{positive} square root and the larger one as the \textbf{negative}. This makes sense, as the larger one can always be computed as the modulus minus the smaller one, which is the definition of the negative in prime fields.


\begin{example} [Quadratic (Non)-Residues and roots in $ \F_5 $] Let us consider our example prime field $\F_5$ again. All square numbers can be found on the main diagonal of the multiplication table XXX. As you can see, in $ \Z_5 $ only the numbers $ 0 $, $ 1 $ and $ 4 $ have square roots and we get $ \sqrt{0} = \{0 \} $, $ \sqrt{1} = \{1,4 \} $, $ \sqrt{2} = \emptyset $, $ \sqrt{3} = \emptyset $ and $ \sqrt{4} = \{2,3 \} $. The numbers $0$, $1$ and $4$ are therefore quadratic residues, while the numbers $2$ and $3$ are quadratic non-residues.
\end{example}
In order to describe whether an element of a prime field is a square number  or not, the so-called Legendre Symbol can sometimes be found in the literature, why we will recapitulate it here:

Let $ p \in \Prim $ be a prime number and $ y \in \F_p $ an element from the associated prime field. Then the so-called \textit{Legendre symbol} of $ y $ is defined as follows:
\begin{equation}
\label{eq: Legendre-symbol}
\left (\frac{y}{p} \right): =
\begin{cases}
1 & \text{if $ y $ has square roots} \\
-1 & \text{if $ y $ has no square roots} \\
0 & \text{if $ y = 0 $}
\end{cases}
\end{equation}
\begin{example}
Look at the quadratic residues and non residues in $\F_5$ from example XXX again, we can deduce the following Legendre symbols, from example XXX.
$$
\begin{array}{ccccc}
\left (\frac{0}{5} \right) = 0, &
\left (\frac{1}{5} \right) = 1, &
\left (\frac{2}{5} \right) = -1, &
\left (\frac{3}{5} \right) = -1, &
\left (\frac{4}{5} \right) = 1 \;.
\end{array}
$$
\end{example}
The Legendre symbol gives a criterion to decide whether or not an element from a prime field has a quadratic root or not. This, however, is not just of theoretic use, as the following so-called \term{Euler criterion} gives a compact way to actually compute the Legendre symbol. To see that, let $ p \in \Prim_{\geq 3} $ be an odd prime number and $ y \in \F_p $. Then the Legendre symbol can be computed as
\begin{equation}
\label{eq: Euler_criterium}
\left (\frac{y}{p} \right) = y^{\frac{p-1}{2}} \;.
\end{equation}
\begin{example}
Looking at the quadratic residues and non residues in $\F_5$ from example XXX\sme{add reference} again, we can compute the following Legendre symbols using the Euler criterium:
\begin{align*}
\left (\frac{0}{5} \right) &= 0^{\frac{5-1}{2}}= 0^2=0\\
\left (\frac{1}{5} \right) &= 1^{\frac{5-1}{2}}= 1^2=1\\
\left (\frac{2}{5} \right) &= 2^{\frac{5-1}{2}}= 2^2=4 = -1\\
\left (\frac{3}{5} \right) &= 3^{\frac{5-1}{2}}= 3^2=4 =-1\\
\left (\frac{4}{5} \right) &= 4^{\frac{5-1}{2}}= 4^2=1
\end{align*}
\end{example}
\begin{exercise} Consider the prime field $\F_{13}$. Find the set of all pairs $(x,y)\in \F_{13}\times \F_{13}$ that satisfy the equation
$$
x^2+y^2 = 1 + 7\cdot x^2\cdot y^2
$$
\end{exercise}
% I think this isn't needed. Will just leave it here in case this changes
%
%So the question remains how to actually compute square roots in prime field. The following algorithms give a solution
%\begin{definition}[Tonelli-Shanks algorithm]
%\label{def: Tonelli-Shanks}
%Let $ p $ be an odd prime number $ p \in \Prim _{\geq 3} $ and $ y $ a quadratic residue in $ \Z_p $. Then the so-called Tonneli \cite{TA} and Shanks \cite{SD} algorithm computes the two square roots of $ y $. It is defined as follows:
%\begin{enumerate}
%\item Find $ Q, S \in \Z $ with $ p-1 = Q \cdot 2 ^ S $ such that $ Q $ is odd.
%\item Find an arbitrary quadratic non-remainder $ z \in \Z_p $.
%\item
%\begin{algorithmic}
%\State $ \begin{array}{ccccc}
%M: = S, & c: = z ^ Q, & t: = y ^ Q, & R: = y ^{\frac{Q + 1}{2}}, & M, c, t, R \in \Z_p
%\end{array} $
%\While{$ t \neq 1 $}
%\State Find the smallest $ i $ with $ 0 <i <M $ and $ t ^{2 ^ i} = 1 $
%\State $ b: = c ^{2 ^{M-i-1}} $
%\State $ \begin{array}{ccccc}
%M: = i, & c: = b ^ 2, & t: = tb ^ 2, & R: = R \cdot b
%\end{array} $
%\EndWhile
%\end{algorithmic}
%The results are then the square roots $ r_1: = R $ and $ r_2: = p-R $ of $y$ in $\F_p$.
%\end{enumerate}
%\end{definition}

%\begin{remark}The algorithm (\ref{def: Tonelli-Shanks}) works in prime fields for any odd prime numbers. From a practical point of view, however, it is efficient only if the prime number is congruent to $ 1 $ modulo $ 4 $, since in the other case the formula from the proposition \ref{theorem: square_roots}, which can be calculated more quickly, can be used.\end{remark}
\paragraph{Exponentiation} TO APPEAR...\sme{write paragraph on exponentiation}
\paragraph{Hashing into prime fields} 
An important problem in SNARK development is the ability to hash to (various subsets) of elliptic curves. As we will see in XXX,\sme{add reference} those curves are often defined over prime fields, and hashing to a curve then might start with hashing to the prime field. It is therefore important to understand how to hash into prime fields.

\tbds{To understand it}, note that in XXX\sme{add reference} we have looked at a few constructions of how to hash into the residue class rings $\Z_n$ for arbitrary $n>1$. As prime fiedls are just special instances of those rings, all hashing into $\Z_n$ functions can be used for hashing into prime fields, too.
\paragraph{Extension Fields}
% references https://blog.plover.com/math/se/finite-fields.html
We defined prime fields in the previous section. They are the basic building blocks for cryptography in general and SNARKs in particular.

However, as we will see in, XX\sme{add reference} so-called \term{pairing based} SNARK systems are crucially dependent on \uterm{group pairings} XXX defined over the group of rational points of elliptic curves. For those pairings to be non-trivial, the elliptic curve must not only be defined over a prime field, but over a so-called \term{extension field} of a given prime field.

We therefore have to understand field extensions. To understand them, first observe that the field $\F'$ is called an \term{extension} of a field $\F$ if $\F$ is a subfield of $\F'$, that is, $\F$ is a subset of $\F'$ and restricting the addition and multiplication laws of $\F'$ to the subset $\F$ recovers the appropriate laws of $\F$.

Now it can be shown that whenever $p\in \Prim$ is a prime and $m\in\N$ a natural number, then there is a field $\F_{p^m}$ with characteristic $p$ and $p^m$ elements such that $\F_{p^m}$ is an extension field of the prime field $\F_p$.

Similarly to the way prime fields $\F_p$ are generated by starting with the ring of integers and then dividing by a prime number $p$ and keeping the remainder, prime field extensions $\F_{p^m}$ are generated by starting with the ring $\F_p[x]$ of polynomials and then dividing them by an irreducible polynomial of degree $m$ and keeping the remainder.

To be more precise, let $P\in F_p[x]$ be an irreducible polynomial of degree $m$ with coefficients from the given prime field $\F_p$. Then the underlying set $\F_{p^m}$ of the extension field is given by the set of all polynomials with a degree less then $m$:
\begin{equation}
\F_{p^m}:=\{a_{m-1} x^{m-1}+a_{k-2}x^{k-2}+\ldots+a_1 x+a_0\;|\; a_i\in \F_p\}
\end{equation}
which can be shown to be the set of all remainders when dividing any polynomial $Q\in \F_p[x]$ by $P$. So elements of the extension field are polynomials of degree less than $m$. This is analogous to how $\F_p$ is the set of all remainders when dividing integers by $p$.

Addition is then inherited from $\F_p[x]$, which means that addition on $\F_{p^m}$ is defined as normal addition of polynomials. To be more precise, we have
\begin{equation}
+:\; \F_{p^m}\times \F_{p^m} \to \F_{p^m}\; ,\; (\sum_{j=0}^m a_j x^j,\sum_{j=0}^m b_j x^j)\mapsto \sum_{j=0}^m (a_j+b_j) x^j
\end{equation}
and we can see that the neutral element is (the polynomial) $0$, and that the additive inverse is given by the polynomial with all negative coefficients.

Multiplication is inherited from $\F_p[x]$, too, but we have to divide the result by our modulus polynomial $P$, whenever the degree of the resulting polynomial is equal or greater to $m$. To be more precise, we have
\begin{equation}
\cdot\; \F_{p^m}\times \F_{p^m} \to \F_{p^m}\; ,\; (\sum_{j=0}^m a_j x^j,\sum_{j=0}^m b_j x^j)\mapsto \Zmod{\left(\sum _{n = 0} ^{2m} \sum _{i = 0} ^{n}{a} _{i }{{b} _{n-i}}{x} ^{n}\right)}{P}
\end{equation}
and we can see that the neutral element is (the polynomial) $1$. It is, however, not obvious from this definition how the multiplicative inverse looks.

We can easily see from the definition of $\F_{p^m}$ that the field is of characteristic $p$, since the multiplicative neutral element $1$ is equivalent to the multiplicative element $1$ from the underlying prime field and hence $\sum_{j=0}^p 1=0$. Moreover, $\F_{p^m}$ is finite and contains $p^m$ many elements, since elements are polynomials of degree $<m$ and every coefficient $a_j$ can have a $p$ number of different values. In addition, we see that the prime field $\F_p$ is a subfield of $\F_{p^m}$ that occurs when we restrict the elements of $\F_{p}$ to polynomials of degree zero.

One key point is that the construction of $\F_{p^m}$ depends on the choice of an irreducible polynomial, and, in fact, different choices will give different multiplication tables, since the remainders from dividing a product by $P$ will be different.

It can, however, be shown that the fields for different choices of $P$ are \term{isomorphic}, which means that there is a one-to-one correspondence between all of them, which means that, from an abstract point of view, they are the same thing. From an implementations point of view, however, some choices are preferable, because they allow for faster computations.

To summarize, we have seen that when a prime field $\F_p$ is given, then any field 
$\F_{p^m}$ constructed in the above manner is a field extension of $\F_p$. To be more general, a field $\F_{p^{m_2}}$ is a field extension of a field $\F_{p^{m_1}}$, if and only if $m_1$ divides $m_2$. From this, we can deduce that, for any given fixed prime number, there are nested sequences of fields
\begin{equation}
\F_p \subset \F_{p^{m_1}} \subset \cdots \subset \F_{p^{m_k}}
\end{equation}
whenever the power $m_j$ divides the power $m_{j+1}$, such that $\F_{p^{m_j}}$ is a subfield of $\F_{p^{m_{j+1}}}$.

To get a more intuitive picture of this, we construct an extension field of the prime field  $\F_3$ in the following example, and we can see how $\F_3$ sits inside that extension field. 
\begin{example}[The Extension field $\F_{3^2}$]In (XXX)\sme{add reference} we have constructed the prime field $\F_3$. In this example, we apply the definition (XXX) of a field extension to construct $\F_{3^2}$. We start by choosing an irreducible polynomial of degree $2$ with coefficients in $\F_3$. We try
$P(t)=t^2+1$. Maybe the fastest way to show that $P$ is indeed irreducible is to just insert all elements from $\F_3$ to see if the result is never zero. We compute
\begin{align*}
P(0) = 0^2+1 &= 1\\
P(1) = 1^2+1 &= 2\\
P(2) = 2^2+1 &=  1+1  = 2
\end{align*}
This implies that $P$ is irreducible. The set $\F_{3^2}$ then contains all polynomials of degrees lower than two, with coefficients in $\F_3$, which is precisely
$$
\F_{3^2}=\{0,1,2,t,t+1,t+2,2t,2t+1,2t+2\}
$$
So our extension field contains $9$ elements as expected. Addition is  defined as addition of polynomials. For example $(t+2) + (2t+2)= (1+2)t +(2+2)= 1$. Doing this computation for all elements gives the following addition table
\begin{center}
  \begin{tabular}{c | c c c c c c c c c}
    + & 0    & 1    & 2    & t    & t+1  & t+2  & 2t   & 2t+1 & 2t+2 \\\hline
    0 & 0    & 1    & 2    & t    & t+1  & t+2  & 2t   & 2t+1 & 2t+2 \\
    1 & 1    & 2    & 0    & t+1  & t+2  & t    & 2t+1 & 2t+2 & 2t   \\
    2 & 2    & 0    & 1    & r+2  & t    & t+1  & 2t+2 & 2t   & 2t+1 \\
    t & t    & t+1  & t+2  & 2t   & 2t+1 & 2t+2 & 0    & 1    & 2    \\
  t+1 & t+1  & t+2  & t    & 2t+1 & 2t+2 & 2t   & 1    & 2    & 0    \\
  t+2 & t+2  & t    & t+1  & 2t+2 & 2t   & 2t+1 & 2    & 0    & 1    \\
   2t & 2t   & 2t+1 & 2t+2 & 0    & 1    & 2    & t    & t+1  & t+2  \\
 2t+1 & 2t+1 & 2t+2 & 2t   & 1    & 2    & 0    & t+1  & t+2  & t    \\
 2t+2 & 2t+2 & 2t   & 2t+1 & 2    & 0    & 1    & t+2  & t    & t+1
  \end{tabular}
\end{center}
As we can see, the group $(\F_3,+)$ is a subgroup of the group $(\F_{3^2},+)$, obtained by only considering the first three rows and columns of this table.

As it was the case in previous examples, we can use the table to deduce the negative of any element from $\F_{3^2}$. For example, in $\F_{3^2}$ we have $-(2t+1)= t+2$, since $(2t+1) + (t+2)=0$

Multiplication needs a bit more computation, as we first have to multiply the polynomials, and whenever the result has a degree $\geq 2$, we have to divide it by $P$ and keep the remainder. To see how this works, compute the product of $t+2$ and $2t+2$ in $\F_{3^2}$
\begin{align*}
(t+2) \cdot (2t+2) &= \Zmod{(2t^2 + 2t + t + 1)}{(t^2+1)} \\
                   &= \Zmod{(2t^2+1)}{(t^2+1)} & \#\; 2t^2+1:t^2+1= 2 + \frac{2}{t^2+1} \\
                   &= 2
\end{align*}
So the product of $t+2$ and $2t+2$ in $\F_{3^2}$ is $2$. Doing this computation for all elements gives the following multiplication table:
\begin{center}
  \begin{tabular}{c | c c c c c c c c c}
$\cdot$ & 0    & 1    & 2    & t    & t+1  & t+2  & 2t   & 2t+1 & 2t+2 \\\hline
      0 & 0    & 0    & 0    & 0    & 0    & 0    & 0    & 0    & 0 \\
      1 & 0    & 1    & 2    & t    & t+1  & t+2  & 2t   & 2t+1 & 2t+2\\
      2 & 0    & 2    & 1    & 2t   & 2t+2 & 2t+1 & t    & t+2  & t+1 \\
      t & 0    & t    & 2t   & 2    & t+2  & 2t+2 & 1    & t+1  & 2t+1  \\
    t+1 & 0    & t+1  & 2t+2 & t+2  & 2t   & 1    & 2t+1 & 2    & t   \\
    t+2 & 0    & t+2  & 2t+1 & 2t+2 & 1    & t    & t+1  & 2t   & 2    \\
     2t & 0    & 2t   & t    & 1    & 2t+1 & t+1  & 2  & 2t+2 & t+2\\
   2t+1 & 0    & 2t+1 & t+2  & t+1  & 2    & 2t   & 2t+2 & t    & 1    \\
   2t+2 & 0    & 2t+2 & t+1  & 2t+1 & t    & 2    & t+2  & 1     & 2t
  \end{tabular}
\end{center}
As it was the case in previous examples, we can use the table to deduce the multiplicative inverse of any non-zero element from $\F_{3^2}$. For example in $\F_{3^2}$ we have $(2t+1)^{-1}= 2t+2 $, since $(2t+1) \cdot (2t+2)=1$.

From the multiplication table, we can also see that the only quadratic residues in $\F_{3^2}$ are the set $\{0,1,2, t, 2t\}$, with
$\sqrt{0}=\{0\}$, $\sqrt{1}=\{1,2\}$, $\sqrt{2}=\{t, 2t\}$, $\sqrt{t}=\{t+2,2t+1\}$ and $\sqrt{2t}=\{t+1,2t+2\}$.

Since $\F_{3^2}$ is a field, we can solve equations as we would for other fields, like the rational numbers. To see that lets find all $x\in\F_{3^2}$ that solve the quadratic equation $(t+1)(x^2 + (2t+2)) = 2$. So we compute:
\begin{align*}
(t+1)(x^2 + (2t+2))    &= 2 &\text{\# 2 distributive law}\\
(t+1)x^2 + (t+1)(2t+2) &= 2 \\
(t+1)x^2 + (t)         &= 2 &\text{\# 2 add the additive inverse of $t$}\\
(t+1)x^2 + (t) + (2t)  &= (2) + (2t) \\
(t+1)x^2               &= 2t+2 & \text{\# multiply with the multiplicative invers of $t+1$}\\
(t+2)(t+1)x^2          &=(t+2)(2t+2) & \text{\# multiply with the multiplicative invers of $t+1$}\\
x^2                    &= 2 & \text{\# 2 is quadratic residue. Take the roots.}\\
x &\in \{t, 2t\}
\end{align*}
Computations in extension fields are arguably on the edge of what can reasonbly be done with pen and paper. Fortunately, Sage provides us with a simple way to do the computations.
\begin{sagecommandline}
sage: Z3 = GF(3) # prime field
sage: Z3t.<t> = Z3[] # polynomials over Z3
sage: P = Z3t(t^2+1)
sage: P.is_irreducible()
sage: F3_2.<t> = GF(3^2, name='t', modulus=P)
sage: F3_2
sage: F3_2(t+2)*F3_2(2*t+2) == F3_2(2)
sage: F3_2(2*t+2)^(-1) # multiplicative inverse
sage: # verify our solution to (t+1)(x^2 + (2t+2)) = 2
sage: F3_2(t+1)*(F3_2(t)**2 + F3_2(2*t+2)) == F3_2(2)
sage: F3_2(t+1)*(F3_2(2*t)**2 + F3_2(2*t+2)) == F3_2(2)
\end{sagecommandline}
\end{example}
\begin{exercise}
Consider the extension field $\F_{3^2}$ from the previous example and find all pairs of elements $(x,y)\in\F_{3^2}$, such that
$$
y^2 = x^3 + 4
$$
\end{exercise}
\begin{exercise} Show that the polynomial $P=x^3+x+1$ from $\F_5[x]$ is irreducible. Then consider the extension field $\F_{5^3}$ defined relative to $P$. Compute the multiplicative inverse of $(2t^2+4)\in\F_{5^3}$ using the extended Euklidean algorithm. Then find all $x\in\F_{5^3}$ that solve the  equation
$$(2t^2+4)(x-(t^2+4t+2))= (2t+3)$$
\end{exercise}
\paragraph{Hashing into extension fields} In XXX\sme{add reference} we have seen how to hash into prime fields. As elements of extension fields can be seen as polynomials over prime fields, hashing into extension fields is therefore possible, if every coefficient of the polynimial is hashed independently. 
\section{Projective Planes}\label{sec:planes}
Projective planes are \tbds{a certain type of geometry} defined over a given field. In a sense, projective plances extend the concept of the ordinary Euclidean plane by including ``points at infinity.''

Such an inclusion of infinity points makes them particularly useful in the description of elliptic curves, as the description of such a curve in an ordinary plane needs an additional symbol "the point at infinity" to give the set of points on the curve the structure of a group. Translating the curve into projective geometry, then includes this "point at infinity" more naturally into the set of all points on a projective plane.

To understand the idea for the construction of projective planes, note that in
an ordinary Euclidean plane, two lines either intersect in a single point, or are parallel. In the latter case both lines are either the same, that is, they intersect in all points, or do not intersect at all. A projective plane can then be thought of as an ordinary plane, but equipped with additional "points at infinity" such that two different lines always intersect in a single point. Parallel lines intersect "at infinity".

To be more precise, let $\F$ be a field, $\F^3:=\F\times \F\times F$ the set of all three tuples over $\F$ and $x\in \F^3$ with $x=(X,Y,Z)$. Then there is exactly one \textit{line} in $\F^3$ that intersects both $(0,0,0)$ and $x$. This line is given by
\begin{equation}
[X:Y:Z] := \{(k\cdot X,k\cdot Y, k\cdot Z)\;|\; k\in\F\}
\end{equation}
A \term{point} in the \textbf{projective plane} over $\F$ is then defined as such a \textit{line} and the projective plane is the set of all such points, that is
\begin{equation}
\F\mathbb{P}^2:=\{[X:Y:Z]\;|\; (X,Y,Z)\in \F^3\text{ with } (X,Y,Z)\neq (0,0,0)\}
\end{equation}
It can be shown that a projective plane over a finite field $\F_{p^m}$ contains $p^{2m}+p^m+1$ many elements.

To understand why $[X:Y:Z]$ is called a line, consider the situation where the underlying field $\F$ are the real numbers $\mathbb{R}$. Then $\mathbb{R}^3$ can be seen as the three dimensional space and $[X:Y:Z]$ is then an ordinary line in this 3-dimensional space that intersects zero and the point with coordinates $X$, $Y$ and $Z$.

The key observation here is that points in the projective plane are lines in the $3$-dimensional space $\F^3$, also for finite fields, the terms space and line share very little visual similarity with their counterparts over the real numbers.

It follows from this that points $[X:Y:Z]\in \F\mathbb{P}^2$ are not simply described by fixed coordinates $(X,Y,Z)$, but by \term{sets of coordinates} rather, where two different coordinates $(X_1,Y_1,Z_1)$ and $(X_2,Y_2,Z_2)$, with describe the same point, if and only if there is some field element $k$, such that $(X_1,Y_1,Z_1) = (k\cdot X_2,k\cdot Y_2,k\cdot Z_2)$. Point $[X:Y:Z]$ are called \term{projective coordinates}.

\begin{notation}[Projective coordinates]
%https://math.mit.edu/classes/18.783/2017/Lecture1.pdf
Projective coordinates of the form $[X:Y:1]$ are descriptions of so-called \textbf{affine points} and projective coordinates of the form $[X:Y:0]$ are descriptions of so-called \textbf{points at infinity}. In particular the projective coordinate $[1:0:0]$ describes the so-called \textbf{line at infinity}.
\end{notation}
\begin{example} Consider the field $\F_3$ from example XXX\sme{add reference}. As this field only contains three elements, it does not take too much effort to construct its associated projective plane $\F_3\mathbb{P}^2$, as we know that it only contain $13$ elements.

To find $\F_3\mathbb{P}^2$, we have to compute the set of all lines in $\F_3\times \F_3\times \F_3$ that intersect $(0,0,0)$. Since those lines are parameterized by tuples $(x_1,x_2,x_3)$. We compute:
\begin{align*}
[0:0:1] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(0,0,1), (0,0,2)\}\\
[0:0:2] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(0,0,2), (0,0,1)\}
          = [0:0:1]\\
[0:1:0] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(0,1,0), (0,2,0)\}\\
[0:1:1] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(0,1,1), (0,2,2)\}\\
[0:1:2] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(0,1,2), (0,2,1)\}\\
[0:2:0] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(0,2,0), (0,1,0)\}
          = [0:1:0]\\
[0:2:1] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(0,2,1), (0,1,2)\}
          = [0:1:2]\\
[0:2:2] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(0,2,2), (0,1,1)\}
          = [0:1:1]\\
[1:0:0] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(1,0,0), (2,0,0)\}\\
[1:0:1] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(1,0,1), (2,0,2)\}\\
[1:0:2] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(1,0,2), (2,0,1)\}\\
[1:1:0] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(1,1,0), (2,2,0)\}\\
[1:1:1] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(1,1,1), (2,2,2)\}\\
[1:1:2]&= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(1,1,2), (2,2,1)\}\\
[1:2:0] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(1,2,0), (2,1,0)\}\\
[1:2:1] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(1,2,1), (2,1,2)\}\\
[1:2:2] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(1,2,2), (2,1,1)\}\\
[2:0:0] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(2,0,0), (1,0,0)\}
          = [1:0:0]\\
[2:0:1] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(2,0,1), (1,0,2)\}
          = [1:0:2]\\
[2:0:2] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(2,0,2), (1,0,1)\}
          = [1:0:1]\\
[2:1:0] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(2,1,0), (1,2,0)\}
          = [1:2:0]\\
[2:1:1] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(2,1,1), (1,2,2)\}
          = [1:2:2]\\
[2:1:2] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(2,1,2), (1,2,1)\}
          = [1:2:1]\\
[2:2:0] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(2,2,0), (1,1,0)\}
          = [1:1:0]\\
[2:2:1] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(2,2,1), (1,1,2)\}
          = [1:1:2]\\
[2:2:2] &= \{(k\cdot x_1,k\cdot x_2, k\cdot x_3)\;|\; k\in\F_3\}
          = \{(2,2,2), (1,1,1)\}
          = [1:1:1]
\end{align*}
Those lines define the $13$ points in the projective plane $\F_3\mathbb{P}$ as follows
\begin{multline*}
\F_3\mathbb{P} = \{ [0:0:1], [0:1:0], [0:1:1], [0:1:2], [1:0:0], [1:0:1], \\ [1:0:2], [1:1:0], [1:1:1], [1:1:2], [1:2:0], [1:2:1], [1:2:2]\}
\end{multline*}
This projective plane contains $9$ affine points, three points at infinity and one line at infinity.

To understand the ambiguity in projective coordinates a bit better, let us consider the point $[1:2:2]$. As this point in the projective plane is a line in $\F_3^3$, it has the projective coordinates $(1,2,2)$ as well as $(2,1,1)$, since the former coordinate give the latter, when multiplied in $\F_3$ by the factor $2$. In addition, note that for the same reasons the points $[1:2:2]$ and $[2:1:1]$ are the same, since their underlying sets are equal.
\end{example}
\begin{exercise}
Construct the so-called \textit{Fano plane}, that is, the projective plane over the finite field $\F_2$.
\end{exercise}
