\chapter{Algebraic Hash Functions}\label{chap:hash}
In this chapter we address the role of algebraic hash functions in modern cryptographic protocols. In contrast to more general hash functions, \textit{algebraic} hash functions map inputs into algebraic structures like groups, fields or elliptic curves, which is often a requirement for zero knowledge proof systems like Groth\_16 or Plonk.

Key topics include a detailed examination of different algebraic hash functions, and their structural characteristics in various cryptographic scenarios. The chapter aims to provide a technical understanding of these functions, their role in ensuring the security and functionality of cryptographic protocols, and their practical applications in the field.

\section{Hash functions}\label{sec:hash-functions} Generally speaking, a hash function is any function that can be used to map data of arbitrary size to fixed-size values. Since binary strings of arbitrary length are a way to represent data in general, we can understand a \textbf{hash function} as the following map where $\{0,1\}^*$ represents the set of all binary strings of arbitrary but finite length and $\{0,1\}^k$ represents the set of all binary strings that have a length of exactly $k$ bits:
\begin{equation}
\label{def:hash_function}
H: \{0,1\}^* \to \{0,1\}^k
\end{equation}
The \term{images} of $H$, that is, the values returned by the hash function $H$, are called \term{hash values}, \term{digests}, or simply \term{hashes}.

\begin{notation}
\label{string_and_hash_notations}
In what follows, we call an element $b\in\{0,1\}$ a \term{bit}. If $s\in\{0,1\}^*$\sme{check footnote} is a binary string, we write $|s|=k$ for its \term{length}, that is, for the number of bits in $s$. We write $<>$ for the empty binary string, and $s=<b_1,b_2,\ldots,b_k>$ for a binary string of length $k$.\footnote{The difference between the notations $b\in\{0,1\}$ and $s\in\{0,1\}^*$ is the following: $b\in\{0,1\}$ means that $b$ is equal to either $0$ or $1$, whereas $s$ is a string composed of an arbitrary number of $0$s and $1$s (and $s$ can also be an empty string).}

If two binary strings $s=<b_1,b_2,\ldots,b_k>$ and $s'=<b'_1,b'_2,\ldots,b'_l>$ are given, then we write $s||s'$ for the \term{concatenation} that is the string 
$s||s'=<b_1,b_2,\ldots,b_k,b'_1,b'_2,\ldots,b'_l>$.

If $H$ is a hash function that maps binary strings of arbitrary length onto binary strings of length $k$, and $s\in\{0,1\}^*$ is a binary string, we write $H(s)_j$ for the bit at position $j$ in the image $H(s)$.
\end{notation}

\begin{example}[$k$-truncation hash]\label{ex:k-truncation-hash} One of the most basic hash functions $H_k:\{0,1\}^*\to \{0,1\}^k$ is given by simply truncating every binary string $s$ of size $|s|> k$ to a string of size $k$ and by filling any string $s'$ of size $|s'|<k$ with zeros. To make this hash function deterministic, we define that both truncation and filling should happen ``on the left''.

For example, if the parameter $k$ is given by $k=3$, $s_1=<0,0,0,0,1,0,1,0,1,1,1,0>$ and $s_2=1$, then $H_3(s_1)=<1,1,0>$ and $H_3(s_2)=<0,0,1>$.
\end{example}

A desirable property of a hash function is \term{uniformity}, which means that it should map input values as evenly as possible over its output range. In mathematical terms, every string of length $k$  from $\{0,1\}^k$ should be generated with roughly the same probability.

Of particular interest are so-called \term{cryptographic} hash functions, which are hash functions that are also \term{one-way functions}, which essentially means that, given a string $y$ from $\{0,1\}^k$ it is infeasible to find a string $x\in\{0,1\}^*$ such that $H(x)=y$ holds. This property is usually called \term{preimage-resistance}.

Moreover, if a string $x_1\in\{0,1\}^*$ is given, then it should be infeasible to find another string $x_2\in\{0,1\}^*$ with $x_1\neq x_2$ and $H(x_1)=H_(x_2)$

In addition, it should be infeasible to find two strings $x_1,x_2 \in\{0,1\}^*$ such that $H(x_1)=H(x_2)$, which is called \term{collision resistance}. It is important to note, though, that collisions always exist, since a function $H: \{0,1\}^* \to \{0,1\}^k$ inevitably maps infinitely many values onto the same hash. In fact, for any hash function with digests of length $k$, finding a preimage to a given digest can always be done using a brute force search in $2^k$ evaluation steps. It should just be practically impossible to compute those values, and statistically very unlikely to generate two of them by chance.

A third property of a cryptographic hash function is that small changes in the input string, like changing a single bit, should generate hash values that look completely different from each other. This is called \term{diffusion} or the avalanche effect.

Because cryptographic hash functions map tiny changes in input values onto large changes in the output, implementation errors that change the outcome are usually easy to spot by comparing them to expected output values. The definitions of cryptographic hash functions are therefore usually accompanied by some test vectors of common inputs and expected digests. Since the empty string $<>$ is the only string of length $0$, a common test vector is the expected digest of the empty string.
\begin{example}[$k$-truncation hash] Consider the $k$-truncation hash from example \ref{ex:k-truncation-hash}. Since the empty string has length $0$, it follows that the digest of the empty string is the string of length $k$ that only contains $0$s:
\begin{equation}
H_k(<>)= <0,0,\ldots, 0,0>
\end{equation}
It is pretty obvious from the definition of $H_k$ that this simple hash function is not a cryptographic hash function. In particular, every digest is its own preimage, since $H_k(y)=y$ for every string of size exactly $k$. Finding preimages is therefore easy, so the property of preimage resistance does not hold.

In addition, it is easy to construct collisions, as all strings $s$ of size $|s|>k$ that share the same $k$-bits ``on the right'' are mapped to the same hash value. This means that this function is not collision resistant, either.

Finally, this hash function does not have a lot of diffusion, as changing bits that are not part of the $k$ right-most bits won't change the digest at all.
\end{example}

Computing cryptographically secure hash functions in pen-and-paper style is possible but tedious. Fortunately, Sage can import the \term{\code{hashlib}} library, which is intended to provide a reliable and stable base for writing Python programs that require cryptographic functions. The following examples explain how to use \code{hashlib} in Sage.

\begin{example}\label{ex:SHA256}An example of a hash function that is generally believed to be a cryptographically secure hash function is the so-called \term{SHA256} hash, which, in our notation, is a function that maps binary strings of arbitrary length onto binary strings of length $256$:
\begin{equation}
SHA256: \{0,1\}^* \to \{0,1\}^{256}
\end{equation}

To evaluate a proper implementation of the $SHA256$ hash function, the digest of the empty string is supposed to be the following:
 
\begin{equation}
SHA256(<>)= {\scriptstyle e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855}
\end{equation}

For better human readability, it is common practice to represent the digest of a string not in its binary form, but in a hexadecimal representation. We can use Sage to compute $SHA256$ and freely transit between binary, hexadecimal and decimal representations. To do so, we import \code{hashlib}'s implementation of SHA256:
\begin{sagecommandline}
sage: import hashlib
sage: test = 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855' 
sage: empty_string = ""
sage: binary_string = empty_string.encode()
sage: hasher = hashlib.sha256(binary_string) 
sage: result = hasher.hexdigest()
sage: type(result)	# Sage represents digests as strings
sage: d = ZZ('0x'+ result) # conversion to an integer
sage: d.str(16) == test	# hash is equal to test vector
sage: d.str(16) # hexadecimal representation
sage: d.str(2) # binary representation
sage: d.str(10) # decimal representation
\end{sagecommandline}
\end{example}

\section{Hashing to Cyclic Groups}\label{sec:hashing-to-groups} 
% https://crypto.stackexchange.com/questions/78017/simple-hash-into-a-prime-field
As we have seen in the previous section, general hash functions map binary strings of arbitrary length onto binary strings of some fixed length. However, it is desirable in various cryptographic primitives to not simply hash to binary strings of fixed length, but to hash into algebraic structures like groups, while keeping (some of) the properties of the hash function, like preimage resistance or collision resistance.

Hash functions like this can be defined for various algebraic structures, but, in a sense, the most fundamental ones are hash functions that map into groups, because they can be easily extended to map into other structures like rings or fields.

To give a more precise definition, let $\G$ be a group and $\{0,1\}^*$ the set of all finite, binary strings, then a \term{hash-to-group} function is a deterministic map
\begin{equation}
H : \{0,1\}^* \to \G
\end{equation}

As the following example shows, hashing to finite cyclic groups can be trivially achieved for the price of some undesirable properties of the hash function:

\begin{example}[Naive cyclic group hash]\label{naive-cyclic-group-hash} Let $\G$ be a finite cyclic group of order $n$. If the task is to implement a hash-to-group function, one immediate approach can be based on the observation that binary strings of size $k$ can be interpreted as integers $z\in\Z$ in the range $0\leq z < 2^k$ using equation \ref{def:binary_representation_integer}.

To be more precise, let $H:\{0,1\}^*\to \{0,1\}^k$ be a hash function for some parameter $k$, $g$ a generator of $\G$, and $s\in\{0,1\}^*$ a binary string. Using equation \ref{def:binary_representation_integer} and notation \ref{string_and_hash_notations}, the following expression is a non-negative integer:

\begin{equation}
z_{H(s)}= H(s)_0\cdot 2^0 + H(s)_1\cdot 2^1 + \ldots + H(s)_k \cdot 2^k
\end{equation}

A hash-to-group function for the group $\G$ can then be defined as a composition of the exponential map $g^{(\cdot)}$ of $g$ with the interpretation of $H(s)$ as an integer:

\begin{equation}
H_{g} : \{0,1\}^* \to \G:\; s \mapsto g^{z_{H(s)}}
\end{equation}

Constructing a hash-to-group function like this is easy for cyclic groups, and it might be good enough in certain applications.\sme{a few examples?} It is, however, almost never adequate in cryptographic applications, as a discrete log relation might be constructible between some hash values $H_g(s)$ and $H_g(t)$, regardless of whether or not $\G$ is DL-secure (see \secname \ref{def:DL-secure}).

To be more precise, a discrete log relation between the group elements $H_g(s)$ and $H_g(t)$ is any element $x\in \Z_n$ such that $H_g(s) = H_g(t)^x$. To see how such an $x$ can be constructed, assume that $z_{H(s)}$ has a multiplicative inverse in $\Z_n$. In this case, the element $x=z_{H(t)}\cdot z_{H(s)}^{-1}$ from $\Z_n$ is a discrete log relation between $H_g(s)$ and $H_g(t)$:
\begin{align*}
g^{z_{H(t)}} & = g^{z_{H(t)}} & \Leftrightarrow\\
g^{z_{H(t)}} & = g^{z_{H(t)}\cdot z_{H(s)}\cdot z_{H(s)}^{-1}} & \Leftrightarrow \\
g^{z_{H(t)}} & = g^{z_{H(s)}\cdot x} & \Leftrightarrow \\
H_g(t) & = (H_g(s))^x
\end{align*}
\end{example}
Therefore, applications where discrete log relations between hash values are undesirable need different approaches. Many of these approaches start with a way to hash into the set $\Z_r$ of modular $r$ arithmetics.

\subsection{\capitalisewords{Pedersen hash}es}
\label{def:Pedersen_hash}
% T. P. Pedersen. “Non-interactive and information-theoretic secure verifiable secret shar- ing”. In: Annual International Cryptology Conference. Springer. 1991, pp. 129–140.
% https://fmouhart.epheme.re/Crypto-1617/TD08.pdf
The so-called \term{\capitalisewords{Pedersen hash function}} \citep{Pedersen92} provides a way to map fixed size tuples of elements from modular arithmetics onto elements of finite cyclic groups in such a way that discrete log relations (see \examplename{} \ref{naive-cyclic-group-hash}) between different images are avoidable. Compositions of a \capitalisewords{Pedersen hash} with a general hash function \eqref{def:hash_function} then provide hash-to-group functions that map strings of arbitrary length onto group elements.

To be more precise, let $j$ be an integer, $\G$ a finite cyclic group of order $n$, and $\{g_1, \ldots, g_j\} \subset \G$ a uniform and randomly generated set of generators of $\G$. Then \term{Pedersen’s hash function} is defined as follows:
\begin{equation}\label{eq:Pedersen_hash}
H_{\{g_1,\ldots,g_j\}} : \left(\Z_r\right)^j \to \G:\; (x_1,\ldots,x_j)\mapsto \Pi_{i=1}^j g_i^{x_i}
\end{equation}

It can be shown that Pedersen’s  hash  function  is  collision-resistant under the assumption that $\G$ is DL-secure (see \secname \ref{def:DL-secure}). It is important to note though, that the following family of functions does not qualify as a \uterm{pseudorandom function family}.

\begin{equation}
\label{Pedersen_not_pseudorandom}
\{H_{\{g_1,\ldots,g_j\}}\;|\;g_1,\ldots,g_j\in \G\}
\end{equation}

From an implementation perspective, it is important to derive the set of generators $\{g_1,\ldots,g_k\}$ in such a way that they are as uniform and random as possible. In particular, any known discrete log relation between two generators, that is, any known $x\in \Z_n$ with $g_h = (g_i)^x$, must be avoided.

\begin{example} To compute an actual Pedersen’s  hash, consider the cyclic group $\Z^*_{5}$ from \examplename{} \ref{example:cyclic_group_F5*}. We know from \examplename{} \ref{example:factor_groupds_of_F*5} that the elements $2$ and $3$ are generators of  $\Z^*_{5}$, and it follows that the following map is a Pedersen's hash function:
\begin{equation}
H_{\{2,3\}}: \Z_4 \times Z_4 \to \Z^*_{5}\;;\; (x,y)\mapsto 2^x \cdot 3^y
\end{equation}

To see how this map can be calculated, we choose the input value $(1,3)$ from $\Z_4 \times Z_4$. Then, using the multiplication table from \eqref{Z5_tables}, we calculate $H_{\{2,3\}}(1,3)= 2^1\cdot 3^3= 2\cdot 2 =4$. 

To see how the composition of a hash function with $H_{\{2,3\}}$ defines a hash-to-group function, consider the $SHA256$ hash function from example \ref{ex:SHA256}. Given some binary string $s\in\{0,1\}^*$, we can insert the two least significant bits $SHA256(s)_0$ and $SHA256(s)_1$ from the image $SHA256(s)$ into $H_{\{2,3\}}$ to get an element in $\F_5^*$. This defines the following hash-to-group function
$$
SHA256\_H_{\{2,3\}}: \{0,1\}^* \to \Z_5^*\;;\; s \mapsto 2^{SHA256(s)_0}\cdot 3^{SHA256(s)_1}
$$
To see how this hash function can be calculated, consider the empty string $<>$. Since we know from the Sage computation in \examplename{} \ref{ex:SHA256}, that $SHA256(<>)_0=1$ and that $SHA256(<>)_1=0$, we get $SHA\_256H_{\{2,3\}}(<>)= 2^1 \cdot 3^0 = 2$. 

Of course, computing $SHA256\_H_{\{2,3\}}$ in a pen-and-paper style is difficult. However, we can easily implement this function in Sage in the following way:
\begin{sagecommandline}
sage: import hashlib
sage: def SHA256_H(x):
....:     Z5 = Integers(5) # define the group type
....:     hasher = hashlib.sha256(x) # compute SHA256
....:     digest = hasher.hexdigest()
....:     z = ZZ(digest, 16) # cast into integer
....:     z_bin = z.digits(base=2, padto=256) # cast to 256bits
....:     return Z5(2)^z_bin[0] * Z5(3)^z_bin[1]
sage: SHA256_H(b"") # evaluate on empty string
sage: SHA256_H(b"SHA") # possible images are {1,2,3}
sage: SHA256_H(b"Math")
\end{sagecommandline}
\end{example}
\begin{exercise}
\label{exercise:Pedersen_hash_1}
Consider the multiplicative group $\Z_{13}^*$ of modular $13$ arithmetic from \examplename{} \ref{ex:Zn*}. Choose a set of $3$ generators of $\Z_{13}^*$, define its associated \capitalisewords{Pedersen hash function}, and compute the \capitalisewords{Pedersen hash} of $(3,7,11)\in \Z_{12}$.
\end{exercise}
\begin{exercise}
Consider the \capitalisewords{Pedersen hash} from \exercisename{} \ref{exercise:Pedersen_hash_1}. Compose it with the $SHA256$ hash function from \examplename{} \ref{ex:SHA256} to define a hash-to-group function. Implement that function in Sage.
\end{exercise}

%\citep{cryptoeprint:2016:492}
\subsection{Pseudorandom Function Families in DDH-secure groups}
% https://fmouhart.epheme.re/Crypto-1617/TD08.pdf
% Proper description in https://eprint.iacr.org/2016/492.pdf sec 3.3
As noted in \ref{def:Pedersen_hash}, the family of Pederson's hash functions, parameterized by a set of generators $\{g_1,\ldots,g_j\}$ does not qualify as a family of pseudorandom functions, and should therefore not be instantiated as such. To see an example of a proper family of pseudorandom functions in groups where the decisional Diffie--Hellman assumption (see \secname \ref{def:DDH-secure}) is assumed to hold true, let $\G$ be a DDH-secure cyclic group of order $n$ with generator $g$, and let $\{a_0,a_1,\ldots,a_k\}\subset \Z_{n}^*$ be a uniform randomly generated set of numbers invertible in modular $n$ arithmetics. Then a family of pseudorandom functions, parameterized by the \uterm{seed} $\{a_0,a_1,\ldots,a_k\}$ is given as follows:
\begin{equation}
\label{prf_in_cyclic_group}
F_{\{a_0,a_1,\ldots,a_k\}}: \{0,1\}^{k} \to \G:\; (b_1,\ldots,b_k)\mapsto g^{a_0\cdot \Pi_{i=1}^k a_i^{b_i}}
\end{equation}
\begin{exercise} Consider the multiplicative group $\Z_{13}^*$ of modular $13$ arithmetic from \examplename{} \ref{ex:Zn*} and the parameter $k=3$. Choose a generator of $\Z_{13}^*$, a seed and \uterm{instantiate} a member of the family given in \eqref{prf_in_cyclic_group} for that seed. Evaluate that member on the binary string $<1,0,1>$.
\end{exercise}
%\begin{example}[p\&{}p-$\F_{13}$-drop-hash]We can consider the same pen\&paper hash function from XXX and define another hash into $\F_{13}$, by deleting the first leading bit from the hash. The result is then a $3$-digit number and therefore guaranteed to be smaller then $13$, since $13$ is equal to $(1101)$ in base $2$.

%Considering the string $S=(1110011101110011)$ from example XXX again we know $\mathcal{H}_{PaP}(S)=(1110)$ and stripping of the leading bit we get $(110)_{10}=6$ as our hash value.

%As we can see this hash function has the drawback of an uneven distribution in $\F_{13}$. In fact this hash function is unable to map to values from $\{8,9,10,11,12\}$ as those numbers have a $1$-bit in position $4$. However as we will see in XXX, this hash is cheaper to implement as a circuit as no expensive modulus operation has to be used.
%\end{example}

\section{Hashing into Modular Arithmetic}
\label{hash-to-modular-arithmetics}
As we have seen in \secname{} \ref{sec:hashing-to-groups}, various constructions for hashing to groups are known and used in applications. As commutative rings are commutative groups when we disregard the multiplication, hash-to-group constructions can be applied for hashing into commutative rings.  We review some frequently used applications below.

\sme{put subsubsection title here}

One of the most widely used applications of hash-into-ring constructions are hash functions that map into the ring $\Z_n$ of modular $n$ arithmetics for some modulus $n$. Different approaches of constructing such a function are known, but probably the most widely used ones are based on the insight that the images of general hash functions can be interpreted as binary representations of integers, as explained in \examplename{} \ref{naive-cyclic-group-hash}.

It follows from this interpretation that one simple method of hashing into $\Z_n$ is constructed by observing that if $n$ is a modulus with a bit length \eqref{def:binary_representation_integer} of $k=|n|$, then every binary string $<b_0,b_1,\ldots,b_{k-2}>$ of length $k-1$ defines an integer $z$ in the rage $0\leq z \leq 2^{k-1}-1< n $:
\begin{equation}
z = b_0\cdot 2^0 + b_1\cdot 2^1 + \ldots + b_{k-2}\cdot 2^{k-2}
\end{equation}
Now, since $z<n$, we know that $z$ is guaranteed to be in the set $\{0,1,\ldots,n-1\}$, and hence it can be interpreted as an element of $\Z_n$. Consequently, if $H:\{0,1\}^*\to\{0,1\}^{k-1}$ is a hash function, then a hash-to-ring function can be constructed as follows:
\begin{equation}\label{eq:hash-Zr}
H_{|n|_2-1}: \{0,1\}^* \to \Z_r: \; s \mapsto
H(s)_0\cdot 2^0 + H(s)_1\cdot 2^1 + \ldots + H(s)_{k-2}\cdot 2^{k-2}
\end{equation}

A drawback of this hash function is that the distribution of the hash values in $\Z_n$ is not necessarily uniform. In fact, if $n$ is larger than $2^{k-1}$, then by design $H_{|n|_2-1}$ will never hash onto values $z\geq 2^{k-1}$. Using this hashing method therefore generates approximately uniform hashes only if $n$ is very close to $2^{k-1}$. In the worst case, when $n=2^k-1$, it misses almost half of all elements from $\Z_n$.

An advantage of this approach is that properties like preimage resistance or collision resistance (see \secname{} \ref{sec:hash-functions}) of the original hash function $H(\cdot)$ are preserved.
\begin{example} To analyze a particular implementation of a $H_{|n|_2-1}$ hash function, we use a $5$-bit truncation of the $SHA256$ hash from \examplename{} \ref{ex:SHA256} and define a hash into $\Z_{16}$ as follows:
$$
H_{|16|_2-5}: \{0,1\}^* \to \Z_{16}:\; s\mapsto
SHA256(s)_0\cdot 2^0 + SHAH256(s)_1\cdot 2^1 + \ldots + SHA256(s)_4\cdot 2^4
$$
Since $k=|16|_2=5$ and $16-2^{k-1}=0$, this hash maps uniformly onto $\Z_{16}$. We can use Sage to implement it:
\begin{sagecommandline}
sage: import hashlib
sage: def Hash5(x):
....:     Z16 = Integers(16)
....:     hasher = hashlib.sha256(x) # compute SHA56
....:     digest = hasher.hexdigest()
....:     d = ZZ(digest, base=16) # cast to integer
....:     d = d.str(2)[-4:] # keep 5 least significant bits
....:     d = ZZ(d, base=2) # cast to integer
....:     return Z16(d) # cast to Z16
sage: Hash5(b'')
\end{sagecommandline}
We can then use Sage to apply this function to a large set of input values in order to plot a visualization of the distribution over the set $\{0,\ldots,15\}$. Executing over $500$ input values gives the following plot:
\begin{sagesilent}
H1 = list_plot([Hash5(ZZ(k).str(2).encode('utf-8')) for k in range(500)])
\end{sagesilent}
\begin{center}
\sageplot[scale=.5]{H1}
\end{center}
To get an intuition of uniformity, we can count the number of times the hash function $H_{|16|_2-1}$ maps onto each number in the set $\{0,1,\ldots,15\}$ in a loop of $100000$ hashes, and compare that to the ideal uniform distribution, which would map exactly 6250 samples to each element. This gives the following result:
\begin{sagesilent}
arr = []
arr = [0 for i in range(16)]
for i in range(100000):
    arr[Hash5(ZZ(i).str(2).encode('utf-8'))] +=1
H2 = list_plot(arr, ymin=0,ymax=10000)
\end{sagesilent}
\begin{center}
\sageplot[scale=.5]{H2}
\end{center}
The lack of uniformity becomes apparent if we want to construct a similar hash function for $\Z_n$ for any other $5$ bit integer $n$ in the range $17\leq n \leq 31$. In this case, the definition of the hash function is exactly the same as for $\Z_{16}$, and hence, the images will not exceed the value $15$. So, for example, even in the case of hashing to $\Z_{31}$, the hash function never maps to any value larger than $15$, leaving almost half of all numbers out of the image range.
\begin{sagesilent}
arr = []
arr = [0 for i in range(31)]
for i in range(100000):
    arr[Hash5(ZZ(i).str(2).encode('utf-8'))] +=1
H3 = list_plot(arr, ymin=0,ymax=10000)
\end{sagesilent}
\begin{center}
\sageplot[scale=.5]{H3}
\end{center}
\end{example}

\sme{put subsubsection title here}

A second widely used method of hashing into $\Z_n$ is constructed by observing the following: If $n$ is a modulus with a bit-length of $|n|_2=k_1$, and $H:\{0,1\}^*\to \{0,1\}^{k_2}$ is a hash function that produces digests of size $k_2$, and $k_2\geq k_1$, then a hash-to-ring function can be constructed by interpreting the image of $H$ as a binary representation of an integer, and then taking the modulus by $n$ to map into $\Z_n$:.
\begin{equation}
H'_{mod_n}: \{0,1\}^* \to \Z_n: \; s \mapsto
\Zmod{\left(H(s)_0\cdot 2^0 + H(s)_1\cdot 2^1 + \ldots + H(s)_{k_2}\cdot 2^{k_2}\right)}{n}
\end{equation}

A drawback of this hash function is that computing the modulus requires some computational effort. In addition, the distribution of the hash values in $\Z_n$ might not be uniform, depending on the number $\Zmod{2^{k_2+1}}{n}$. An advantage of this function is that potential properties of the original hash function $H(\cdot)$ (like preimage resistance or collision resistance) are preserved, and the distribution can be made almost uniform, with only negligible bias depending on what modulus $n$ and images size $k_2$ are chosen.
\begin{example} To give an implementation of the $H_{mod_n}$ hash function, we use  $k_2$-bit truncation of the $SHA256$ hash from \examplename{} \ref{ex:SHA256}, and define a hash into $\Z_{23}$ as follows:
\begin{multline*}
H_{mod_{23},k_2}: \{0,1\}^* \to \Z_{23}:\; \\
s\mapsto
\Zmod{\left(SHA256(s)_0\cdot 2^0 + SHAH256(s)_1\cdot 2^1 + \ldots + SHA256(s)_{k_2}\cdot 2^{k_2}\right)}{23}
\end{multline*}
We want to use various instantiations of $k_2$ to visualize the impact of truncation length on the distribution of the hashes in $\Z_{23}$. We can use Sage to implement it as follows:
\begin{sagecommandline}
sage: import hashlib
sage: Z23 = Integers(23)
sage: def Hash_mod23(x, k2):
....:     hasher = hashlib.sha256(x.encode('utf-8')) # Compute SHA256
....:     digest = hasher.hexdigest()
....:     d = ZZ(digest, base=16) # cast to integer
....:     d = d.str(2)[-k2:] # keep k2+1 LSB
....:     d = ZZ(d, base=2) # cast to integer
....:     return Z23(d) # cast to Z23
\end{sagecommandline}

We can then use Sage to apply this function to a large set of input values in order to plot visualizations of the distribution over the set $\{0,\ldots,22\}$ for various values of $k_2$, by counting the number of times it maps onto each number in a loop of $100000$ hashes. We get the following plot:
\begin{sagesilent}
arr1 = []
arr1 = [0 for i in range(23)]
for i in range(100000):
    arr1[Hash_mod23(ZZ(i).str(2),5)] +=1
H3 = list_plot(arr1, ymin=0,ymax=10000,color='red', legend_label='k2=5')
arr2 = []
arr2 = [0 for i in range(23)]
for i in range(100000):
    arr2[Hash_mod23(ZZ(i).str(2),7)] +=1
H4 = list_plot(arr2, ymin=0,ymax=10000,color='blue', legend_label='k2=7')
arr3 = []
arr3 = [0 for i in range(23)]
for i in range(100000):
    arr3[Hash_mod23(ZZ(i).str(2),9)] +=1
H5 = list_plot(arr3, ymin=0,ymax=10000,color='yellow', legend_label='k2=9')
arr4 = []
arr4 = [0 for i in range(23)]
for i in range(100000):
    arr4[Hash_mod23(ZZ(i).str(2),16)] +=1
H6 = list_plot(arr4, ymin=0,ymax=10000,color='black', legend_label='k2=16')
\end{sagesilent}
\begin{center}
\sageplot[scale=.6]{H3+H4+H5+H6}
\end{center}
\end{example}

\subsection{The ``try-and-increment'' method}\label{def:try_and_increment_hash}

A third method that can sometimes be found in implementations is the so-called \term{``try-and-increment'' method}. To understand this method, we define an integer $z\in\Z$ from any hash value $H(s)$ as we did in the previous methods:

\begin{equation}
z = H(s)_0\cdot 2^0 + H(s)_1\cdot 2^1 + \ldots + H(s)_{k-1}\cdot 2^{k}
\end{equation}

Hashing into $\Z_n$ is then achievable by first computing $z$, and then trying to see if $z\in\Z_n$. If it is, then the hash is done; if not, the string $s$ is modified in a deterministic way and the process is repeated until a suitable element $z\in\Z_n$ is found. A suitable, deterministic modification could be to concatenate the original string by some bit counter. A ``try-and-increment'' algorithm would then work like in \algname{} \ref{alg_try_and_increment}.
\begin{algorithm}\caption{Hash-to-$\Z_n$}
\label{alg_try_and_increment}
\begin{algorithmic}[0]
\Require $n \in \Z$ with $|n|_2=k$ and $s\in\{0,1\}^*$
\Procedure{Try-and-Increment}{$n,k,s$}
\State $c \gets 0$
\Repeat
\State $s' \gets s||c\_bits()$
\State $z \gets H(s')_0\cdot 2^0 + H(s')_1\cdot 2^1 + \ldots + H(s')_{k}\cdot 2^{k}$
\State $c\gets c+1$
\Until{$z<n$}
\State \textbf{return} $x$
\EndProcedure
\Ensure $ z\in \Z_n$
\end{algorithmic}
\end{algorithm}

Depending on the parameters, this method can be very efficient. In fact, if $k$ is sufficiently large and $n$ is close to $2^{k+1}$, the probability for $z<n$ is very high, and the repeat loop will almost always be executed a single time only. A drawback is, however, that the probability of having to execute the loop multiple times is not zero.


\section{Hashing into prime fields}\label{hashing-prime-fields}
In cryptography, a crucial challenge is the ability to hash data to specific subsets of elliptic curves. In \ref{chap:elliptic_curves}, we will explore how these curves are often defined over prime fields, which means that hashing to a curve may involve hashing to the prime field first. Therefore, it is crucial to have a clear understanding of how to hash into prime fields.

Previously, in \ref{hash-to-modular-arithmetics}, we examined various methods of hashing into modular arithmetic rings $\Z_n$ for any $n>1$. Since prime fields are a type of modular arithmetic ring, all the hashing methods for $\Z_n$ rings can be utilized for hashing into prime fields.


\subsection{MiMC Symmetric Cyphers} 
In the context of zk-SNARKs, having access to cryptographically secure hash functions that map strings of elements from prime fields to elements of prime fields is essential:
\begin{equation}
H_p : (\F_p)^* \to \F_p
\end{equation}
This is because many real-world zk-SNARKs are based on computational models that use elements of prime fields as their most basic computational units, similar to how traditional computational models use bits as their most basic computation units. In these cases, it can be computationally expensive to implement common hash functions like SHA2 in the new models.

One family of a hash function suitable for use with zk-SNARKs is called MimC hashes, that was introduced in 2016 by Albrecht et al. \cite{albrecht-16}. MimC is designed to have low multiplicative complexity, which means that the number of multiplications is low in its execution. This is especially useful in protocols like Groth\_16, where multiplications are more costly than additions, as we will see in \ref{sec:gorth_16}.

Central to the family of MimC hashes are the MimC symmetric cyphers, which are invertible functions, that take a key and a plaintext and transform the plaintext reversibly into a cyphertext. 

To be more specific, let $\F_p$ be a prime field with prime modulus $p\in\Prim$, let $n$ be the smallest natural number such that $gcd(n, p-1) = 1$, let $r$ be the smallest integer greater than or equal to $\frac{log(p)}{log_2(3)}$  and let $C=\{c_i\in \F_p \;|\; 0\leq i < r\; , c_0 =0 \}$ be a set of randomly generated field elements. Then MimC's \term{symmetric (or block) cypher} for key $k\in \F_p$ is defined as follows: 
\begin{equation}
\label{def:mimc-block-cypher}
E_k(\cdot): \F_p \to \F_p\; x \mapsto F_{r-1}\left(\cdots F_1\left(F_0(x)\right)\ldots\right)
\end{equation}
In this expression the function $F_i$ is defined as $F_i(x)= (x+k+c_i)^n$ for all natural numbers $0\leq 1 < r$.
\begin{example}[MimC block cypher in $\F_{13}$]
\label{ex:mimc-block-cypher-f13}
 To give a better understanding of the MimC block cypher construction, consider the prime field $\F_{13}$ from exercise \ref{prime_field_F13}. In order to construct a MimC block cypher for plaintexts $x\in \F_{13}$, we first have to find the smallest integer $n\geq 2$, such that the greatest common divisior of $n$ and $12=(13-1)$ is $1$. Since the integers $2$, $3$ and $4$ are all factors of $12$, we conclude $n=5$. 

Choosing $n$ coprime to $p-1$, i.e $gcd(n,12)=1$ in our case, guarantees that the function $(\cdot)^n: \F_{13}\to \F_{13}\; x \mapsto x^n$ is invertible and hence a 1:1 correspondence (also called a permutation).

In the next step we have to compute the round number $r$, which defines the number of functions $F_i$ that occur in the nested expression \ref{def:mimc-block-cypher}. We use sage's ceil function, which computes the smallest integer greater than or equal to $\frac{log(p)}{log_2(3)}$ and get:
\begin{sagecommandline}
sage: ceil(log(13)/log(3,2))  
\end{sagecommandline}
This implies that the number of rounds is $r=2$, from which follows that we need to choose a single 'random' constant $c_1\in \F_{13}$, which we decide to be $c_1=7$. For given key $k\in \F_{13}$ our MimC block cypher is then defined as the following polynomial:
$$
E_k(\cdot): \F_{13} \to \F_{13}: x \mapsto  ( (x+k)^5 + k + 7 )^5 + k 
$$
To see how the block cypher is computed consider the key $k=12$ and the cleartext $x=7$. In this case the expression $E_{12}(7)$ is computed as follows:
\begin{align*}
E_{12}(7) = & ( (7 + 12)^5 + 12 + 7 )^5 + 12 \\ 
          = & ( (6)^5 + 12 + 7 )^5 + 12
          = (2 + 12 + 7)^5 + 12
          =  8 + 12 \\
          = & 7
\end{align*}
We can implement the block cypher as a function in sage and apply it to all elements in $\F_{13}$ to see that it is indeed a permutation (a 1:1 correspondence): 
\begin{sagecommandline}
sage: F13= GF(13)
sage: # define our block cypher
sage: def MimC(x,k):
....:     return ( (x+k)^5 + k + F13(7) )^5 + k  
sage: # apply to the field
sage: L_F13 = [x for x in F13]
sage: Perm_F13_k0 = [MimC(x,0) for x in L_F13] # key=0
sage: Perm_F13_k12 = [MimC(x,12) for x in L_F13] # key=12
sage: L_F13
sage: Perm_F13_k0
sage: Perm_F13_k12
\end{sagecommandline}
\end{example}
\subsubsection{MiMC Hash Functions} 
In the literature, various constructions such as the Merkle–Damgård construction \cite{Mirvaziri-07}, the Miyaguchi–Preneel construction \cite{Mirvaziri-07}, or the Sponge construction \cite{Bertoni-01} are known to transform block ciphers like \ref{def:mimc-block-cypher} into a cryptographic hash function or a one-way compression function in general.

To give an example, lets look at how the Miyaguchi–Preneel construction can be used to construct a cryptographic hash function that compresses strings of field elements $X:=\langle x_1,x_2,\ldots, x_j\rangle\in (\F_p)^*$ of arbitrary length $j\in \N$ onto a single field element $H(X)$.
\begin{algorithm}\caption{Miyaguchi–Preneel MimC Hash}
\label{alg_mpmimc}
\begin{algorithmic}[0]
\Require $E_{(\cdot)}(\cdot)$ MimC block cypher for field $\F_p$
\Require $X=\langle x_1,x_2,\ldots, x_j\rangle\in (\F_p)^*$ message 
\Procedure{MimC-Hash}{$X$}
\State $H_0 \gets 0$ \Comment{initialize}
\For{$1\leq i \leq j$}
	\State $H_i = E_{H_{i-1}}(x_i) + H_{i-1} + x_i $ \Comment{update}
\EndFor
\State \textbf{return} $H_j$ \Comment{digest}
\EndProcedure
\end{algorithmic}
\end{algorithm}
\begin{example}[MimC Hash Function in $\F_{13}$]
\label{ex:mimc-hash-f13} To get a better understanding of the Miyaguchi–Preneel construction, consider the MimC block cypher from example \ref{ex:mimc-block-cypher-f13}. We can use this cypher and compute a Miyaguchi–Preneel MimC hash of the string $\langle 3, 7\rangle\in (\F_{13})^*$ using the computations from \ref{ex:mimc-block-cypher-f13}:
\begin{align*}
H(\langle 3, 7\rangle): & \#\; H_0 = 0\\
                        & \#\; H_1 = E_0(3) + 0 + 3 = 12 \\
                        & \#\; H_2 = E_{12}(7) + 12 + 7 = 0 \\
                       = &\;  0
\end{align*}
Since we will use this hash function extensively in our Plonk example XXX, we implement the hash function according to algorithm \ref{alg_mpmimc} in sage:
\begin{sagecommandline}
sage: def MimC_Hash(m):
....:     H= F13(0) # init
....:     for x in m:
....:         H = MimC(x,H) + H + x # update
....:     return H # digest
sage: # apply to message string 
sage: m = []
sage: m0 = [F13(0) ]
sage: m1 = [F13(3), F13(7) ]
sage: MimC_Hash(m) # digest of empty string
sage: MimC_Hash(m0)
sage: MimC_Hash(m1)
\end{sagecommandline}
\end{example}
\begin{exercise}
\label{ex:mimc-f43}
Consider the prime field $\F_{43}$ and define a Miyaguchi–Preneel MimC hash function $H: (\F_{43})^*\to \F_{43}$.
\end{exercise}
\subsection{Algebraic Sponge Constructions}
Cryptographic sponge constructions \cite{bert-08} builds upon symmetric cyphers and can be used to achieve various goals such as encryption, authentication, or hashing. 

At their core, cryptographic sponges consist of a set of algorithms that operate on a state $S$ comprising $n$ bits, which can be further divided into an inner portion of $c$ bits (known as the \term{capacity}) and an outer portion of $r$ bits (referred to as the \term{rate}), with the equation $n=c+r$ holding true. 
\begin{equation}
S = [\underbrace{b_0,\ldots,b_{r-1}}_{\text{rate}} \;||\; \underbrace{b_r,\ldots,b_{r+c}}_{\text{capacity}}]
\end{equation}
Sponge constructions generally comprises two primary functions: \term{absorb} and \term{squeeze}. In the absorb phase, data is compressed into the state, with $r$ bits processed at each step, and this process is intertwined with the evaluation of a pseudo-random permutation $P$ that permutes the full $n$-bit state. Following absorb phases, in a squeeze phase, a digest is extracted from the state, with $r$ bits extracted at each step, once again interleaved with the evaluation of the same pseudo-random permutation $P$.

\term{Algebraic sponge constructions} like the Poseidon hash functions \cite{lor-19} or the MimC-sponges \cite{albrecht-16} adopt a similar mechanism but for a state of $n$ field elements, partitioned into an inner section of capacity $c$ field elements and an outer section of $r$ field elements, adhering to the same constraint $n=r+c$. The pseudo-random permutation $P$ used in these constructions generally maintains the algebraic structure by permuting $n$ field elements to another set of $n$ field elements.  

To be more precise, let $n$, $r$ and $c \in \N$ be natural numbers such that $n = r + c$ where $r$ is called the \term{algebraic rate} and $c$ is called the \term{algebraic capacity}. Furthermore, let $\F$ denote a finite field, and let $P: \F^n \to \F^n$ represent a pseudo-random permutation. Then an \term{algebraic sponge construction}, also known as an algebraic sponge function or algebraic sponge object, is composed of a \textit{state} denoted as $S$ and a set of three algorithms:
\begin{itemize}\label{def:algebraic-sponge}
\item (Initialization-Phase): $S\leftarrow \textsc{Start}(c,r)$: Algorithm $\textsc{Start}$ takes a capacity $c$ and rate $r$ as input and computes an initial state $S=[0,\ldots,0\;||\;0,\ldots,0]$ consisting of $r$ outer field elements and
$c$ inner field elements, where all field elements are initialized to $0\in\F$.
\item (Absorb-Phase): $S\leftarrow \textsc{Absorb}((x_1,\ldots,x_r),S)$: For any given state of the sponge construction $S=[o_1,\ldots,o_r\;||\;i_1,\ldots,i_c]$ and sequence of field elements $(x_1,\ldots,x_r)$, algorithm $\textsc{Absorb}$ computes an update of $S$ as follows: 
\begin{equation}
S \leftarrow P\left([o_1+x_1,\ldots,o_r+x_r\;||\;i_1,\ldots,i_c]\right)
\end{equation}
\item (Squeeze-Phase): $((x_1,\ldots,x_r), S)\leftarrow\textsc{Squeeze}(S)$: For any given state of the sponge construction $S=[o_1,\ldots,o_r\;||\;i_1,\ldots,i_c]$, algorithm $\textsc{Squeeze}$ generates a sequence of field elements $(x_1,\ldots,x_r)$ and an updated state $S$ as follows: 
\begin{equation}
((x_1,\ldots,x_r), S) \leftarrow ((o_1,\ldots,o_r),P([o_1,\ldots,o_r\;||\;i_1,\ldots,i_c]))
\end{equation}
\end{itemize}
This is a simplified definition and more nuanced algebraic sponge constructions like \cite{aum-23} are known in the literature, designed to be more secure in real world applications.

\subsubsection{The Poseidon Sponge Construction}\label{sec:poseidon-sponge}
Based on our general definition of algebraic sponges \ref{def:algebraic-sponge}, the question remains how to define the required pseudo-random permutations $P$ that map $n$ fields elements in a a 1:1 correspondence onto $n$ field elements. As demonstrated in \cite{lor-19}, this can be achieved through a two-step process, which bears resemblance to well-known constructions used in symmetric block ciphers, involving S-boxes and MDS matrices\footnote{The process follows well know constructions for symmetric block cyphers based on S-boxes and MDS matrices. To understand this in detail compare \cite{lor-19} e.g. against the definition of AES as in \cite{duv-18}}:

In the initial step a so called \term{S-Box} function $\textsc{S-Box}:\F \to \F$ is constructed that maps field elements onto field elements in a 1:1 correspondence. As previously discussed \ref{def:mimc-block-cypher}, the MimC block cipher is a suitable candidate for constructing such a map. In the case of Poseidon, a simplified variation of MimC is chosen, which performs a single round with a key value of $k=0$ and vanishing constants $c_i=0$. Consequently, in Poseidon, the S-Boxes are defined as follows:
\begin{equation}
\textsc{S-Box}: \F_p \to \F_p\;;\; x \mapsto x^k
\end{equation}
where $k$ is the smallest integer, such that $gcd(k,p-1)=1$, ensuring that $\textsc{S-Box}$ is a 1:1 correspondence (a permutation). 

In the second step, we construct what is known as a \term{maximum distance separable} (MDS) matrix to expand the S-Box into the desired permutation $\textsc{P}:\F^n \to \F^n$. Since we haven't previously defined matrices in this book, we can conceptualize an MDS matrix as a collection of $n^2$ field elements, represented as $\textsc{Mds} = \{m_{i,j}\in \F \; | \; 1 \leq i, j \leq n\}$, such that permutation $P$ can be defined as follows:
\begin{equation}
\textstyle\textsc{P}:\F^n \to \F^n\;;\; (x_1,\ldots,x_n) \mapsto
(\sum_{j=1}^n m_{1,j}\cdot\textsc{S-Box}(x_j),\ldots, \sum_{j=1}^n m_{n,j}\cdot\textsc{S-Box}(x_j))
\end{equation}
It's important to note though, that not every set of field elements $\{m_{i,j}\in \F \; | \; 1 \leq i, j \leq n\}$ qualifies as a suitable MDS matrix, and developers must exercise caution in their design choices. One fundamental option is to define $m_{i,j}=(x_i+y_j)^{-1}$ for $x_i\in \F$ and $y_j\in \F$, with the condition that $x_i \neq y_j$ and $x_i + y_j \neq 0$ for all indices $i$ and $j$.

Once a properly constructed permutation $P$ is available, the algorithms outlined in \ref{def:algebraic-sponge} come together to form a sponge construction known as a \term{Poseidon sponge}.
\subsubsection{Poseidon Sponges for the Moon-Math-Curve}
In this section, we will apply the Poseidon protocol from the previous section \ref{sec:poseidon-sponge} and discuss two sponge constructions for the moon-math-curve \ref{BLS6}, which are crucial for complex multi-move Fiat Shamir constructions as explained in \ref{sec:Fiat-Shamir}. The Poseidon protocols was  chosen for its simplicity in pen and paper computations due to its low algebraic complexity.

To elaborate, the first construction, denoted as $BLS6\_6\_\textsc{scalar}$, will absorb and squeeze field elements from the scalar field $\F_{13}$ of the moon-math-curve $BLS6\_6$ and the second construction $BLS6\_6\_\textsc{base}$, will absorb field elements from the base field $\F_{43}$ of $BLS6\_6$ and squeeze field elements from $\F_{13}$. 

To do so we first decide on the algebraic rate $r=1$ and the capacity $c=1$ for both constructions. These values are commonly chosen in real-world applications. This selection implies that the absorb function can absorb one field element at a time into the sponge, and correspondingly, the squeeze function will provide one element at a time as well.
\paragraph{The Scalar Field Sponge} In order to construct $BLS6\_6\_\textsc{scalar}$, we first observe that since $gcd(5,12)=1$ the Poseidon S-Box function is defined as 
$$
\textsc{S-Box}: \F_{13} \to \F_{13}\;;\; x \mapsto x^5
$$
Since both, the capacity and the rate are $1$ in our case,  the sponge state consists of $2$ elements $S=[o\;||\;i]$ and the permutation $P$ therefore has to be a map 
$$ 
P : \F_{13}\times \F_{13} \to \F_{13}\times \F_{13}
$$
From this follows that our MDS matrix is a set of four field elements $M = \{m_{1,1}, m_{1,2}, m_{2,1},  m_{2,2}\}$, which we can define as explained in \ref{sec:poseidon-sponge}. We follow the approach and choose four field elements
$x_1=1$, $x_2=2$, $y_1=3$ and $y_2=4$, which gives
\begin{align*}
\textsc{Mds} = & \{(x_1+y_1)^{-1},(x_1+y_2)^{-1}, (x_2+y_1)^{-1},(x_2+y_2)^{-1}\}
    =  \{4^{-1},5^{-1}, 5^{-1},6^{-1}\}\\
    = & \{10,8,8,11\}
\end{align*}
Combining our S-Box definition with our choice of an MDS matrix, we get the following state permutation function for the scalar field sponge construction:
\begin{align}
P([o\;||\;i]) &= [10\cdot \textsc{S-Box}(o_1) + 8\cdot \textsc{S-Box}(i_1) \;||\; 8\cdot \textsc{S-Box}(o_1) + 11\cdot \textsc{S-Box}(i_1)]\\
  &= [10\cdot o_1^5 + 8\cdot i_1^5 \;||\; 8\cdot o_1^5 + 11\cdot i_1^5]
\end{align}
With our state permutation ad hand, we can define the three algorithms of the scalar field moon-math-sponge $BLS6\_6\_\textsc{scalar}$:
\begin{itemize}
\item $\textsc{Start}(1,1)$: The algorithm computes the initial state $S=[0\;||\;0]$  consisting of a single zero field element from $F_{13}$ for the rate and a single zero field element for the capacity.
\item $\textsc{Absorb}(x,S)$: Given a state $S=[o_1\;||\;i_1]$ and a field element $x\in \F_{13}$ the sponge absorbs the field element and computes a new state in the following way:
$$
S \leftarrow [10\cdot (o_1+x)^5 + 8\cdot i_1^5 \;||\; 8\cdot (o_1+x)^5 + 11\cdot i_1^5]
$$
\item $\textsc{Squeeze}(S)$: For any given state $S=[o_1\;||\;i_1]$, algorithm $\textsc{Squeeze}$ computes a field element $x$ and an updated state $S$ as follows:
\begin{align*}
x \leftarrow & o_1 \\
S \leftarrow & [10\cdot o_1^5 + 8\cdot i_1^5 \;||\; 8\cdot o_1^5 + 11\cdot i_1^5]
\end{align*} 
\end{itemize}
Since we will use the scalar field moon-math-sponge frequently for Fiat Shamir transformations (see e.g. XXX) we will implement it in sage
\begin{sagecommandline}
sage: F13 = GF(13) # prime field
sage: def mm_scalar_sponge_start():
....:     return (F13(0), F13(0))
sage: def mm_scalar_sponge_absorb(x, S):
....:     o, i = S
....:     return (F13(10)*(o+x)^5 + F13(8)*i^5, F13(8)*(o+x)^5 + F13(11)*i^5)
sage: def mm_scalar_sponge_squeeze(S):
....:     o, i = S
....:     return o, (F13(10) * o^5 + F13(8) * i^5, F13(8) * o^5 + F13(11) * i^5)
sage: S = mm_scalar_sponge_start()
sage: S = mm_scalar_sponge_absorb(F13(4), S)
sage: mm_scalar_sponge_squeeze(S)
\end{sagecommandline}

\paragraph{The Base Field Sponge}
Since the base field sponge is required to absorb from base and squeeze to scalar, we have to make choices. Either the internal state consist of base field elements o scalar fiel. If base field then we have to project onto scalar and since base larger we get trouble with collisions. Hence state scalar. But naive projection gives also collision EXAMPLE. HENCE MORE ELABORATE


 
For the base field sponge we need a way to map base

Applications like Plonk often require the input values to be from other algebraic types the the return type of the squeeze function. For example it is common to consider curve points defined over different fields as input values. 

In cases like this it is common practice to simply project the elements of the other field onto the field of the sponge construction. It should be noted however that this is potentially dangerous as it opens to possibility of collision attacks. 

To see this consider the two elements from the prime field $\F_{41}$....

TODO put this under attacks on Fiat Shamir:

Consider the elements $(35, 28) , (35, 15) \in \G_1[13]$. If we apply a simple mod reduction they are a collision under the moon-math sponge construction, because 
$(\Zmod{35}{13}, \Zmod{28}{13}) = (9,2) = (\Zmod{35}{13}, \Zmod{15}{13})$. So
 




   

\begin{exercise}
Define a Poseidon sponge construction for the parameters $r=2$, $c=1$ over the prime field $\F_{41}$.
\end{exercise}


\section{Hashing to Curves} Elliptic curve cryptography frequently requires the ability to hash data onto elliptic curves. If the order of the curve is not a prime number, hashing to prime order subgroups is of importance, too and in the context of pairing-friendly curves, it is sometimes necessary to hash specifically onto the pairing group $\G_1$ or $\G_2$ as introduced in \ref{sec:pairing_groups}.

As we have seen in section \ref{sec:hashing-to-groups}, some general methods are known for hashing into finite cyclic groups and since elliptic curves over finite fields are finite and cyclic groups, those methods can be utilized in this case, too. However, in what follows we want to describe some methods specific to elliptic curves that are frequently used in real-world applications. 

\subsection{Try-and-increment hash functions}
One of the most straight-forward ways of hashing onto an elliptic curve point in a secure way is to use a cryptographic hash function together with one of the hashing into modular arithmetics methods as described in section \ref{hash-to-modular-arithmetics}.

Both constructions can be combined in such a way that the image provides an element of the base field of the elliptic curve together with a single auxiliary bit. The base field element can then be interpreted as the $x$-coordinate of a potential curve point, and the auxiliary bit can be used to determine one of the two possible $y$ coordinates of that curve point as explained in \ref{sec:affine_point_compression}.

Such an approach would be deterministic and easy to implement, and it would conserve the cryptographic properties of the original hash function. However, not all $x$ coordinates generated in such a way will result in quadratic residues when inserted into the defining equation. It follows that not all field elements give rise to actual curve points. 

In fact,
% https://www.cs.umd.edu/users/gasarch/TOPICS/res/burgess.pdf
on a prime field, only half of the field elements are quadratic residues. Hence, assuming an even distribution of the hash values in the field, this method would fail to generate a curve point in about half of the attempts. 

One way to account for this problem is the following so-called \term{try-and-increment} method. Instead of simply hashing a binary string $s$ to the field, this method use a try-and-increment hash to the base field as described in \ref{def:try_and_increment_hash} in combination with a single auxiliary bit derived from the underlying cryptographic hash function.

If any try of hashing to the field does not result in a field element or a valid curve point, the counter is incremented, and the hashing is repeated. This is done until a valid curve point is found (see the algorithm below).

\begin{algorithm}\label{alg:hash-to-e}\caption{Hash-to-$E(\F_p)$}
\begin{algorithmic}[0]
\Require $p \in \Z$ with $|p|=k$ and $s\in\{0,1\}^*$
\Require Curve equation $y^2 = x^3 + ax +b$ over $\F_p$
\Procedure{Try-and-Increment}{$r,k,s$}
\State $c \gets 0$	\Comment{Try-and-Increment counter}
\Repeat
\State $s' \gets s||Bits(c)$
\State $x \gets H(s')_0\cdot 2^0 + H(s')_1\cdot 2^1 + \ldots + H(s')_{k}\cdot 2^{k}$ \Comment{potential $x$}
\State $y^2 \gets z^3 + a\cdot z + b$ \Comment{potential $y^2$}
\State $c\gets c+1$
\Until{$x<p$ and $\Zmod{(y^2)^{\frac{p-1}{2}}}{r}=1$ } \Comment{Check $x$ in field and $y^2$ has root}
\If {$H(s')_{k+1} == 0$} \Comment{auxiliary bit decides root}
\State $y \gets y'\in \sqrt{y^2}$ with $0\leq y' \leq (p-1)/2$
\Else 
\State $y \gets y'\in \sqrt{y^2}$ with $(p-1)/2 < y' < p$
\EndIf
\State \textbf{return} $(x,y)$
\EndProcedure
\Ensure $(x,y)\in E(\F_r)$
\end{algorithmic}
\end{algorithm}

The try-and-increment method is relatively easy to implement, and it maintains the cryptographic properties of the original hash function. It should be noted that if the curve is not of prime order, the image of the try-and-increment hash will be a general curve point that might not be an element from the large prime-order subgroup. To map onto the large prime order subgroup it is therefore necessary to apply the technique of cofactor clearing as explained in \ref{def:cofactor_clearing}.

\begin{example} Consider the \curvename{Tiny-jubjub} curve from \examplename{} \ref{TJJ13}. We want to construct a try-and-increment hash function that maps a binary string $s$ of arbitrary length onto the large prime-order subgroup of size $5$ from \examplename{} \ref{eq:TJJ13-logarithmic-order}. 

Since the curve $TJJ\_13$ is defined over the field $\F_{13}$, and the binary representation of $13$ is $Bits(13)=<1,1,0,1>$, one way to implement a try-and-increment function is to apply SHA256 from Sage's hashlib library on the concatenation $s||c$ for some binary counter string $c$, and use the first $4$ bits of the image to try to hash into $\F_{13}$. In case we are able to hash to a value $x$ such that $x^3 +8\cdot x + 8$ is a quadratic residue in $\F_{13}$, we use the fifth bit to decide which of the two possible roots of $x^3 + 8\cdot x + 8$ we will choose as the $y$ coordinate. The result is a curve point different from the point at infinity. To project it onto the large prime order subgroup $TJJ\_13[5]$, we multiply it with the cofactor $4$. If the result is not the point at infinity, it is the result of the hash.

To make this concrete, let $s=<1,1,1,0,0,1,0,0,0,0>$ be our binary string that we want to hash onto $TJJ_13[5]$. We use a binary counter string starting at zero, that is, we choose $c=<0>$. Invoking Sage, we define the try-hash function as follows:
\begin{sagecommandline}
sage: import hashlib
sage: def try_hash(s,c):
....:     s_1 = s+c # string concatenation
....:     hasher = hashlib.sha256(s_1.encode('utf-8')) # compute SHA256
....:     digest = hasher.hexdigest()
....:     z = ZZ(digest, 16) # cast into integer
....:     z_bin = z.digits(base=2, padto=256) # cast to 256 bits
....:     x = z_bin[0]*2^0 + z_bin[1]*2^1 + z_bin[2]*2^2+z_bin[3]*2^3
....:     return (x,z_bin[4])
sage: try_hash('1110010000','0')
\end{sagecommandline}

As we can see, our first attempt to hash into $\F_{13}$ was not successful, as $15$ is not an element in $\F_{13}$, so we increment the binary counter by $1$ and try again: 
\begin{sagecommandline}
sage: try_hash('1110010000','1')
\end{sagecommandline}

With this try, we found a hash into $\F_{13}$. However, this point is not guaranteed to define a curve point. To see that, we insert $x=3$ into the right side of the \concept{short Weierstrass} equation of the \curvename{Tiny-jubjub} curve, and compute $3^3 + 8\cdot 3 + 8 = 7$. However, $7$ is not a quadratic residue in $\F_{13}$, since $7^{\frac{13-1}{2}}=7^6=12=-1$. This means that the field element $7$ is a not suitable as the $x$-coordinate of any curve point. We therefore have to increment the counter another time: 
\begin{sagecommandline}
sage: try_hash('1110010000','10')
\end{sagecommandline}
Since $12^3 + 8\cdot 12 + 8 = 12$, and we have $\sqrt{12} = \{5, 8\}$, we finally found the valid $x$-coordinate $x=12$ for a curve point hash. Now, since the auxiliary bit of this hash is $1$, we choose the larger root $y=8$ as the $y$ coordinate and get the following hash which is a valid curve point on the \curvename{Tiny-jubjub} curve:
$$
H_{TJJ\_13}(<1,1,1,0,0,0,0,0>) = (12,8)
$$

In order to project this onto the ``large'' prime-order subgroup, we have to do cofactor clearing, that is, we have to multiply the point with the cofactor $4$. Using sage we get
\begin{sagecommandline}
sage: P = TJJ_13(12,8)
sage: (4*P).xy()
\end{sagecommandline}

This implies that hashing the binary string $<1,1,1,0,0,0,0,0>$ onto the large prime order subgroup $TJJ\_13[5]$ gives the hash value $(8,8)$ as a result. 
$$
H_{TJJ\_13[5]}(<1,1,1,0,0,0,0,0>) = (8,8)
$$
\end{example}
\begin{exercise}
Use our definition of the $try\_hash$ algorithm to implement a hash function $H_{TJJ\_13[5]} : \{0,1\}^*\to TJJ\_13(\F_{13})[5]$ that maps binary strings of arbitrary length onto the $5$-torsion group of $TJJ13(\F_{13})$. 
\end{exercise}
\begin{exercise}
Implement a cryptographic hash function $H_{secp256k1} : \{0,1\}^*\to secp256k1$ that maps binary strings of arbitrary length onto the elliptic curve \curvename{secp256k1}. 
\end{exercise}