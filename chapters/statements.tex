\chapter{Statements}
% https://people.cs.georgetown.edu/jthaler/ProofsArgsAndZK.pdf
% https://docs.zkproof.org/reference.pdf
As we have seen in the informal introduction XXX, a snarks is a short non-interactive argument of knowledge, where the knowledge-proof attests to the correctness of statements like "I know the prime factorization of a given number" or "I know the preimage to a given SHA3 digest value" and similar things. However human readable statements like those are imprecise and not very useful from a formal perspective. 

In this chapter we therefore look more closely at ways to formalize statements in mathematically rigorous ways, useful for snark development. We start by introducing formal languages as a way to define statements properly. We will then look at algebraic circuits and rank-1 constraint systems as two particulary useful ways to define statements in certain formal languages.

As in many other parts of the book the emphesis is on an introduction from the developers point of view. Proper statement design should be of high priority in the development of snarks, since unintended true statements can lead to potentially severe and almost undetectable attacks on the applications of snarks.

\section{Formal Languages} Formal languages provide the theoretical backround in which logical statements can be formulated in a logically regious way. One might argue that understanding of formal languages is not very important in snark development and associated statement design, but terms from that field of research are standard jargon in many papers on zero knowledge proofs. We therefore believe that at least some introduction to formal languages and how they fit into the picture of snark development is beneficial, mostly to give developers a better intuition where all this is located in the bigger picture of the logic landscape.

Roughly speaking a formal language is nothing but a set of words, that are strings of letters taken from some alphabet and formed according to some defining rules of that language. 

To be more precise, let $\Sigma$ be any set and $\Sigma^*$ the set of all finite tupels $(x_1,\ldots,x_n)$ of elements $x_j$ from $\Sigma$ including the empty tupel $(\;)\in \Sigma^*$. Then a \textbf{formal language} $L$ is in its most general definition nothing but a subset of $\Sigma^*$. In this context, the set $\Sigma$ is called the \textbf{alphabet} of the language $L$, elements from $\Sigma$ are called letters and elements from $L$ are called \textbf{words}. The rules that specify which tupels from $\Sigma^*$ belong to the language and which don't, are called the \textbf{grammar} of the language. 

\paragraph{Checking Relations}
In the context of snark development it is common to formalize the grammar of a laguage by a so called \textbf{checking relation} $R\subset \Sigma^*$. If a checking relation is given, a tupel $x\in \Sigma^*$ is a word $x\in L_R$ in the language $L_R$ associated to the relation $R$, if $x\in R$, which is usually written as $R(x)$.

Unfortunately, writing $R(x)$ for the relation $R$ is a bit misleading since it makes $R$ look like as if it is a function. It is nevertheless the common way of describing checking relations. For the sake of this book we therefore adopt a different point of view and work with what we might call \textbf{checking functions} inatead:
\begin{equation}
R: \Sigma^* \to \{TRUE, FALSE\}
\end{equation}
A checking function therefore decides if a tupel $x\in \Sigma^*$ is an element of the language or not. In this case the language itself can be written as the set of all tupels that satisfies the grammar, i.e as:
\begin{equation}
L := \{x\in \Sigma^*\;|\; R(x)=TRUE\}
\end{equation}
In our context a \textbf{statement} is then a claim, that language $L$ contains a word $x$, i.e a statement claims $x\in L$ and one way to constructively \textit{proof} a staitment is to provided an actual \textbf{instance}, that is an actual word of the language.

Of course formal languages should not be confused with the more intuitive concept of a natural langual. The following examples will provide some intuition about formal languages.
\begin{example}[Alternating Binary strings] To consider a very basic formal language with an almost trivial grammar consider the set $\{0,1\}$ of the two letters $0$ and $1$ as our alphabet $\Sigma$ and imply the rule that a proper word must consist of alternating binary letters of arbitrary length. 

Then the associated language $L_{alt}$ is the set of all finite binary tupels, where a $1$ must follow a $0$ and vice versa. So for example $(1,0,1,0,1,0,1,0,1)\in L_{alt}$ is a proper word as well as $(0)\in L_{alt}$ or the empty word $(\;)\in L_{alt}$. However the binary tupel $(1,0,1,0,1,0,1,1,1)\in \{0,1\}^*$ is not a proper word as it violates the grammer of $L_{alt}$. In addition the tupel $(0,A,0,A,0,A,0)$ is not a proper word as its letter are not from the proper alphabet. 

Inside language $L_{alt}$ it makes sense to claim the following statement: "There exists an alternating string." One way to proof this statement would be by proving an actual instance, that is finding actual alternating string like 
$x = (1,0,1)$. Constructing a string like $(1,0,1)$ therefore proofs that statement "There exists an alternating string."

Atempting to write the grammar of this language in a more formal way, we can define the following checking relation:
$$
R: \{0,1\}^* \to \{TRUE,FALSE\}\;;\; (x1,x_2,\ldots,x_n) \mapsto 
\begin{cases}
TRUE & x_j \neq x_{j+1} \text{ for all } 1\leq j < n \\
FALSE & \text{ else}
\end{cases}
$$
We can use this relation to check if arbitrary binary tupels are words in $L_{alt}$ or not. For example $R(1,0,1)=TRUE$, $R(0)=TRUE$ and $R()=TRUE$, but $R(1,1)=FALSE $ and so on. 
\end{example}
\begin{example}[Programing Language]Programming languages are a very important class of formal languages. In this case the alphabet is usually (a subset) of the ASCII Table and the grammar is defined by the rules of the programming language's compiler. Words are then nothing but properly written computer programms that the compiler accepts. The compiler can therefore be interpreted as the checking relation.

To give an unusual example strange enough to highlight the point, consider the programing language Malbolge as defined in XXX. This language was specifically designed to be almost impossible to use and writing programs in this language is a difficult task. An intersting claim is therefore the statement: "There exists a computer program in Malbolge". As it turned out proofing this statement by providing an actual instance was not an easy task as it took two years after the introduction of Malbolge, to write a program that its compiler accepts. So for two years no one was able to proof the statement.

To look at this high level description more formally, we write $L_{Malbolge}$ for the language, that uses the ASCII table as its alphabet and words are tuples of ASCII letters that the Malbolge compiler accepts. Prooving the statement "There exists a computer program in Malbolge" is the equivalent to the task of finding some word $x\in L_{Malbolge}$. The string
$$
\scriptstyle (=<'\#9]~6ZY327Uv4-QsqpMn\&+Ij"'E\%e\{Ab~w=\_:]Kw\%o44Uqp0/Q?xNvL:'H\%c\#DD2\wedge WV>gY;dts76qKJImZkj
$$
is an example of such a proof as it is excepted by the Malbolge compiler and is compiled to an executable binary that displays "Hello, World." (See XXX)
\end{example}
\begin{example}[3-Factorization]
As one of our main runing examples in this book, we want to develop a snark that proofs knowledge of three factors of some element from the finite field $\F_{13}$. There is nothing particulary useful about this example from an application point of view, however in a sense it is the most simple example that gives rise to a non trivial snark in some of the most common zero knowledge proofing systems. 

Formalizing the a high level description, we can define $\Sigma := \F_{13}$ as the alphabet of our language and then define a language $L_{3.fac}$ to consists of those tupels of field elements from $\F_{13}$, that contain exactly $4$ letters $w_1,w_2,w_3,w_4$ which satisfy the equation $w_1\cdot w_2\cdot w_3 =w_4$.   

So for example the tuple $(2, 12, 4, 5)$ is a word in $L_{3.fac}$, while neither $(2, 12, 11)$, nor $(2, 12, 4, 7)$ nor $(2, 12, 7, 168)$ are words in $L_{3.fac}$ as they dont satisfy the grammar or are not define over the proper alphabet. 

We can describe the language $L_{3.fac}$ more formally by introducing a checking function as described in XXX:
$$
R_{3.fac} : \F_{13}^* \to \{TRUE, FALSE\}\;;\;
(x_1,\ldots,x_n) \mapsto
\begin{cases}
TRUE & n=4 \text{ and } x_1\cdot x_2 \cdot x_3 = x_4\\
FALSE & else
\end{cases}
$$
Having defined the language $L_{3.fac}$ it then makes sense to claim the statement like "There is a word in $L_{3.fac}$" which is logically equivalent to say "There are four elements $w_1,w_2,w_3,w_4$ from the finite field $\F_{13}$" such that the equation $w_1\cdot w_2\cdot w_3 =w_4$ holds. Proofing the correctness of this statement could then be achieved by finding actual field elements like $x_1= 2$, $x_2 =12$, $x_3=4$ and $x_4 = 5$ giving the $L_{3.fac}$ instance $(2,12,4,5)$, which is a proof for our statement.
\end{example}
\paragraph{Instance and Witness}
% https://www.claymath.org/sites/default/files/pvsnp.pdf
As we have seen statements can formulated as membership claims in formal languages and instances can serve as constructive proofs for those claims. However, in the context of \textit{zero-knowledge} proofs  its possible to hide parts of the proofing instance and still be able to proof the statement. In such a context the instance is therefore split into a \textit{public part} which again is called the \textbf{instance} and a not publically known part (a private part) called the \textbf{wittness}.

To acknowledge for this seperation of a proof instance into a public and a private part, our previous definition of a formal language needs a refinement in the context of zero-knowledge proofs. Instead of a single alphabet we consider two alphabets $\Sigma_I$ and $\Sigma_W$, such that the words of the languages are tupels $(i|w)\in \Sigma_I^* \times \Sigma_W^*$ subject to a checking relation
\begin{equation}
R: \Sigma_I^* \times \Sigma_W^* \to \{TRUE, FALSE\}
\end{equation}
that decides if a tupel $(i|w)\in \Sigma_I^* \times \Sigma_W^*$ is an element of the language or not. Again the language itself can be written as the set of all tupels that satisfies the grammar, i.e as:
\begin{equation}
L := \{(i,w)\in \Sigma_I^* \times \Sigma_W^*\in \Sigma^*\;|\; R(i|w)=TRUE\}
\end{equation}
In this case we call a public input $i$ an \textbf{instance} and a private input $w$ a \textbf{wittness} of the relation $R$.

In this context a \textbf{statement} is the claim, that given an instance $i\in\Sigma_I^*$ there is a wittness $w\in \Sigma_W^*$, such that language $L$ contains a word $(i|w)$. As in the situation of more general languages one way to constructively \textit{proof} such a statement is to provided an actual pair $(i|w)$ with $R(i|w)$, however the point of zero-knowledge proofing systems, as we will see in XXX, is to proof such a statement, without revealing any knowledge about $w$. 

So while statements in the sense of the previous chapter can be seen as membership proofs, statements in this refined definition in combination with publically known instances are knowledge-proofs rather.
\begin{example}[3-factorization]
Consider the language $L_{3.fac}$ from example XXX again. Providing instances for the membership statement in this languages is equivalent to providing knowledge of 4 elements $x_1$, $x_2$, $x_3$ and $x_4$ from $\F_{13}$, such that the modular $13$ product of the first three elements is equal to the $4$'th element. 

Splitting instances into private and public parts, we can reformulate the problem introducing different levels of zero-knowledge into the problem. For example we could reformulate the non-hiding membership statement "there are $4$ field elements $x_1$, $x_2$, $x_3$ and $x_4$ from $\F_{13}$, such that $x_1\cdot x_2\cdot x_3 = x_4$ into a statement, where all factors $x_1$, $x_2$, $x_3$ of $x_4$ are private and only the product $x_4$ is public. 

A statement would then be something like "Given a publically known field element $x_4$, there are three factors of $x_4$" and as we will see in XXX a zero-knowledge proofing system is able to proof this statement without revealing anything about the factors $x_1$, $x_2$, or $x_3$.

Formalizing this new language, which we might call $L_{3.fac\_zk}$ we define a checking relation by 
\begin{multline*}
R_{3.fac\_zk} : \F_{13}^* \times \F_{13}^* \to \{TRUE, FALSE\}\;;\;\\
((i_1,\ldots,i_n)|(w_1,\ldots, w_m)) \mapsto
\begin{cases}
TRUE & n=1,\; m=3,\; i_1 = w_1\cdot w_2 \cdot w_3\\
FALSE & else
\end{cases}
\end{multline*}
and as usual $L_{3.fac\_zk}$ is then defined by all tupels from $\F_{13}^* \times \F_{13}^*$ that are mapped onto $TRUE$ under $R_{3.fac\_zk}$. 

Since words in $L_{3.fac\_zk}$ are tuples $(i|w)$ consisting of an instance and a wittness, there are different possibilities to formulate statements. The most general one would be equivalent to the one in XXX claiming "there are words in $L_{3.fac\_zk}$" and a proof, could be given by a concrete pair $(i|w)\in L_{3.fac\_zk}$, such as$(5|2,12,4)$. 

However as explained in XXX, in the context of zero knowlege proofs, statements are rather knowledge-claims like "Given public input $i$, there is a private input $w$. So for example in $L_{3.fac\_zk}$ with public input $i=5$ a proof for the associated statement could be given by $w=(2,12,4)$.

As we will see in XXX, zero-knowledge proofing systems provide techniques to statements like this without revealing anything about the wittness.

One question that arises in this context might be why we decided the factors $x_1$, $x_2$ and $x_3$ to be the wittness and the product $x_4$ to be the instance. This of course was just an arbitrary choice and we could have decided on any other constellation. For example nothing stops us from declaring all variables as private or just $x_1$ or whatever. The actual choice is determined by the application only.
\end{example}
\begin{example}[SHA256 -- Knowlege of Preimage] A standard example to show the power of zero knowlege proofs is proving the knowledge of some preimage of a cryptographic hash function like the SHA256 function, without actually revealing it. 

To understand this example in detail, lets start with introducing a language well suited to build a snark for this problem. Since SHA256 is a function
$$
SHA256: \{0,1\}^* \to {0,1}^{256}
$$
that maps binary string of arbitrary length onto binary strings of length $256$ and we want to proof knowledge of preimages, we have to consider binary strings of size $256$ as instances and binary strings of arbitrary length as witnesses. 

An appropriate alphabet for both the set of all witnesses and the set of all instances is therefore the set $\{0,1\}$ of the two binary letters. We then define a checking relation by
\begin{multline*}
R_{SHA256} : \{0,1\}^* \times \{0,1\}^* \to \{TRUE, FALSE\}\;;\;\\
(i|w) \mapsto
\begin{cases}
TRUE & i.len()=256,\; i = SHA256(w)\\
FALSE & else
\end{cases}
\end{multline*}
and we write $L_{SHA256}$ for the associated language that consists of instance, witness pairs $(i|w)\in L_{SHA256}$ where the instance $i$ is the SHA256 image of the witness $w$. 

Given an instance $i\in \{0,1\}^{256}$ a statements in $L_{SHA256}$ then is the claim, that there is a wittness $w\in \{0,1\}^{*}$, such that $i$ is the image of SHA256 of $w$. One way to proof such a statement would therefore be to actually provide some data that hashes onto $i$. 
\end{example}
\begin{example}[Knowledge of a Discrete Logarithm] As we have explained in XXX computing discrete logarithms can be hard in certain prime fields. An interesting problem is therefore a system that can proof knowledge of discrete logarithms without revealing them.

To formalize a proper statement, lets look at the problem a bit closer. In a more precise sense proofing knowledge of discrete logarithms, is the same thing as finding a solution $x$ to an equation
$$
b^x = y
$$ 
providing that both the base $b$ and the value $y$ are given and that it is understood that $b$ and $y$ are elements from the multiplicative group $\F_{p}^*$ of a prime field for some prime number $p$ and that $x\in \Z_{p-1}$ is a number from modular $(p-1)$-arithmetics, where $(p-1)$ is the order of the multiplicative group of $\F_p$.

We moreover assume $b$ and $p$ to be fixed system parameters and that $y$ is the public input to the problem, while $x$ is the private input. 

We can then define the alphabet $\Sigma_I \times \Sigma_W := \F_p^* \times \Z_{p-1}$ and a checking function
\begin{multline*}
R_{LOG_b(\cdot)} : (\F_p^*)^* \times (\Z_{p-1})^* \to \{TRUE, FALSE\}\;;\;\\
(i|w) \mapsto
\begin{cases}
TRUE & i.len()=1,\; w.len()=1,\; b^w = i\\
FALSE & else
\end{cases}
\end{multline*}
Given an instance $i\in \F_p^*$ a statements in $L_{LOG_b(\cdot)}$ then is the claim, that there is a number $w\in \Z_{p-1}$, such that $w$ is base $b$ discrete logarithm of $i$ in the prime field $\F_p$. One way to proof such a statement would therefore be to actually provide some concrete $w$. 
\end{example}
\paragraph{Modularity and Gadgets} From a developers perspective it is often useful to construct complex statements and their representing languages from simple ones. In the context of zero knowledge proofs those simple building blocks are often called \textit{gadgets} and mature proofing systems usually contain libraries that contain many useful gadgets, like representations of basic types as booleans, unit32, preimage proofs for Hash functions, elliptic curve cryptography and so on. Implementers can then combine these fundamental building blocks to write complex real world applications. 

To understand what this means on the level of formal languages defined by checking relations as explained in XXX, we need to look at the \textit{intersection} of two formal languages, which can be constructed whenever both languages are defined over the same alphabet. In this case the intersection language consists of words that are contained in both languages. To be more precise, let $L_1$ and $L_2$ be two formal languages defined over the instance and witness alphabets $\Sigma_I$ and $\Sigma_W$. Then
\begin{equation}
L_1 \cap L_2 := \{x\;|\; x\in L_1 \text{ and } x\in L_2\}
\end{equation} 
If both languages are defined by checking functions $R_1$ and $R_2$ as explained in XXX, the following function is a checking function for the intersection language $L_1 \cap L_2$:
\begin{equation}
R_{L_1 \cap L_2}: \Sigma_I^* \times \Sigma_W^* \to \{TRUE, FALSE\}\;;\;
(i,w) \mapsto R_1(i,w) \text{ and } R_2(i,w)
\end{equation}
This is an important fact from an implementations point of view as it allows to construct complex checking relations and their statements from simple building blocks. Given a publically known instance $i\in \Sigma_I^*$ a statement in an intersection language then claims knowledge of a wittness that satisfies all relations simultaniously.

\section{Statement Representations} As we have seen in the previous section, formal languages and their definition by checking relations are a powerful tool to describe statements in a formaly regurous manner. 

However from the perspective of existing zero knowledge proofing systems not all ways to actually represent checking relations are equally useful. Depending on the proofing system ad hand some are more suitable then others. In this section will therefore describe the most common ways to represent checking relation and their statements.
\subsection{Rank-1 Quadratic Constraint Systems}
Although checking relations for formal languages can be expressed in various ways, many zk-proof systems require the ralation to be expressed in terms of a systems of rank-1 quadratic equations over a finite field. This true in particular for pairing based proofing systems like XXX, roughly because it is possible to check solutions to those equations "in the exponent" of pairing friendly cryptographic groups. 

\paragraph{R1CS representation} To understand what \textit{rank-1 (quadratic) constraint systems} are in detail, let $\F$ be a finite field, $n$, $m$ and $k$ three numbers and $a_j^i$, $b_j^i$ and $c_j^i$ constants from $\F$ for every $0\leq j \leq n+m$ and $1\leq i \leq k$. Then a R1CS is given by: 
\begin{align*}
\scriptstyle\left(a^1_0 + \sum_{j=1}^n a^1_j \cdot I_j + \sum_{j=1}^m a^1_{n+j} \cdot W_j  \right) \cdot 
\left(b^1_0 + \sum_{j=1}^n b^1_j \cdot I_j + \sum_{j=1}^m b^1_{n+j} \cdot W_j  \right) &= 
\scriptstyle c^1_0 + \sum_{j=1}^n c^1_j \cdot I_j + \sum_{j=1}^m c^1_{n+j} \cdot W_j\\
       & \vdots\\
\scriptstyle\left(a^k_0 + \sum_{j=1}^n a^k_j \cdot I_j + \sum_{j=1}^m a^k_{n+j} \cdot W_j  \right) \cdot 
\left(b^k_0 + \sum_{j=1}^n b^k_j \cdot I_j + \sum_{j=1}^m b^k_{n+j} \cdot W_j  \right) &= 
\scriptstyle c^k_0 + \sum_{j=1}^n c^k_j \cdot I_j + \sum_{j=1}^m c^k_{n+j} \cdot W_j       
\end{align*}
If such a R1CS is given, the parameter $k$ is called the \textbf{number of constraints} and if a tuple $(I_1,\ldots, I_n,W_1,\ldots,W_m)$ of field elements satisfies theses equations, $(I_1,\ldots, I_n)$ is called an \textbf{instance} and $(W_1,\ldots,W_m)$ is called a \textbf{wittness} of the system.

\begin{remark}[Matrix notation] The presentation of rank-1 constraint systems can be greatly siplified using the notation of vectors and matrices. In fact if
$x= (1,I,W)\in \F^{1+n+m}$ is a $(n+m+1)$-dimensional vector, $A$, $B$, $C$ are $(n+m+1)\times k$-dimensional matrices and $\odot$ is the Schur/Hadamard product, then a R1CS can be written as
$$
Ax \odot Bx = Cx
$$
However since we did not introduced matrix calculus in the book, we stick to XXX as the defining equations for rank-1 constraints systems. The latter notation just contains more syntactic suggar and the one frequently used in the research literature.
\end{remark}
A major property of rank-1 constraint systems is, that is can be shown, that every bounded computation can be expressed as a R1CS. R1CS are therefore a universal model for bounded computation. We will see in XXX how "normal" computations can be translated into R1CS and how to construct R1CS compilers that transform high level (bounded) computer programs into R1CS.

On a very general level, it can be said that during computations, the idea of R1CS is to keep track of all the values that each variable assumes and to bind the relationships among all those variables that are implied by the computation itself. This way doing a computation in the expected way is ensured by enforcing relations between every consecutive steps in the computation.
\begin{example}[3-Factorization]Consider the language $L_{3.fac\_zk}$ from example XXX again. As we have seen $L_{3.fac\_zk}$ consist of words $(I_1;W_1,W_2,W_3)$ over the alphabet $\F_{13}$, such that $I_1 = W_1\cdot W_2\cdot W_3$. We show how to reformulate the defining grammar as a rank-1 constraint system.

Since R1CS are quadratic equations, expressions like $W_1\cdot W_2\cdot W_3$ that contain products which contain more then two factors, i.e. which are not quadratic, needs to be rewritten in a process often called "flattening". Do do so, we can introcuce a new variable $W_4$ and then define the following two constraints
\begin{align*}
W_1 \cdot W_2 & = W_4 & \text{constraint } 1\\
W_4 \cdot W_3 & = I_1 & \text{constraint } 2
\end{align*}
It can be shown, that given $I_1$, any solution $(W_1,W_2,W_3,W_4)$ to this system provides a solution to the original equation $I_1 = W_1\cdot W_2\cdot W_3$ and vice versa. Both equations are therefore equivalent in the sense that solutions are in a 1:1 correspondence.

To see that XXX is a rank-1 constraint system, choose the parameter $n=1$, $m=4$ and $k=2$ as well as
$$
\begin{array}{llllll}
a_0^1 = 0 & a_1^1= 1 & a_2^1= 0 & a_3^1 = 0 & a_4^1= 0  & a_5^1= 0 \\ 
a_0^2 = 0 & a_1^2= 0 & a_2^2= 0 & a_3^2 = 0 & a_4^2= 1  & a_5^2= 0 \\ 
\\
b_0^1 = 0 & b_1^1= 0 & b_2^1= 1 & b_3^1 = 0 & b_4^1= 0  & b_5^1= 0 \\ 
b_0^2 = 0 & b_1^2= 0 & b_2^2= 0 & b_3^2 = 1 & b_4^2= 0  & b_5^2= 0 \\ 
\\
c_0^1 = 0 & c_1^1= 0 & c_2^1= 0 & c_3^1 = 0 & c_4^1= 1  & c_5^1= 0 \\ 
c_0^2 = 0 & c_1^2= 0 & c_2^2= 0 & c_3^2 = 0 & c_4^2= 0  & c_5^2= 1 
\end{array} 
$$
Then the R1CS of our $3$-factorization problem contains $4$ witness variables $W_1$, $W_2$, $W_3$ and $W_4$ and one instance variable $I_4$. The witness $W_4$ expresses and intermediate computational state and is constrained to be the product of $W_1$ and $W_2$. Summarizing the definition the R1CS  can be written as follows:
\begin{align*}
\scriptstyle
\left(a^1_0 + a_1^1 I_1 + a_2^1 W_1 + a_3^1 W_2 + a_4^1 W_3 + a_5^1 W_4\right)\cdot
\left(b^1_0 + b_1^1 I_1 + b_2^1 W_1 + b_3^1 W_2 + b_4^1 W_3 + b_5^1 W_4\right) &=
\scriptstyle
\left(c^1_0 + c_1^1 I_1 + c_2^1 W_1 + c_3^1 W_2 + c_4^1 W_3 + c_5^1 W_4\right)\\
\scriptstyle
\left(a^2_0 + a_1^2 I_2 + a_2^2 W_2 + a_3^2 W_2 + a_4^2 W_3 + a_5^2 W_4\right)\cdot
\left(b^2_0 + b_1^2 I_2 + b_2^2 W_2 + b_3^2 W_2 + b_4^2 W_3 + b_5^2 W_4\right) &=
\scriptstyle
\left(c^2_0 + c_1^2 I_2 + c_2^2 W_2 + c_3^2 W_2 + c_4^2 W_3 + c_5^2 W_4\right)
\end{align*}
\end{example}

\paragraph{R1CS Satisfiability}To understand how rank-1 constraint systems give rise to formal languages, oberserve that every given R1CS over a fields $\F$ defines a checking relations over the alphabet $\Sigma_I \times \Sigma_W = \F \times \F$ in the following way:
\begin{equation}
R_{R1CS} : \F^* \times \F^* \to \{TRUE, FALSE\}\;;\;
(I;W) \mapsto
\begin{cases}
TRUE & (I;W) \text{ satisfies R1CS}\\
FALSE & else
\end{cases}
\end{equation}
We write $L_{R1CS-SAT}$ for the associated language and call it \textbf{$R1CS$-satisfyability}. The grammar of this language is exapressed by the constraints in the R1Cs and words in this language are solutions to the R1CS. 

Now since every R1CS gives rise to a formal language, a \textbf{statement} for the given R1CS, is a knowledge claim "Given instance $I$, there is a witness $W$, such that $(I;W)$ satisfies the constraints system". One way to proof such a claim is therefore to to produce an assignment to the variables which satisfies the constraints. If the R1CS represents a computations, such a solution is then a proof proper execution of the computation.
\begin{example}[3-Factorization]Consider the language $L_{3.fac\_zk}$ from example XXX and the R1CS defined in example XXX. As we have seen in XXX solutions to the R1CS are in 1:1 correspondense with solutions to the checking relations of $L_{3.fac\_zk}$. Both languages are therefore equivalent in the sense that there is a 1:1 correspondence between words in both languages. Due to this identification, from now on we weite $L_{3.fac\_zk}$ for the language defined by the R1CS XXX.

To give an intuition of how a naive proof of a statement in $L_{3.fac\_zk}$ looks like, consider the instance $I_1= 11$. To provide a proof for the statement "There exist a witness $W$, such that $(I_1;W)$ is a word in $L_{3.fac\_zk}$" a proofer therefore has to privide proper values for the variables $W_1$, $W_2$, $W_3$ and $W_4$. Since the alphabet is $\F_{13}$, such a proof might be given by
$W=(2,3,4,6)$ since $(I_1;W)$ satisfies the R1CS
\begin{align*}
W_1 \cdot W_2 &= W_4 & \text{since } 2\cdot 3 = 6\\
W_4 \cdot W_3 &= I_1 & \text{since } 6\cdot 4 = 11
\end{align*}
\end{example}


\subsection{Algebraic Circuits} As we have seen in the previous paragraphs, rank-1 constraint systems define equations that instance witness pairs have to satisfy in order to provide words in the associated language.

However R1CS equations can be large and are non-linear in general. It is therefore not posible to solve those equations efficiently in order to find proper witnesses for given instances. A practical question is therefore how to actually compute witneses for R1CS?

In this paragraph we look at a statement representation called \textit{algebraic circuits}, that is closely related to R1CS but which is suitable to not just define the constraints, but to provide a mechanism to compute witnesses efficiently.

\paragraph{Algebraic circuit representation}
The general idea of an algebraic circuit is that, given some finite field $\F$ every rational function over $\F$ can be seen as a directed acyclic (multi)graph: Inner nodes have exactly two incoming edges and represent the fundamental field operationd \textit{addition} and \textit{multiplication}. Nodes with only outgoing edges (source nodes) represent the variables and constants of the functions and nodes with only incoming edges (sink nodes) represent the results of the function. Edges in the graph then represent the connections between the operations of the individual nodes.

To be more precise let $\F$ be a finite field. Then a directed, acyclic multi-graph $C(\F)$ is called an \textbf{algebraic circuit} over $\F$, if the graph has an ordering, every edge has either an intance label $I_j$ or a witness label $W_j$, where $j$ is the edge's position in the graph's order and every node has two labels in the following way:
\begin{itemize}
\item Every source node (called input) has either a symbol that represents a variable from $\F$ or a constant $c\in\F$ as label.
\item Every sink node (called output) has a symbol that represents a variable from $\F$ as label.
\item Every other node has exactly two incoming edges and has a label that represents either addition or multiplication in $\F$.
\end{itemize}
Internal nodes that are neither source nor sink nodes are also often called \textbf{aithmetic gates}. Arithmetic gates that are decorated with the "$+$"-label are called \textbf{addition-gates} and gates that are decorated with the "$\cdot$"-label are called \textbf{multiplication-gates}.

It should be noted that subtraction gates can be simulated in a two step process by first multiplying the subtrahend with $-1$ and then use an addition gate. Also division gates can be simulated for example by using Fermat's little theorem, which states that the multiplicative inverse of a field element $x\in\F$ is given by the power $x^{p-2}$ and powers are nothing but repeated multiplication. However simulating division this way might be inefficient.

It should also be noted that our definition differs slighly from the way algebraic circuits are used in many other parts of mathematics. They are however useful in the context of zero-knowledge proofing systems, because the edge labels define witness and instances, while the restriction on the number of incoming edges simplifies the translation into R1CS. We will look at this more closely in XXX.

It can be shown that every rational function over $\F$ can be transformed into an algebraic circuit, a process often called \textit{flattening}.
\begin{example}[Generalized factorization snark] To give a simple example of an algebraic circuit, consider our $3$-factorization problem from example XXX again.  To express the problem in the algebraic circuit model, consider the following function 
\[
f_{3.fac}:\mathbb{F}_{13}\times\mathbb{F}_{13}\times\mathbb{F}_{13}\to\mathbb{F}_{13};(x_{1},x_{2},x_{3})\mapsto(x_{1}\cdot x_{2})\cdot x_{3}
\]
The zero-knowledge $3$-factorization problem as described in XXX can then be described in the following way: Given instance $I_1\in \F_{13}$ a valid witness a preimage of $f_{3.fac}$ at the point $I_1$, i.e. a valid witness are three values $W_1$, $W_2$ and $W_3$ from $\F_{13}$, such that $f_{3.fac}(W_1,W_2,W_3)=I_1$. 

To flatten function $f_{3.fac}$, that is to find some circuit that describes the function, observe that $f_{3.fac}$ needs two multiplications to compute its output. We therefore need a circuit that contains two multiplication gates and nothing more. We get:
\[
\xymatrix{x_1\ar^{W_1}[dr] &  & x_2\ar_{W_2}[dl]\\
 & \cdot \ar^{W_4}[drr] &   & & x_3\ar_{W_3}[dl]\\
  &  &  & \cdot\ar_{I_1}[d]\\
  &  &  & f(x_1,x_2,x_3)
}
\]
So our directed acyclic multi-graph, is is binary tree with three leaves (the source nodes) labeled by $x_1$, $x_2$ and $x_3$, one root (the single sink node) labeled by $f(x_1,x_2,x_3)$ and two internal nodes, which are labeled as multiplication gates. 

The order we used to label the edges is choosen to make the edge labeling consistent with our decision from example XXX. Its not an order that arises from common ordering algorithms like deapth-first or breath-first ordering. In addition we declear the function output as the instance and the inputs as well as every step in the computation as private inputs.
\end{example}
\begin{example} To give a more realistic example of an algebraic circuit look at the defining equation XXX of the tiny-jubjub curve again. A pair of field elements 
$(x,y)\in \F_{13}^2$ is a curve point, precisely if
$$
3\cdot x^2 + y^2 = 1+ 8\cdot x^2\cdot y^2
$$  
We can rewrite this equation by shifting all terms to the right and get
\begin{align*}
3\cdot x^2 + y^2 & = 1+ 8\cdot x^2\cdot y^2 & \Leftrightarrow\\
0 & = 1+ 8\cdot x^2\cdot y^2 - 3\cdot x^2 - y^2 & \Leftrightarrow\\
0 & = 1+ 8\cdot x^2\cdot y^2 + 10\cdot x^2 +12 y^2
\end{align*}
And we can use this expression to define a function, such that all points of tiny-jubjub are characterized its the preimages of $0$:
$$
f_{tiny-jj}: \F_{13}\times \F_{13}\to \F_{13}\; ; \;
(x,y)\mapsto 1+ 8\cdot x^2\cdot y^2 + 10\cdot x^2 +12 y^2
$$
then of course every pair $(x,y)\in \F_{13}^2$ with $f_{tiny-jj}(x,y)=0$ is a point on the tiny-jubjub curve.

We can flatten this function into an algebraic circuit over $\F_{13}$. To do so we have to decide which edges to declare instance and which to declare witness. In principle we could choose any combination, but for the sake of zero-knowledge proofs as we will discuss later, we only declare the output and the pair $(x,y)$ to be the instance. Everything else should be part of the witness. A proper circuit then might be:
\begingroup
    \fontsize{8pt}{10pt}\selectfont
\[
\xymatrix{
 & &  & &
x\ar_{I_1}@/^/[d]\ar@/_/[d]  &  & & 
y\ar_{I_2}@/^/[d]\ar@/_/[d]\\
 & 10 \ar_{W_1}[dr]& & &
\cdot \ar_{W_1}[drr] \ar^{W_1}[dll]&  & &
\cdot \ar^{W_1}[dl] \ar^{W_1}[ddr] &  &
12 \ar^{W_1}[ddl] \\
1 \ar_{W_1}[dr] & &  
 \cdot \ar_{W_1}[dl] & & 
 8 \ar_{W_1}[dr] & &
 \cdot \ar_{W_1}[dl] & & 
 \\
 & 
 + \ar_{W_1}[drr] & & & &
 \cdot \ar_{W_1}[dll] & & &
 \cdot \ar_{W_1}[ddlll]\\
 & & & 
 + \ar_{W_1}[drr]& & & 
 \\
 & & & & & 
 + \ar_{I_3}[d]\\
 & & & & & 
 f_{tiny-jj}(x,y)
}
\]
\endgroup
\end{example}

\begin{example} To give an intuition about hoto consider algebraic functions over finite fields as directed acyclic graphs, consider the function
$$
f:\F_{13}^2 \to \F_{13}^4\;;\;
(x,y) \mapsto (4x^2 - \frac{x^2}{y^2},- 2xy,  x^2y^2, 4y)
$$
We can write this functiona as a directed acyclic graph. Since the function has $2$ inputs and computes $3$ outputs, the result is a graph with $2$ incoming nodes and $3$ outgoing nodes. Every internalk node representa field operation. We get
\begingroup
    \fontsize{8pt}{10pt}\selectfont
\[
\xymatrix{
x\ar@/^/[d]\ar@/_/[d]  \ar@/_/[drrrr]   &  & 
y\ar@/^/[d]\ar@/_/[d]\ar@/_/[drr] \ar@/^/[drrrr] & & 
4\ar@/^/[ddllll] \ar@/^/[drr]& & 
2 \ar@/^/[dd] & & 
-1\ar@/^/[dddll]\\
\cdot \ar@/^/[d]\ar@/^/[ddrr] \ar@/^/[ddrrrr]&  & 
\cdot \ar@/^/[dd] \ar@/^/[ddrr]& & 
\cdot \ar@/^/[drr] & & 
\cdot \ar@/^/[ddddrr]\\
\cdot \ar@/^/[dd]& & 
\cdot & & 
 & & 
\cdot \ar@/^/[d]\\
 & & 
\cdot \ar@/^/[ddrr]& & 
\cdot/\cdot \ar@/^/[dllll] & & 
\cdot \ar@/^/[ddllll]\\
- \ar@/^/[d] & & 
 & & 
 & & 
\\
\star & & 
\star & & 
\star & & 
& & 
\star
}
\]  
\endgroup
\end{example}

\paragraph{Circuit Execution}
 While R1CS can only check proper witnesses, circuit execution provides a way to actually compute witnesses to given instances. 

 
  

\begin{remark}[Circuit-SAT] Every circuit with $n$ input nodes and $m$ output nodes can be seen a function that transforms strings of size $n$ from $\Sigma_I \times \Sigma_W$ into strings of size $m$ over the same alphabet. The transformation is done by sending the strings from a node along the outgoing edges to other nodes. If those nodes are gates, then the string is transformed according to the label.

By executing the previous transformation, every node of a circuit has an associated letter from $\Sigma_I \times \Sigma_W$ and this defines a checking relation over $\Sigma_I^* \times \Sigma_W^*$. To be more precise, let $C$ be a circuit with $n$ nodes and $(i,w) \in \Sigma_I^j \times \Sigma_W^k$ a string. Then $R_C(i,w)$ iff THE CIRCUIT IS SATISFIED WHEN ALL LABELS ARE ASSOCIATED TO ALL NODES IN THE CIRCUIT.... BUT MORE PRECISE

MODULO ERRORS. TO BE CONTINUED.....

An Assignment associates field elements to all edges (indices) in an algebraic circuit. An Assignment is valid, if the field element arise from executing the circuit. Every other assignment is invalid.

The checking relation for circuit-SAT then is satidfied if valid asignment (TODO: THE WITNESS/INSTANCE SPLITTING)

Valid assignments are proofs for proper circuit execution.
\end{remark}



So to summarize, algebraic circuits (over a field $\mathbb{F}$) are directed acyclic graphs, that express arbitrary, but bounded computation. Vertices with only outgoing edges (leafs, sources) represent inputs to the computation, vertices with only ingoing edges (roots, sinks) represent outputs from the computation and internal vertices represent field operations (Either addition or multiplication). It should be noted however that there are many circuits that can represent the same laguage...

Circuits have a notion of execution, where input values are send from leafs along edges, through internal vertices to roots.

\begin{remark}
Algebraic circuits are usually derived by  Compilers, that transform  higher languages to circuits. An example of such a compiler is XXX. Note: Different Compiler give very different circuit representations and Compiler optimization is important.
\end{remark}


\begin{example}[Generalized factorization snark]
\label{main_example_2_4}
Consider our generalized factorization example \ref{main_example_2_1} with associated language \ref{main_example_2_3}.

To write this example in circuit-SAT, consider the following function 
\[
f:\mathbb{F}_{13}\times\mathbb{F}_{13}\times\mathbb{F}_{13}\to\mathbb{F}_{13};(x_{1},x_{2},x_{3})\mapsto(x_{1}\cdot x_{2})\cdot x_{3}
\]

A valid circuit for $f:\mathbb{F}_{11}\times\mathbb{F}_{11}\times\mathbb{F}_{11}\to\mathbb{F}_{11};(x_{1},x_{2},x_{3})\mapsto(x_{1}\cdot x_{2})\cdot x_{3}$ is given by:

\[
\xymatrix{\star\ar^{in_1}[dr] &  & \star\ar_{in_2}[dl]\\
 & \star_{m_1}\ar^{mid_1}[drr] &   & & \star\ar_{in_3}[dl]\\
  &  &  & \star_{m_2}\ar_{out_1}[d]\\
  &  &  & \star
}
\]
with edge-index set $I:=\{in_{1},in_{2},in_{3},mid_{1},out_{1}\}$.

To given a valid assignment, consider the set $I_{valid}:=\{in_{1},in_{2},in_{3},mid_{1},out_{1}\} = \{2,3,4,6,10\}$

\[
\xymatrix{\star\ar^{2}[dr] &  & \star\ar_{3}[dl]\\
 & \star_{m_1}\ar^{6}[drr] &   & & \star\ar_{4}[dl]\\
  &  &  & \star_{m_2}\ar_{10}[d]\\
  &  &  & \star
}
\]
Appears from multiplying the input values at $m_1$, $m_2$ in $\mathbb{F}_{13}$, hence by executing the circuit.

Non valid assignment: $I_{err}:=\{in_{1},in_{2},in_{3},mid_{1},out_{1}\} =\{2,3,4,7,8\}$
\[
\xymatrix{\star\ar^{2}[dr] &  & \star\ar_{3}[dl]\\
 & \star_{m_1}\ar^{7}[drr] &   & & \star\ar_{4}[dl]\\
  &  &  & \star_{m_2}\ar_{8}[d]\\
  &  &  & \star
}
\]
Can not appear from multiplying the input values at $m_1$, $m_2$ in $\mathbb{F}_{13}$

To match the requirements of the inital task \ref{main_example_2_1}, we have to split the statement into instance and witness. So given index set $I:=\{in_{1},in_{2},in_{3},mid_{1},out_{1}\}$, we assume that every step in the computation other then $in_3$ and $out_1$ are part of the witness. So we choose:
\begin{itemize}
\item Instance $S=\{in_3, out_1\}$. 
\item Witness $W=\{in_1, in_2, mid_{1}\}$.
\end{itemize}
\end{example}

\begin{example}[Baby JubJub for BLS6-6]

\end{example}

\begin{example}[ECDH as a circuit]
over BLS6
\end{example}

\begin{example}[BLS Signature]
example of one layer recursion over MNT4 and MNT6
\end{example}


\begin{example}[Boolean Circuits]

\end{example}

\begin{example}[Algebraic (Aithmetic) Circuits]

\end{example}

Any program  can be reduced to  an arithmetic circuit  (a circuit that contains only addition and multiplication gates). A particular reduction can be found for example in [BSCG+13]

 we produce a circuit whose satisfiability encodes the correctness of execution of
the program. L


\section{Computational Models}
Proofs are the evidence of correctness of the assertions, and people can verify the cor-rectness by reading the proof. However, we obtain much more than the correctness itself:After you read one proof of an assertion, you know not only the correctness, but also why itis correct. Is it possible to solely show the correctness of an assertion without revealing theknowledge of proofs? It turns out that it is indeed possible, and this is the topic of todayâ€™slecture: Zero Knowledge Systems.
% from http://resources.mpi-inf.mpg.de/departments/d1/teaching/ss14/gitcs/notes6.pdf

\begin{example}[Generalized factorization snark]
\label{main_example_2_1}
As one of our major running examples we want to derive a zk-SNARK for the following generalized factorization problem: 

Given two numbers $a,b\in \mathbb{F}_{13}$, find two additional numbers $x,y\in \mathbb{F}_{13}$, such that
$$
(x\cdot y) \cdot a = b 
$$
and proof knowledge of those numbers, without actually revealing them.

Of course this example reduces to the classic factorization problem (over $\F_{13}$ by setting $y=1$)

This zero knowledge system deals with the following situation: "Given two publicly known numbers $a,b \in \mathbb{F}_{13}$ a proofer can show that they know two additional numbers $x,y\in \mathbb{F}_{13}$, such that $(x\cdot y) \cdot a = b$, without actually revealing $x$ or $y$." 

Of course our choice of what information to hide and what to reveal was completely arbitrary. Every other split would also be possible, but eventually gives a different problem. 

For example the task could be to not hide any of the variables.  Such 
a system has no zero knowledge and deals with verifiable computations: "A worker can proof that they multiplied three publicly known numbers $a,b,x \in \mathbb{F}_{13}$ and that the result is $z \in \mathbb{F}_{13}$, in such a way that no verifier has to repeat the computation."
\end{example}

\paragraph{Gadgets}
Rank $1$ contraints systems can become very large ....

\subsubsection{Boolean Algebra} 
% implementations can be found here: https://github.com/filecoin-project/zexe/tree/master/snark-gadgets/src/bits

Sometimes it is necessary to assume that a statement describes boolean variables. However by definition the alphabet of a statement is a finite field, which is often the scalar field of a large prime order cyclic group. So developers need a way to simulate boolean algebra inside other finite fields.

The most common way to do this, is to interpret the additive and multiplicate neutral element $\{0,1\}\subset F$ as boolean values. This is convinient because they are defined in any field. 

In what follows we will define a few of the most basic R1CS to check boolean expressions in R1CS satidfyability. We will leave other basic constructions as exercises to the reader. 

We start with actually constraining field elements to boolean values then   
Once field elements are boolean constraint, we need constraints that are able to enforce boolean algebra on them. We therefore give constraints for the functionally complete set of Boolean operators give by $AND$ and $NOT$. As all other boolean operations can be constructed from $AND$ and $NOT$, this sufficies. However in actual implementations it is of high importance to limit the number of constraints as much as possible. In reality it is therefor advantageous to implement all logic operators in constraints.

\paragraph{Boolean Constraint}
So when a developer needs boolean variables as part of their statement, a R1CS is required on those variables, that enforces the variable to be either $1$ or $0$. So to "constrain a field element $x\in \F$ to be $1$ or $0$ what we need is a system of equation $(A_ix)\cdot (B_ix) = C_ix$ for some $A_i,B_i,C_i\in \F$, such that the only possible solutions for $x$ are $0$ or $1$.
As it turns out such a system can be realized by a single equation
$x \cdot (1-x) =0$
We see that indeed $0$ and $1$ are the only solutions here, since for the right side to be zero, at least one factor on the left side needs to be zero and this only happens for $0$ and $1$. 

So now that we have found a correct equation for a boolean constrain, we have to translate it into the associated R1CS format, which is given by 
$$
\begin{pmatrix}0 & 1 \end{pmatrix} \begin{pmatrix} 1 \\ x \end{pmatrix}\odot
\begin{pmatrix}1 & -1 \end{pmatrix} \begin{pmatrix} 1 \\ x \end{pmatrix} =
\begin{pmatrix}0 & 0 \end{pmatrix} \begin{pmatrix} 1 \\ x \end{pmatrix}
$$
So we get the following statement $\phi = (1,i,w) = (1, x)$, with instance (public input) $i=x$ and now witness (private input) $w$. In addition we get the matrices
$A=\begin{pmatrix}0 & 1\end{pmatrix}$, $B=\begin{pmatrix}1 & -1\end{pmatrix}$ and $C=\begin{pmatrix}0 & 0\end{pmatrix}$.

To make those constraints easily accesable for R1CS developers, a gadget is convinient:

%\begin{algorithmic}
%\Require $m$ Bitlength of modulus
%\Statement $\phi \gets [x,b[m],mid[m]]$
%\State $tmp \gets 0$
%\For{$j\gets 0,\ldots, m-1$}
%	\State \textbf{Constrain:} $b[j]\cdot (1-b[j]) == 0$
%	\State \textbf{Constrain:} $b[j] \cdot 2^j == mid[j]$
%\EndFor
%\State \textbf{Constrain:} $(mid[0]+\ldots+mid[m-1])\cdot 1 == x$
%\end{algorithmic}
   

\paragraph{AND-constraints} Given three field elements $x,y,z\in\F$ that represent boolean variables, we want to find a R1CS, such that $w=(1,x,y,z)$ satisfies the constraint system if and only if $x\; AND \; y =z$. 

So first we have to constrain $x$, $y$ and $z$ to be boolean as explained in XXX. The next thin is we need to find a R1CS that enforces the $AND$ logic. We can simply choose $x\cdot y =z$, since (for boolean constraint values) $x\cdot y$ equals $1$ if and only if both $x$ and $y$ are $1$.  

Now that we have found a correct equation for a boolean constrain, we have to translate it into the associated R1CS format, which is given by 
$$
\begin{pmatrix}0 & 1 & 0 & 0 \end{pmatrix} \begin{pmatrix} 1 \\ x \\ y \\ z \end{pmatrix}\odot
\begin{pmatrix}0 & 0 & 1 & 0 \end{pmatrix} \begin{pmatrix} 1 \\ x \\ y \\ z \end{pmatrix} =
\begin{pmatrix}0 & 0 & 0 & 1 \end{pmatrix}\begin{pmatrix} 1 \\ x \\ y \\ z \end{pmatrix}
$$
Combining this R1CS with the required fthree boolean constraints for $x$, $y$ and $z$ we get
$$
\begin{pmatrix}
0 & 1 & 0 & 0 \\
\hline
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 
\end{pmatrix} \begin{pmatrix} 1 \\ x \\ y \\ z \end{pmatrix}\odot
\begin{pmatrix}
0 & 0 & 1 & 0 \\
\hline
1 & -1 & 0 & 0 \\
1 & 0  & -1 & 0 \\
1 & 0 & 0 & -1 
\end{pmatrix} \begin{pmatrix} 1 \\ x \\ y \\ z \end{pmatrix} =
\begin{pmatrix}
0 & 0 & 0 & 1 \\
\hline
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 
\end{pmatrix}\begin{pmatrix} 1 \\ x \\ y \\ z \end{pmatrix}
$$
So from the way this R1CS is constructed, we know that whatever the underlying field $\F$ is, the only solutions to this equations are
$$
\{(0,0,0), (0,1,0), (1,0,0), (1,1,1)\}
$$
which is the set of all $(x,y,z)\in\{0,1\}^3$ such that $x\; AND\; y = z$.
\paragraph{NOT constraint}
Given two field elements $x,y\in\F$ that represent boolean variables, we want to find a R1CS, such that $w=(1,x,y)$ satisfies the constraint system if and only if $x=\lnot y$. 

So again we have to constrain $x$ and $y$ to be boolean as explained in XXX. The next think is we need to find a R1CS that enforces the $NOT$ logic. We can simply choose $(1-x) =y$, since (for boolean constraint values) this enforces that $y$ is always the boolean opposite of $x$. 

Now that we have found a correct equation for a boolean constrain, we have to translate it into the associated R1CS format, which is given by 
$$
\begin{pmatrix}1 & -1 & 0 \end{pmatrix} \begin{pmatrix} 1 \\ x \\ y \end{pmatrix}\odot
\begin{pmatrix}1 & 0 & 0 \end{pmatrix} \begin{pmatrix} 1 \\ x \\ y \end{pmatrix} =
\begin{pmatrix}0 & 0 & 1 \end{pmatrix}\begin{pmatrix} 1 \\ x \\ y \end{pmatrix}
$$
So actually we wrote the linear equation $1-x=y$ like $(1-x)\cdot 1 = y$ and translated that into the matrix equation.

Combining this R1CS with the required fthree boolean constraints for $x$, $y$ and $z$ we get
$$
\begin{pmatrix}
1 & -1 & 0 \\
\hline
0 & 1 & 0 \\
0 & 0 & 1
\end{pmatrix} \begin{pmatrix} 1 \\ x \\ y \end{pmatrix}\odot
\begin{pmatrix}
1 & 0 & 0 \\
\hline
1 & -1 & 0 \\
1 & 0  & -1 
\end{pmatrix} \begin{pmatrix} 1 \\ x \\ y \end{pmatrix} =
\begin{pmatrix}
0 & 0 & 1 \\
\hline
0 & 0 & 0 \\
0 & 0 & 0 \\
\end{pmatrix}\begin{pmatrix} 1 \\ x \\ y \end{pmatrix}
$$
So from the way this R1CS is constructed, we know that whatever the underlying field $\F$ is, the only solutions to this equations are
$$
\{(0,1), (1,0)\}
$$
which is the set of all $(x,y)\in\{0,1\}^2$ such that $x=\lnot y$.

EXERCISE: DO OR; XOR; NAND

More complicated logical constraints can then be optained by combining all sub-R1CS together. For example if the task is to enforce $(in_1\; AND \lnot in_2 ) AND in_3 = out_1$ we first apply the FLATTENING technique from XXX, which gives is
$$
\begin{array}{lcr}
\lnot in_2 &=& mid_1\\
in_1\; AND \; mid_1 &=& mid_2\\
mid_2 \; AND \; in_3 &=& out_1
\end{array}
$$
So we have the statement $w=(1,in_1,in_2,in_3, mid_1, mid_2,out_1)$, $6$ boolean constraints for the variables, $2$ constraints for the $2$ $AND$ operations and $1$ constraint for the $NOT$ operation.

\subsubsection{Binary representations}
In circuit computations its is often necessary to use the binary representation of a prime field element. Binary representations of prime field elements work execactly like binary representations of ordinary unsigned integers. Only the algebraic operations are different. To compute the binary representation of some number $x\in \F_p$ we need to know the number of bits in the binary representation of $p$ first. We write this as $m= |p_{bin}|$. 

Then a bitstring $(b_0,\ldots,b_m)\in \{0,1\}^m$ is the binary representation of the field element $x$, if and only if
$$
x = b_0\cdot 2^0 + b_1\cdot 2^1 + \ldots + b_m\cdot 2^m
$$ 
Note that, since $p$ is a prime number that has a leading bit $1$ at position $m$. Moreover every prime number $p>2$ is odd and hence has least significat bit set to $1$. Hence all numbers $2^j$ for $0\leq j \leq m$ are elements of $\F_p$ and the equation is well defined. We can therefore enforce this equation as a R1CS, by flattening the equation: 
$$
\begin{array}{lcl}
b_0 \cdot 1 &=& mid_0\\
b_1 \cdot 2 &=& mid_1\\
\cdots & = & \cdots \\
b_m \cdot 2^m &=& mid_m\\
(mid_0 + mid_1 + \ldots + mid_m)\cdot 1 &=& x
\end{array}
$$
So we have the statement $w = (1, x, b_0,\ldots,b_m, mid_0,\ldots,mid_m)$ and we need $(m+1)$ constraints to enforce the binary representation in addition to the $m$ constraints that enforce booleanness.

At this point we see, that writing more complex R1CS becomes clumbsy and in actual implementations people therefore use languages to makes the constraint system more readable. In this example we could write for example something like this:

%\begin{algorithmic}
%\Require $m$ Bitlength of modulus
%\Statement $w \gets [x,b[m],mid[m]]$
%\State $tmp \gets 0$
%\For{$j\gets 1,\ldots, m$}
%	\State \textbf{Constrain:} $b[j]\cdot (1-b[j]) == 0$
%	\State \textbf{Constrain:} $b[j] \cdot 2^j == mid[j]$
%	\State $tmp = tmp + mid[j]$
%\EndFor
%\State \textbf{Constrain:} $tmp \cdot 1 == x$
%\end{algorithmic}

keeping in mind that this is a meta level algorith to \textbf{generate} the R1CS, not the R1CS itself, as constructs like for loops have not direct meening on the level of the R1CS itself.


\begin{example} Considering the prime field $\F_{13}$, we want to enforce the binary representation of $7\in \F_{13}$. To find the number of bits that we need to consider in our R1Cs, we start with the binary representation of $13$, which is $(1,0,1,1)$ since 
$13= 1\cdot 2^0 + 0\cdot 2^1 + 1\cdot 2^2 + 1\cdot 2^3$. So $m=4$ and we have to enforce a $4$-bit representation for $7$, which is $(1,1,1,0)$, since $7= 1\cdot 2^0 + 1\cdot 2^1 + 1\cdot 2^2 + 0\cdot 2^3$.

A valid statement is then given by $w=(1,7,1,1,1,0,1,2,4,0)$ and indeed we satify the $9$ required constraints 
$$
\begin{array}{lclr}
1\cdot (1-1) &=& 0 & \text{// boolean constraints}\\
1\cdot (1-1) &=& 0 &\\
1\cdot (1-1) &=& 0 &\\
0\cdot (1-0) &=& 0  &\\
\\
1 \cdot 1 &=& 1\\
1 \cdot 2 &=& 2\\
1 \cdot 4 &=& 4\\
0 \cdot 8 &=& 0\\
(1 + 2 + 4 + 0)\cdot 1 &=& 7
\end{array}
$$
\end{example}

\subsubsection{Conditional (ternary) operator}
It is often required to implement the terniary conditional operator $?:$ as a R1CS. In general this operator takes three arguments, a boolean value $b$ and two expressions $if\_true$ and $if\_false$, usually written as $b\;?\; c\; :\; d$ and executes $c$ and $d$ according to the value of $b$.

If we assume all three arguments to be values from a finite field, such that $b$ is boolean constraint (XXX), we can enforce a field element $x$ to be the result of the conditional operator as 
$$
x = b\cdot c + (1-b)\cdot d
$$
Flattening the code gives
$$
\begin{array}{lcl}
b \cdot c &=& mid_0\\
(1-b) \cdot d &=& mid_1\\
(mid_0 + mid_1)\cdot 1 &=& x
\end{array}
$$
So we have the statement $w = (1, x, b,c,d, mid_0,mid_1)$ and we need $3$ constraints to enforce the conditional operator in addition to $1$ constraint that enforces booleanness of $b$.


NOTE: THERE WAS THIS PODCAST WITH ANNA AND THE GUY JAN TALKE TO WHERE HE SAID; CONDITIONALS CAN BE IMPLEMENTED SUCH THAT NOT BOTH BRANCHES ARE EXCUTED: LOOK THAT UP

\subsubsection{Range Proofs}
$x>5$...

\subsubsection{UintN}
STUFF ABOUT HOW UINTN COMPUTATIONS ARE NOT STANDARDIZED AND THAT THERE ARE IMPLEMENTATIONS OTHER THEN MOD-N.... WE FIX ON MOD-N. WHAT DO ZEXE CIRCOM ECT FIX ON?

As we know circuits are not defined over integers but over finite fields instead. We therefore have no notation of integers in circuits. However on computers we also not use integers natively but Uint's instead.

As we know a UintN type is a representation of integers in the range of $0 \ldots 2^N$ with the exception that algebraic operations like addition and multiplication deviate from actual integers, whenever the result exceeds the largest representable number $2^N-1$. 

In circuit design it is therefore important to distinguish between various things tht might look like integers, but are actually not. For example Haskells type NAT is an actual implementation of natural numbers. In particular this means ....

\begin{example}[Uint8]
What is $0xFFF0 + 0xFFF0$ and so on...
\end{example}

\paragraph{Bit constraints}
In prime fields, addition and multiplication behaves exactly like addition and multiplication with integers as long as the result does not exceed the modulus. 

This makes the representation of UintNs in a prime field $\F_{p}$ potentially ambigious, as there are two possible representations, whenever $2^N-1 < p$. In that case any element of $UintN$ could be interpreted as an element of $\F_{p}$. This however is dangerous as the algebraic laws like addition and multiplication behave very different in general.  

It is therefore common to represent UintN types in circuits as binary constraints strings of field elements of length $N$.

\begin{example}
Consider the Uint4 type over the prime field $\F_{17}$. Since $2^4=16$, Uint4 can represent the numbers $0,\ldots, 15$ and it would be possible to interpret them as elements in $\F_{17}$. However addition 
\end{example} 



\subsubsection{Twisted Edwards curves}
Sometimes it required to do elliptic curve cryptography "inside of a circuit". This means that we have to implement the algebraic operations (addition, scalar multiplication) of an elliptic curve as a R1CS. To do this efficiently the curve that we want to implement must be defined over the same base field as the field that is used in the R1CS. 

% implmentations https://github.com/iden3/circomlib/blob/master/circuits/babyjub.circom

\begin{example}
So for example when we consider an R1CS over the field $\F_{13}$ as we did in example XXX, then we need a curve that is also defined over $\F_{13}$. Moreover it is advantegous to use a (twisted) Edwards curve inside a circuit, as the addition law contains no branching (See XXX). As we have seen in XXX our Baby-Jubjub curve is an Edwards curve defined over $\F_{13}$. So it is well suited for elliptic curve cryptography in our pend and paper examples
\end{example}

\paragraph{Twisted Edwards curves constraints} As we have seen in XXX, an Edwards curve over a finite field $F$ is the set of all pairs of points $(x,y)\in \F\times \F$, such that $x$ and $y$ satisfy the equation $a\cdot x^2+y^2= 1+d\cdot x^2y^2$. 

We can interpret this equation as a constraint on $x$ and $y$ and rewrite it as a R1CS by applying the flattenin technique from XXX.
$$
\begin{array}{lcr}
x \cdot x &=& x\_sq\\
y \cdot y &=& y\_sq\\
x\_sq \cdot y\_sq &=& xy\_sq\\
(a\cdot x\_sq+y\_sq)\cdot 1 &=& 1+d\cdot xy\_sq
\end{array}
$$
So we have the statement $w=(1,x,y,x\_sq, y\_sq, xy\_sq)$ and we need 4 constraints to enforce that $x$ and $y$ are points on the Edwards curve $x^2+y^2= 1+d\cdot x^2y^2$. Writing the constraint system in matrix form, we get:
\begingroup
    \fontsize{9pt}{9pt}\selectfont
$$
\begin{pmatrix}
0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & a & 1 & 0 
\end{pmatrix} \begin{pmatrix} 1 \\ x \\ y \\ x\_sq \\ y\_sq \\ xy\_sq \end{pmatrix}\odot
\begin{pmatrix}
0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 \\
1 & 0 & 0 & 0 & 0 & 0 
\end{pmatrix}  \begin{pmatrix} 1 \\ x \\ y \\ x\_sq \\ y\_sq \\ xy\_sq \end{pmatrix} =
\begin{pmatrix}
0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 & 0 & d 
\end{pmatrix} \begin{pmatrix} 1 \\ x \\ y \\ x\_sq \\ y\_sq \\ xy\_sq \end{pmatrix}
$$
\endgroup
EXERCISE: WRITE THE R1CS FOR WEIERSTRASS CURVE POINTS 
\begin{example}[Baby-JubJub]
Considering our pen and paper Baby JubJub curve over from XXX, we know that the curve is defined over $\F_{13}$ and that $(11,9)$ is a curve point, while $(2,3)$ is not a curve point. 

Starting with $(11,9)$, we can compute the statement $w=(1,11,9,4,3,12)$. Substituting this into the constraints we get
$$
\begin{array}{lcr}
11 \cdot 11 &=& 4\\
9 \cdot 9 &=& 3\\
4 \cdot 3 &=& 12\\
(1\cdot 4+3)\cdot 1 &=& 1+7\cdot 12
\end{array}
$$
which is true in $\F_{13}$. So our statement is indeed a valid assignment to the twisted Edwards curve constraining system.

Now considering the non valid point $(2,3)$, we can still come up with some kind of statement $w$ that will satisfy some of the constraints. But fixing $x=2$ and $y=3$, we can never satisfy all constraints. For example $w=(1,2,3,4,9,10)$ will satisfy the first three constraints, but the last constrain can not be satisfied. Or $w=(1,2,3,4,3,12)$ will satisfy the first and the last constrain, but not the others.
\end{example}
\paragraph{Twisted Edwards curves addition} As we have seen in XXX one the major advantages of working with (twisted) Edwards curves is the existence of an addition law, that contains no branching and is valid for all curve points. Moreover the neutral element is not "at infinity" but the actual curve poin $(0,1)$.

As we know from XXX, give two points $(x_1,y_1)$ and $(x_2,y_2)$ on a twisted Edwards curve their sum is given by
$$
(x_3,y_3) = \left(\frac{x_1y_2+y_1x_2}{1+d\cdot x_1x_2y_1y_2}, \frac{y_1y_2-a\cdot x_1x_2}{1-d\cdot x_1x_2y_1y_2}\right)
$$
% https://z.cash/technology/jubjub/
We can realize this equation as a R1CS as follows: First not that we can rewrite the addition law as
$$
\begin{array}{lcl}
x_1\cdot x_2 &=& x_{12}\\
y_1\cdot y_2 &=& y_{12}\\
x_1\cdot y_2 &=& xy_{12}\\
y_1\cdot x_2 &=& yx_{12}\\
x_{12}\cdot y_{12} &=& xy_{1212}\\
x_3\cdot (1+d\cdot xy_{1212}) &=& xy_{12}+yx_{12}\\
y_3\cdot (1-d\cdot xy_{1212}) &=& y_{12}-a\cdot x_{12}
\end{array}
$$
So we have the statement $w=(1,x_1,y_1,x_2,y_2,x_3,y_3,x_{12},y_{12},xy_{12},yx_{12},xy_{1212})$ and we need 7 constraints to enforce that $(x_1,y_1)+(x_2,y_2)=(x_3,y_3)$ 
\begin{example}[Baby-JubJub]
Considering our pen and paper Baby JubJub curve over from XXX. We recall from XXX that $(11,9)$ is a generator for the large prime order subgroup. We therefor already know from XXX that
$(11,9) + (7,8) = (11,9) + [3](11,9) = [4](11,9) = (2,9)$. So we compute a valid statement as 
$w=(1,11,9,7,8,2,9,12,7,10,11,6)$. Indeed
$$
\begin{array}{lcl}
11\cdot 7 &=& 12\\
9\cdot 8 &=& 7\\
11\cdot 8 &=& 10\\
9\cdot 7 &=& 11\\
10\cdot 11 &=& 6\\
2\cdot (1+7\cdot 6) &=& 10 + 11\\
9\cdot (1-7\cdot 6) &=& 7 -1\cdot 12
\end{array}
$$
\end{example}
There are optimizations for this using only 6 constraints, available:
% https://github.com/filecoin-project/zexe/blob/master/snark-gadgets/src/groups/curves/twisted_edwards/mod.rs#L129

\paragraph{Twisted Edwards curves inversion} Similar to elliptic curves in Weierstrass form, inversion is cheap on Edwards curve as the negative of a curve point $-(x,y)$ is given by $(-x,y)$. So a curve point $(x_2,y_2)$ is the additive inverse of another curve point $(x_1,y_1)$ precisely if the equation $(x_1,y_1) = (-x_2,y_2)$ holds. We can write this as
$$
\begin{array}{lcl}
x_1 \cdot 1 &=& -x_2 \\
y_1 \cdot 1 &=& y_2
\end{array}
$$
We therefor have a statement of the form $w=(1,x_1,y_1,x_2,y_2)$ and can write the constraints into a matrix equation as
$$
\begin{pmatrix}
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0
\end{pmatrix} \begin{pmatrix} 1 \\ x_1 \\ y_1 \\ x_2 \\ y_2 \end{pmatrix}\odot
\begin{pmatrix}
1 & 0 & 0 & 0 & 0\\
1 & 0 & 0 & 0 & 0
\end{pmatrix} \begin{pmatrix} 1 \\ x_1 \\ y_1 \\ x_2 \\ y_2 \end{pmatrix} =
\begin{pmatrix}
0 & 0 & 0 & -1 & 0\\
0 & 0 & 0 & 0 & 1
\end{pmatrix} \begin{pmatrix} 1 \\ x_1 \\ y_1 \\ x_2 \\ y_2 \end{pmatrix}
$$

In addition we need the following constraints:
$$
\begin{array}{lcl}
x_1 \cdot 1 &=& -x_2 \\
y_1 \cdot 1 &=& y_2
\end{array}
$$

\paragraph{Twisted Edwards curves scalar multiplication} 
% original circuit is here https://iden3-docs.readthedocs.io/en/latest/_downloads/33717d75ab84e11313cc0d8a090b636f/Baby-Jubjub.pdf

Although there are highly optimzed R1CS implementations for scal multiplication on elliptic curves, the basic idea is somewhat simple: Given an elliptic curve $E/\F_r$, a scalar $x\in \F_r$ with binary representation $(b_0,\ldots,b_m)$ and a curve point $P\in E/\F_r$, the scalar multiplication $[x]P$ can be written as
$$
[x]P = [b_0]P + [b_1]([2]P) + [b_2]([4]P) + \ldots + [b_m]([2^m] P)
$$
and since $b_j$ is either $0$ or $1$, $[b_j](kP)$ is either the neutral element of the curve or $[2^j]P$. However $[2^j]P$ can be computed inductively by curve point doubling, since $[2^j]P= [2]([2^{j-1}]P)$.

So scalar multiplication can be reduced to a loop of length $m$, where the original curve point is repeadedly douled and added to the result, whenever the appropriate bit in the scalar is equal to one.

So to enforce that a curve point $(x_2,y_2)$ is the scalar product $[k](x_1,y_1)$ of a scalar $x\in F_r$ and a curve point $(x_1,y_1)$, we need an R1CS the defines point doubling on the curve (XXX) and an R1CS that enforces the binary representation of $x$ (XXX). 

In case of twisted Edwards curve, we can use ordinary addition for doubling, as the constraints works for both cases (doublin is addition, where both arguments are equal). Moreover $[b](x,y)=(b\cdot x, b\cdot y)$ for boolean $b$. Hence flattening equation XXX gives
$$
\begin{array}{lclr}
b_0\cdot x_1 &=& x_{0,1} & // [b_0]P\\
b_0\cdot y_1 &=& y_{0,1}\\

\end{array}
$$
In addition we need to constrain $(b_0,\ldots, b_N)$ to be the binary representation of $x$ and we need to constrain each $b_j$ to be boolean.

As we can see a R1CS for scalar multiplication utilizes many R1CS that we have introduced before. For efficiency and readability it is therefore useful to apply the concept of a gadget (XXX). A pseudocode method to derive the associated R1CS could look like this:

%\begin{algorithmic}
%\Require $m$ Bitlength of modulus
%\Statement $w \gets [x,b[m],mid[m]]$
%\State $tmp \gets 0$
%\For{$j\gets 1,\ldots, m$}
%	\State \textbf{Constrain:} $b[j]\cdot (1-b[j]) == 0$
%	\State \textbf{Constrain:} $b[j] \cdot 2^j == mid[j]$
%	\State $tmp = tmp + mid[j]$
%\EndFor
%\State \textbf{Constrain:} $tmp \cdot 1 == x$
%\end{algorithmic}

%\begin{codebox}
%\Procname{$\proc{Insertion-Sort}(A)$}
%\li \For $j \gets 2$ \To $\id{length}[A]$
%\li     \Do$\id{key} \gets A[j]$
%\li         \Comment Insert $A[j]$ into the sorted sequence $A[1 \twodots j-1]$.
%\li         $i \gets j-1$\li         \While $i > 0$ and $A[i] > \id{key}$
%\li             \Do$A[i+1] \gets A[i]$
%\li                 $i \gets i-1$\End
%\li         $A[i+1] \gets \id{key}$\End
%\end{codebox}

\paragraph{Curve Cycles} A particulary interesting case with far reaching implication is the situation when we have two curve $E_1$ and $E_2$, such that the scalar field of curve $E_1$ is the base field of curve $E_2$ and vice versa. In that case it is possible to implement the group laws of one curve in circuits defined over the scalar field of the other curve. 

\subsubsection{The RAM Model}
FROM THE PODCAST WITH ANNA R. AND THE GUY FROM JAN....


\subsubsection{Generalizations}
many circuits can be found here:
% https://github.com/iden3/circomlib

\subsection{Quadratic Arithmetic Programs}
As shown by [Pinocchio] rank-1 constraint systems can be transformed into so called quadratic  arithmetic  programs  assuming $\F$.

taken from the pinocchio paper. For proving arithmetic circuit-sat.  Given a R1CS QAPs transform potential solution vectors into two polynomials $p$ and $t$, such that $p$ is divisible by $t$ if and only if the vector is a solution to the R1CS. 

They are major building blocks for \textbf{succinct} proofs, since with high probability, the divisibility check can be performed in a single point of those polynomials. So computationally expensive polynomial division check is reduced TO WHAT? (IN FIELDS THERE IS ALWAYS DIVISIBILITY) 
% https://courses.cs.ut.ee/MTAT.07.022/2013_fall/uploads/Main/alisa-report

\begin{definition}[Quadratic Arithmetic Program]
Assume we have a Galois field $\F$, three numbers $i,j,k$ as well as three $(i+j+1) \times k$ matrices $A$, $B$ and $C$  with coefficients in $\F$ that define the R1CS
$Ax \odot Bx = Cx $ for some statement $x=(1,i,w)$ and let $m_1,\ldots,m_k\in \F$ be arbitrary field elements. 

Then a \textbf{quadratic arithmetic program} of the R1CS is the following set of polynomials over $\F$
$$
QAP = \left\{t\in \F[x],\left\{a_h,b_h,c_h\in \F[x]\right\}_{h=1}^{i+j+1}\right\}
$$
where $t(x) := \Pi_{l=1}^k (x- m_l)$ is a polynomial f degree $k$, called the \textbf{target polynomial} of the QAP and $a_h(x)$, $b_h(x)$ as well as $c_h(x)$ are the unique degree $k-1$ polynomials that are defined by the equations
$$
\begin{array}{lllr}
a_h(m_l)=A_{h,l} & b_h(m_l)=B_{h,l} & c_h(m_l)=C_{h,l} & h= 1, \ldots , i+j+1, l=1,\ldots,k 
\end{array}
$$  
\end{definition}
The major point is that R1CS-sat can be reformulated into the divisibility of a polynomials defined by any QAP.
\begin{theorem}
Assume that an R1CS and an associated QAP as defined in XXX are given. Then the affine vector $y=(1,i,w)$ is a solution to the R1CS, if and only if the polynomial
$$
p(x) = \left(\sum y_h\cdot a_h(x)\right)\cdot \left(\sum y_h\cdot b_h(x)\right)  - \sum y_h\cdot c_h(x) 
$$
is divisible by the target polynomial $t$.
\end{theorem}

The polynomials $a_h$, $b_h$ and $c_h$ are uniquely defined by the equations in XXX. However to actually compute them we need some algorithm like the Langrange XXX from XXX.

\begin{example}[Generalized factorization snark]
In this example we want to transform the R1CS from example \ref{main_example_2_3} into an associated QAP.

We start by choosing an arbitrary field element for every constraint in the R1CS, since we have $2$ constraints we choose $m_{1}=5$ and $m_{2}=7$

With this choice we get the target polynomial $t(x)=(x-m_1)(x-m_2)= (x-5)(x-7)= (x+8)(x+6)= x^2 + x +9$.

Since our statement has structure $w=(1, in_1,in_2,in_3,m_1,out_1)$ we have to compute the following degree $1$ polynomials

$\{a_{c},a_{in_{1}},a_{in_{2}},a_{in_{3}},a_{mid_{1}},a_{out}\}$
$\{b_{c},b_{in_{1}},b_{in_{2}},b_{in_{3}},b_{mid_{1}},b_{out}\}$
$\{c_{c},c_{in_{1}},c_{in_{2}},c_{in_{3}},c_{mid_{1}},c_{out}\}$

\item Apply QAP rule XXX to the $a_{k\in I}$ polynomials gives
$$
\begin{array}{llllll}
a_{c}(5)=0, & a_{in_{1}}(5)=1, & a_{in_{2}}(5)=0, & a_{in_{3}}(5)=0, & a_{mid_{1}}(5)=0, & a_{out}(5)=0 \\
a_{c}(7)=0, & a_{in_{1}}(7)=0, & a_{in_{2}}(7)=0, & a_{in_{3}}(7)=0, & a_{mid_{1}}(7)=1, & a_{out}(7)=0\\
\\
b_{c}(5)=0, & b_{in_{1}}(5)=0, & b_{in_{2}}(5)=1, & b_{in_{3}}(5)=0, & b_{mid_{1}}(5)=0, & b_{out}(5)=0 \\
b_{c}(7)=0, & b_{in_{1}}(7)=0, & b_{in_{2}}(7)=0, & b_{in_{3}}(7)=1, & b_{mid_{1}}(7)=0, & b_{out}(7)=0\\
\\
c_{c}(5)=0, & c_{in_{1}}(5)=0, & c_{in_{2}}(5)=0, & c_{in_{3}}(5)=0, & c_{mid_{1}}(5)=1, & c_{out}(5)=0 \\
c_{c}(7)=0, & c_{in_{1}}(7)=0, & c_{in_{2}}(7)=0, & c_{in_{3}}(7)=0, & c_{mid_{1}}(7)=0, & c_{out}(7)=1
\end{array}
$$

Since our polynomials are of degree $1$ only we don't have to invoke Langrange method but can deduce the solutions right away. 

Polynomials are defined on the two values $5$ and $7$ here.
Linear Polynomial $f(x)=m\cdot x + b$ is fully determined by this. Derive the general equation:
\begin{itemize}                        
\item  $5m+b=f(5)$  and $7m+b=f(7)$  
\item  $b=f(5)-5m$ and  $b=f(7)-7m$   
\item  $b=f(5)+8m$ and  $b=f(7)+6m$  
\item  $f(5)+8m=f(7)+6m$              
\item  $8m-6m=f(7)-f(5)$               
\item  $2m=f(7)+ 12f(5)$              
\item  $7\cdot 2m=7(f(7)+12f(5))$              
\item  $m=7(f(7)+12f(5))$ 
\item             
\item  $b=f(5)+8m$                   
\item  $b=f(5)+8\cdot(7(f(7)+12f(5)))$
\item  $b=f(5)+4(f(7)+12f(5))$ 
\item  $b=f(5)+4f(7)+9f(5)$ 
\item  $b= 10f(5)+4f(7)$ 
\end{itemize}
Gives the general equation: $f(x)=7(f(7)+12f(5))x+10f(5)+4f(7)$

For $a_{in_1}$ the computation looks like this:
\begin{itemize}
\item $ a_{in_{1}}(x) = 7(a_{in_{1}}(7)+12a_{in_{1}}(5))x+ 
10a_{in_{1}}(5)+4a_{in_{1}}(7)=$
\item $7(0 + 12\cdot 1)x+ 
10\cdot 1 +4\cdot 0 =$
\item $7\cdot 12 x + 10=$
\item $6x+10$
\end{itemize}
\begin{itemize}
\item $ a_{mid_{1}}(x) = 7(a_{mid_{1}}(7)+12a_{mid_{1}}(5))x+ 
10a_{mid_{1}}(5)+4a_{mid_{1}}(7)=$
\item $7(1 + 12\cdot 0)x+ 10\cdot 0 +4\cdot 1=$
\item $7\cdot 1x +4=$
\item $7x+4 $
\end{itemize}


\begin{tabular}{|l|l|l|}\hline 
$a_{c}(x)=0 $ &$ b_{c}(x)=0   $ & $c_{c}(x)=0$ \tabularnewline\hline 
$a_{in_{1}}(x)=6x+10 $ &$ b_{in_{1}}(x)=0   $ & $c_{in_1}(x)=0$ \tabularnewline\hline 
$a_{in_{2}}(x)=0    $ &$ b_{in_{2}}(x)=6x+10$ & $c_{in_2}(x)=0$ \tabularnewline\hline 
$a_{in_{3}}(x)=0    $ &$ b_{in_{3}}(x)=7x+4$ & $c_{in_{3}}(x)=0$ \tabularnewline\hline 
$a_{mid_{1}}(x)=7x+4$ &$ b_{mid_{1}}(x)=0  $ & $c_{mid_{1}}(x)=6x+10$ \tabularnewline\hline 
$a_{out}(x)=0       $ &$ b_{out}(x)=0      $ & $c_{out}(x)=7x+4$ \tabularnewline\hline 
\end{tabular}
This gives the quadratic arithmetic program for our generalized factorization snark as
$$QAP=\{x^{2}+x+9,\{0,6x+10,0,0,7x+4,0\},\{0,0,6x+10,7x+4,0,0\},\{0,0,0,0,6x+10,7x+4\}\}$$

Now as we recall, the main point for using QAPs in snarks is the fact, that solutions to R1CS are in 1:1 correspondence to the divisibility of a polynomial $p$, constructed from a R1CS solution and the polynomials of the QAP and the target polynomial.

So lets see this in our example. We already know from example XXX, that 
$I=\{1,2,3,4,6,11\}$ is a solution to the R1CS XXX of our problem. To see how this translates to polyinomial divisibility we compute the polynomial $p_I$ by
\begin{align*}
p_I(x)& = (\sum_{h\in |I|} I_h\cdot a_h(x))\cdot 
(\sum_{h\in |I|} I_h\cdot b_h(x)) - 
(\sum_{h\in |I|} I_h\cdot c_h(x)) \\
= & (2(6x+10)+6(7x+4))\cdot(3(6x+10)+4(7x+4))-(6(6x+10)+11(7x+4)) \\
= & ((12x+7)+(3x+11))\cdot((5x+4)+(2x+3))-((10x+8)+(12x+5)) \\
= & (2x+5)\cdot(7x+7)-(9x) \\
= & (x^{2}+2\cdot7x+5\cdot7x+5\cdot7)-(9x) \\
= & (x^{2}+x+9x+9)-(9x) \\
= & x^{2}+x+9
\end{align*}
And as we can see in this particular example $p_I(x)$ is equal to the target polynomial $t(x)$ and hence it is divisible by $t$ with $p/t=1$.

To give a counter example we already know from XXX that $I=\{1,2,3,4,8, 2\}$ is not a solution to our R1CS. To see how this translates to polyinomial divisibility we compute the polynomial $p_I$ by
\begin{align*}
p_I(x)& = (\sum_{h\in |I|} I_h\cdot a_h(x))\cdot 
(\sum_{h\in |I|} I_h\cdot b_h(x)) - 
(\sum_{h\in |I|} I_h\cdot c_h(x)) \\
= & (2(6x+10)+6(7x+4))\cdot(3(6x+10)+4(7x+4))-(6(6x+10)+11(7x+4)) \\
= & 8x^{2}+11x+3
\end{align*}
This polynomial is not divisible by the target polynomial $t$ since
Not divisible by $t$: $(8x^{2}+11x+3)/(x^{2}+x+9) =8+\frac{3x+8}{x^{2}+x+9} $
\end{example}



\subsection{Quadratic span programs}

\section{proof system}
\subsection{Pairing Based SNARKS}
Technique. All pairing-based SNARKs in the literature follow a common paradigm
where the prover computes a number of group elements using generic group operations
and the verifier checks the proof using a number of pairing product equations. Bitansky
et al. [BCI+13] formalize this paradigm through the definition of linear interactive proofs
(LIPs). A linear interactive proof works over a finite field and the proverâ€™s and verifierâ€™s
messages consist of vectors of field elements. It furthermore requires that the prover
computes her messages using only linear operations. Once we have an approriate 2-move
LIP, it can be compiled into a SNARK by executing the equations â€œin the exponentâ€
using pairing-based cryptography. One source of our efficiency gain is that we design
a LIP system for arithmetic circuits where the prover only sends 3 field elements. In
comparison, the quadratic arithmetic programs by [GGPR13,PHGR13] correspond to
LIPs where the prover sends 4 field elements.
A second source of efficiency gain compared


Now a \textit{proof system} is nothing but a game between two parties, where one parties task is to convince the other party, that a given string over some alphabet is a statement is some agreed on language. To be more precise. Such a system is more over \textit{zero knowledge} if this possible without revealing any information about the (parts of) that string.
\begin{definition}[(Interactive) Proofing System]
% https://link.springer.com/content/pdf/10.1007/BF00195207.pdf
Let $L$ be some formal language over an alphabet $\Sigma$. Then an \textbf{interactive proof system} for $L$ is a pair $(P,V)$ of two probabilistic interactive algorithms, where $P$ is called the \textbf{prover} and $V$ is called the \textbf{verifier}. 

Both algorithms are able to send messages to one another. Each algorithm only sees its own state, some shared initial state and the communication messages. 

The verifier is bounded to a number of steps which is polynomial in the size of the shared initial state, after which it stops in an accept state or in a reject state. We impose no restrictions on the local computation conducted by the prover. 

We require that, whenever the verifier is executed the following two conditions hold:
\begin{itemize}
\item (Completeness) If a string $x\in \Sigma^*$ is a member of language $L$, that is $x\in L$ and both prover and verifier follow the protocol; the verifier will accept.
\item (Soundness) If a string $x\in \Sigma^*$ is not a member of language $L$, that is $x\notin L$ and the verifier follows the protocol; the verifier will not be convinced.
\item (Zero-knowledge) If a string $x\in \Sigma^*$ is a member of language $L$, that is $x\in L$ and the prover follows the protocol; the verifier will not learn anything about $x$ but $x\in L$.
\end{itemize}
\end{definition}

In the context of zero knowledge proving systems definition XXX gets a slight adaptation:
\begin{itemize}
\item Instance: Input commonly known to both prover (P) and verifier (V), and used to support the statement of what needs to be proven. This common input may either be local to the prover-verifier interaction, or public in the sense of being known by external parties (Some scientific articles use "instance" and "statement" interchangeably, but we distinguish between the two.).
\item Witness: Private input to the prover. Others may or may not know something about the witness.
\item Relation: Specification of relationship between instances and witness. A relation can be viewed as a set of permissible pairs (instance, witness).
\item Language: Set of statements that appear as a permissible pair in the given relation.
\item Statement:Defined by instance and relation. Claims the instance has a witness in the relation(which is either true or false).
\end{itemize}

The following subsections define ways to describe checking relations that are particularly useful in the context of zero knowledge proofing systems

\subsection{Succinct NIZK}
Preprocessing style: trusted setup, multi party ceremony

Blum, Feldman and Micali
% Manuel  Blum,  Paul  Feldman,  and  Silvio  Micali.   Non-interactive  zero-knowledge  and  itsapplications.  InSTOC, pages 103â€“112, 1988.
 extended the notion tonon-interactivezero-knowledge(NIZK)  proofs in the  common  reference  string  model.  NIZK  proofs  are  useful  in  theconstruction of non-interactive cryptographic schemes, e.g., digital signatures and CCA-secure public key encryption.
 
\begin{definition} 
Let $\mathcal{R}$ be a relation generator that given a security parameter $\lambda$ in unary returns a polynomial time decidable binary relation $R$. For pairs $(i,w)\in R$ we call $i$ the instance\footnote{Note that in Groth16 this is called the statement. We think the term instance is more consistent with SOMETHING. } and $w$ the witness. We define $R_\lambda$ to be the set of possible relations $R$ the relation generator may output given $1^\lambda$. We will in the following for notational simplicity assume $\lambda$ can be deduced from the description of $R$. The relation generator may also output some side information, an auxiliary input $z$, which will be given to the adversary. An efficient prover publicly verifiable non-interactive argument for $R$ is a quadruple of probabilistic polynomial algorithms $(\textsc{Setup},\textsc{Prove},\textsc{Vfy},\textsc{Sim})$ such 
\begin{itemize}
\item Setup: $(CRS,\tau)\rightarrow Setup(R)$: The setup produces a common reference string $CRS$ and a simulation trapdoor $\tau$ for the relation $R$.
\item Proof: $\pi\rightarrow Prove(R,CRS,i,w)$: The prover algorithm takes as input a common reference string $CRS$ and a statement $(i,w)\in R$ and returns an argument $\pi$.
\item Verify: $0/1\rightarrow Vfy(R,CRS,i,\pi)$: The  verification algorithm  takes as input a common reference string $CRS$, an instance $i$ and an argument $\pi$ and returns 0 (reject) or 1 (accept).
\item $\pi\rightarrow Sim(R,\tau,i)$: The simulator takes as input a simulation trapdoor $\tau$ and instance $i$ and returns an argument $\pi$. 
\end{itemize}
\end{definition}

\paragraph{Common Reference String Generation}
Also called trusted setup phase. The field elements needed in this step are called toxic waste ...

\subparagraph{Trusted third party} The most simple approach to generate a common reference string is a so called \textit{trusted third party}. By assumption the entire systems trusts this party to generate the common reference string exactly accoring to the rules and the party will delete all traces of the toxic waste after CRS generation.

\subparagraph{Player exchangeable Multi Party Ceremonies}

Achive soundness if only a single party is honest and correctly deletes toxix waste. Is always zero knowledge.

State of the art
%https://eprint.iacr.org/2017/1050.pdf
works in the random beacon model. 

A random beacon produces publicly available and verifyable random values at fixed intervals. The difference between random beacons and random oracles, is that random beacons are not available until certain time slots. Random beacons can be instanciated for example by evaluation of say $2^{40}$ iterations of SHA256 on some high entropy, publically available data like the closing value of the stock market on a certain date, the output of a selected set of national lotteries and so on. 

The assumption is that any given random beacon value contains large amounts of entropy that is independent from the influence of an adversary in previous time slots. 




\subsubsection{Groth16}
Grothâ€™s  constant  size  NIZK  argument  is  based  on  constructing  a  set  of  polynomial equations and using pairings to efficiently verify these equations. Gennaro, Gentry,Parno and Raykova [Pinocchio] found an insightful construction of polynomial equations based on Lagrange interpolation polynomials yielding a pairing-based NIZK argumentwith a common reference string size proportional to the size of the statement and wit-ness.

It constructs a snark  for arithmetic circuit satisfiability, where a proof consists of only 3 group elements. In addition to being small, the proof is also easy to verify. The verifier just needs to compute a number of exponentiations proportional to the instance size and check a single pairing product equation, which only  has  3  pairings.  

The  construction  can  be  instantiated  with  any  type  of  pairings including Type III pairings, which are the most efficient pairings. The argument has perfect completeness and perfect zero-knowledge. For soundness ?? 

In the common reference string model.

Setup: 
\begin{itemize}
\item random elements $\alpha,\beta,\gamma, \delta, s \in \mathbb{F}_{scalar}$ 
\item Common reference string $CRS_{QAP}$, specific to the $QAP$ and the choice of statement and witness $CRS_{QAP}= (CRS_{\mathbb{G}_1},CRS_{\mathbb{G}_2})$, with $n=deg(t)$: 
$$
CRS_{\mathbb{G}_{1}}=\left\{ \begin{array}{c}
[\alpha]g,[\beta]g,[\delta]g,\left\{ [s^{k}]g\right\} _{k=0}^{n-1},\left\{ [\frac{\beta a_{k}(s)+\alpha b_{k}(s)+c_{k}(s)}{\gamma}]g\right\} _{k\in I}\\
\left\{ [\frac{\beta a_{k}(s)+\alpha b_{k}(s)+c_{k}(s)}{\delta}]g\right\} _{k\in W},\left\{ [\frac{s^{k}t(s)}{\delta}]g\right\} _{k=0}^{n-2}
\end{array}\right\} 
$$
$$
CRS_{\mathbb{G}_{2}}=\left\{ [\beta]h ,[\gamma]h,[\delta]h,\left\{[s^k]h\right\} _{k=0}^{n-1}\right\} 
$$
\item Toxic waste: Must delete random elements after $CRS_{QAP}$ generation.
\end{itemize} 

\begin{example}[Generalized factorization snark]
\label{main_example_2_5}
In this example we want to compile our main example in Groth16. Input is the R1CS from example \ref{main_example_2_4}. We choose the following global parameters

\begin{tabular}{ccccc}
\\
curve = BLS6-6 & $\mathbb{G}_1=$ BLS6-6(13) & $g = (13,15) $
& $\mathbb{G}_2=$ & $h=(7v^2,16v^3)$ and $\mathbb{G}_T = \F_{43^6}^*$.
\end{tabular} 
\end{example}
\begin{example}[Trusted third party for the factorization snark]
We consider ourself as a trusted third part to generate the common reference string for our generalized factorization snark. We therefore choose the following secret field elements $\alpha=6$, $\beta=5$, $\gamma=4$, $\delta=3$, $s=2$ from $\mathbb{F}_{13}$ and are very careful to hide them from anyone how hasn't read this book. From those values we can then instantiate the common reference string XXX:
$$
CRS_{\mathbb{G}_{1}}=\left\{ \begin{array}{c}
[6](13,15),[5](13,15),[3](13,15),\left\{ [s^{k}](13,15)\right\} _{k=0}^{1},\left\{ [\frac{5 a_{k}(2)+6 b_{k}(2)+c_{k}(2)}{4}](13,15)\right\} _{k\in S}\\
\left\{ [\frac{5 a_{k}(2)+6 b_{k}(2)+c_{k}(2)}{3}](13,15)\right\} _{k\in W},\left\{ [\frac{s^{k}t(2)}{3}](13,15)\right\} _{k=0}^{0}
\end{array}\right\}
$$
Since we have instance indices $I=\{1, in_1,in_2\}$ and witness indices $W=\{in_3,mid_1,out_1\}$ we have 
The instance parts.
\begin{multline*}
\left[\frac{5 a_{c}(2)+6 b_{c}(2)+c_{c}(2)}{4}\right](13,15) = 
\left[\frac{5\cdot 0 +6\cdot 0 + 0 }{4}\right](13,15) =
\left[0\right](13,15) = \mathcal{O}
\end{multline*}
\begin{multline*}
\left[\frac{5 a_{in_3}(2)+6 b_{in_3}(2)+c_{in_3}(2)}{4}\right](13,15) =
\left[(5\cdot 0+6\cdot(7\cdot 2 +4)+0)\cdot 10\right](13,15) =\\
\left[(6\cdot 5 )\cdot 10\right](13,15) =
\left[1\right](13,15) =
(13,15)
\end{multline*}
\begin{multline*}
\left[\frac{5 a_{out}(2)+6 b_{out}(2)+c_{out}(2)}{4}\right](13,15) = 
\left[(5\cdot 0 +6\cdot 0 + (7\cdot 2 + 4))\cdot 10 \right](13,15) =\\
\left[5\cdot 10 \right](13,15) =
\left[11\right](13,15) = 
(33,9)
\end{multline*}

Witness part:
\begin{multline*}
\left[\frac{5 a_{in_1}(2)+6 b_{in_1}(2)+c_{in_1}(2)}{3}\right](13,15) = 
\left[(5\cdot (6\cdot 2 +10) +6\cdot 0 +0 )\cdot 9\right](13,15) = \\
\left[(5\cdot 9)\cdot 9\right](13,15) =
\left[2\right](13,15) = (33,34)
\end{multline*}
\begin{multline*}
\left[\frac{5 a_{in_2}(2)+6 b_{in_2}(2)+c_{in_2}(2)}{3}\right](13,15) = 
\left[(5\cdot 0 +6\cdot (6\cdot 2 + 10) + 0 )\cdot 9\right](13,15) = \\
\left[(6\cdot 9)\cdot 9\right](13,15) =
\left[5\right](13,15) =
(26,34)
\end{multline*}
\begin{multline*}
\left[\frac{5 a_{mid_1}(2)+6 b_{mid_1}(2)+c_{mid_1}(2)}{3}\right](13,15) = 
\left[(5\cdot (7\cdot 2 + 4) +6\cdot 0 + 0 )\cdot 9\right](13,15) = \\
\left[(5\cdot 5)\cdot 9\right](13,15) =
\left[4\right](13,15) =
(35,28)
\end{multline*}
For $\left\{\left[\frac{s^{k}t(2)}{3}\right](13,15)\right\} _{k=0}^{0}$ we get
\begin{multline*}
\left[\frac{2^{0}t(2)}{3}\right](13,15)=
[t(2)\cdot 9](13,15)= 
[(2^2+2+9)\cdot 9](13,15)= 
[5](13,15) =
(26,34)
\end{multline*}
All together, the $\mathbb{G}_1$ part of the CRS is:
$$
CRS_{\mathbb{G}_{1}}=\left\{ \begin{array}{c}
(27,34),(26,34),(38,15),\left\{(13,15),(33,34)\right\},
\left\{\mathcal{O}, (13,15), (33,9)\right\}\\
\left\{(33,34),(26,34),(35,28)\right\},
\left\{(26,34)\right\}
\end{array}\right\}
$$
To compute the $\mathbb{G}_2$ part 
$$
CRS_{\mathbb{G}_{2}}=\left\{ [5](7v^2,16v^3) ,[4](7v^2,16v^3),[3](7v^2,16v^3),\left\{[2^k](7v^2,16v^3)\right\} _{k=0}^{1}\right\} 
$$
$$
CRS_{\mathbb{G}_{2}}=\left\{ [5](7v^2,16v^3) ,[4](7v^2,16v^3),[3](7v^2,16v^3),\left\{[1](7v^2,16v^3), [2](7v^2,16v^3)\right\}\right\} 
$$
$$
CRS_{\mathbb{G}_{2}}=\left\{(16v^2,28v^3) ,(37v^2,27v^3),(42v^2,16v^3),\left\{(7v^2,16v^3), (10v^2,28v^3)\right\}\right\} 
$$

So alltogether our common reference string is 
$$
\begin{pmatrix}
\left\{ \begin{array}{c}
(27,34),(26,34),(38,15),\left\{(13,15),(33,34)\right\},
\left\{\mathcal{O}, (13,15), (33,9)\right\}\\
\left\{(33,34),(26,34),(35,28)\right\},
\left\{(26,34)\right\}
\end{array}\right\}\\
\left\{(16v^2,28v^3) ,(37v^2,27v^3),(42v^2,16v^3),\left\{(7v^2,16v^3), (10v^2,28v^3)\right\}\right\}
\end{pmatrix}
$$
\end{example}
\begin{example}[Player exchangeable multi party ceremony for the factorization snark] In this example we want to simulate a real world player exchangeable multi party ceremony for our factorization snark XXX as explained in XXX.

We use our TinyMD5 hash function XXX to hash to $\mathbb{G}_2$.


We assume that we have a coordinator $Alice$ together with three parties $Bob$, $Carol$ and $Dave$ that want to contribute their randomness to the protocol. Since the degree $n$ of the target polynomial is $2$, we need to compute the common reference string
$$
CRS= \left\{\right\}
$$
For contributer $j>0$ in phase $l$ to compute the proof of knowledge XXX, we need to define the $transcript_{l,j-1}$ of the previous round. We define it as sha256 of $MPC_{l,j-1}$. To be more precise we define
$$
transcript_{1,j-1}= 
MD5(
'[s]g_1 [s]g_2 [s^2] g_1 [\alpha]g_1 [\alpha\cdot s]g_1
[\beta]g_1 [\beta]g_2[\beta \cdot s]g_1'
)
$$
The only thing actually important about the transcript, is that it is publically available data that is not accesable for anyone before the MPC-data of round $j-1$ in phase $l$ exists.

We start with the first round usually called the 'powers of tau' EXPLAIN THAT TERM...
The computation is initialized With $s=1$, $\alpha=1$, $\beta=1$. Hence the computation starts with the following data
$$
MPC_{1,0}= \left\{
\begin{array}{lcl}
([s]g_1, [s]g_2) &=& ((13,15),(7v^2,16v^3))\\ 
{}[s^2] g_1 &=& (13,15)\\
{}[\alpha]g_1 &=& (13,15)\\ 
{}[\alpha\cdot s]g_1 &=& (13,15)\\ 
([\beta]g_1,[\beta]g_2) &=& ((13,15),(7v^2,16v^3))\\ 
{}[\beta \cdot s]g_1 &=& (13,15)
\end{array}
\right\}
$$
Then 
\begin{multline*}
transcript_{1,0}=\\ 
MD5('(13,15)(7v^2,16v^3)(13,15)(13,15)(13,15)(13,15)(7v^2,16v^3)(13,15)') =\\ f2baea4d3dba5eef5c63bb210920e7d9
\end{multline*}
We obtain that hash by computing

$printf '\%s' "(13,15)(7v\textasciicircum 2,16v\textasciicircum 3)(13,15)(13,15)(13,15)(13,15)(7v\textasciicircum 2,16v\textasciicircum 3)(13,15)" | md5sum$
% note the actual code is printf '%s' "(13,15)(7v^2,16v^3)(13,15)(13,15)(13,15)(13,15)(7v^2,16v^3)(13,15)" | md5sum

Everyone agreed, that the MPC starts on the 21.03.2020 and everyone can contribute for exactly a year until the 20.03.2021. 


  
It then proceeds in a round robin style, starting with Bob, who optains that data in $MPC_{1,0}$ and then computes his contribution. Lets assume that $Bob$ is honest and that bought 
a 13-sided dice (PICTURE OF 13-SIDED DICE) to randomly find three secret field values from our prime field $\F_{13}$. He though the dice and got $\alpha = 4$, $\beta=8$ and $s= 2$. He then updates $MPC_{1,0}$:  
$$
MPC_{1,1}= \left\{
\begin{array}{lclcl}
([s]g_1, [s]g_2) &=& ([2](13,15),[2](7v^2,16v^3)) &=& ((33,34),(10v^2,28v^3))\\ 
{}[s^2] g_1 &=& [4](13,15)&=& (35,28)\\
{}[\alpha]g_1 &=& [4](13,15)&=& (35,28)\\ 
{}[\alpha\cdot s]g_1 &=& [8](13,15) &=& (26,9)\\ 
([\beta]g_1,[\beta]g_2) &=& ([8](13,15),[8](7v^2,16v^3))&=& ((26,9),(16v^2,15v^3))\\ 
{}[\beta \cdot s]g_1 &=& [3](13,15)&=& (38,15)
\end{array}
\right\}
$$
In addition he compute 
$$
POK_{1,1} \left\{
\begin{array}{lcl}
y_{s} &=& POK(2, f2baea4d3dba5eef5c63bb210920e7d9) = ((33,34),(16v^2 , 28v^3))\\
y_{\alpha} &=& POK(4, f2baea4d3dba5eef5c63bb210920e7d9) = ((35,28),(10v^2 , 15v^3))\\ 
y_{\beta} &=& POK(8, f2baea4d3dba5eef5c63bb210920e7d9) = ((26,9),(16v^2 , 28v^3))\\
\end{array}
\right\}
$$
since $[s]g_1 = (33,34)$, $[\alpha] g_1 = (35,28)$ and $[\beta] g_1 = (26,9)$. as well as 
\begin{align*}
TinyMD5_{2}('(33,34)f2baea4d3dba5eef5c63bb210920e7d9') =\\ H_2(MD5('(33,34)f2baea4d3dba5eef5c63bb210920e7d9').trunc(3))=\\ H_2(2066b3b6b6d97c46c3ac6ee2ccd23ad9.trunc(3))= H_2(ad9) = \\
H_2(101 011 011 001)=\\
[8\cdot 4^{1}\cdot 5^{0}\cdot 7^{1}](7v^2 , 16v^3)+
[12\cdot 1^{0}\cdot 3^{1}\cdot 8^{1}](42v^2 , 16v^3 )+\\
[2\cdot 3^{0}\cdot 9^{1}\cdot 11^{1}](17v^2 , 15v^3 ) +
[3\cdot 6^{0}\cdot 9^{0}\cdot 10^{1}](10v^2 , 15v^3 ) =\\
[8\cdot 4\cdot 7](7v^2 , 16v^3)+
[12\cdot 3\cdot 8](42v^2 , 16v^3 )+
[2\cdot 9\cdot 11](17v^2 , 15v^3 ) +
[3\cdot 10](10v^2 , 15v^3 ) =\\
[8\cdot 4\cdot 7](7v^2 , 16v^3)+
[12\cdot 3\cdot 8](42v^2 , 16v^3 )+
[2\cdot 9\cdot 11](17v^2 , 15v^3 ) +
[3\cdot 10](10v^2 , 15v^3 ) =\\
[3](7v^2 , 16v^3)+
[2](42v^2 , 16v^3 )+
[3](17v^2 , 15v^3 ) +
[4](10v^2 , 15v^3 )=\\
[3](7v^2 , 16v^3)+
[2*3](7v^2 , 16v^3 )+
[3*7](7v^2 , 16v^3 ) +
[4*11](7v^2 , 16v^3 )=\\
(42v^2 , 16v^3)+
(17v^2 , 28v^3 )+
(16v^2 , 15v^3 ) +
(16v^2 , 28v^3 )=\\
[3](7v^2 , 16v^3)+
[6](7v^2 , 16v^3 )+
[8](7v^2 , 16v^3 ) +
[5](7v^2 , 16v^3 )=\\
[3+6+8+5](7v^2 , 16v^3)=
(37v^2 , 16v^3 )
\end{align*}
So we get $[2](37v^2 , 16v^3 )= (16v^2 , 28v^3 )$

===================

\begin{align*}
TinyMD5_{2}('(35,28)f2baea4d3dba5eef5c63bb210920e7d9') =\\ H_2(MD5('(35,28)f2baea4d3dba5eef5c63bb210920e7d9').trunc(3))=\\ H_2(ad54fa3674f6a84fab9208d7a94c9163.trunc(3))= H_2(163) = \\
H_2(000 101 100 011)=\\
[8\cdot 4^{0}\cdot 5^{0}\cdot 7^{0}](7v^2 , 16v^3)+
[12\cdot 1^{1}\cdot 3^{0}\cdot 8^{1}](42v^2 , 16v^3 )+\\
[2\cdot 3^{1}\cdot 9^{0}\cdot 11^{0}](17v^2 , 15v^3 ) +
[3\cdot 6^{0}\cdot 9^{1}\cdot 10^{1}](10v^2 , 15v^3 ) = \\
[8](7v^2 , 16v^3)+
[12\cdot 8](42v^2 , 16v^3 )+
[2\cdot 3](17v^2 , 15v^3 ) +
[3\cdot 9\cdot 10](10v^2 , 15v^3 ) = \\
[8](7v^2 , 16v^3)+
[5](42v^2 , 16v^3 )+
[6](17v^2 , 15v^3 ) +
[10](10v^2 , 15v^3 ) = \\
[8](7v^2 , 16v^3)+
[5*3](7v^2 , 16v^3 )+
[6*7](7v^2 , 16v^3 ) +
[10*11](7v^2 , 16v^3 )=\\
(16v^2 , 15v^3)+
(10v^2 , 28v^3 )+
(42v^2 , 16v^3 ) +
(17v^2 , 28v^3 )=\\
[8](7v^2 , 16v^3)+
[2](7v^2 , 16v^3 )+
[3](7v^2 , 16v^3 ) +
[6](7v^2 , 16v^3 )=\\
[8+2+3+6](7v^2 , 16v^3)=
(17v^2 , 28v^3 )
\end{align*}
So we get $[4](17v^2 , 28v^3 )= (10v^2 , 15v^3 )$

\begin{align*}
TinyMD5_{2}('(26,9)f2baea4d3dba5eef5c63bb210920e7d9') =\\ H_2(MD5('(26,9)f2baea4d3dba5eef5c63bb210920e7d9').trunc(3))=\\ H_2(b87b632f7027ad78cadc2452beb30e9a.trunc(3))= H_2(e9a) = \\
H_2(111 010 011 010)=\\
[8\cdot 4^{1}\cdot 5^{1}\cdot 7^{1}](7v^2 , 16v^3)+
[12\cdot 1^{0}\cdot 3^{1}\cdot 8^{0}](42v^2 , 16v^3 )+\\
[2\cdot 3^{0}\cdot 9^{1}\cdot 11^{1}](17v^2 , 15v^3 ) +
[3\cdot 6^{0}\cdot 9^{1}\cdot 10^{0}](10v^2 , 15v^3 )= \\
[8\cdot 4\cdot 5\cdot 7](7v^2 , 16v^3)+
[12\cdot 3](42v^2 , 16v^3 )+
[2\cdot 9\cdot 11](17v^2 , 15v^3 ) +
[3\cdot 9](10v^2 , 15v^3 )= \\
[2](7v^2 , 16v^3)+
[10](42v^2 , 16v^3 )+
[3](17v^2 , 15v^3 ) +
[1](10v^2 , 15v^3 )= \\
[2](7v^2 , 16v^3)+
[10*3](7v^2 , 16v^3 )+
[3*7](7v^2 , 16v^3 ) +
[1*11](7v^2 , 16v^3 )=\\
(10v^2 , 28v^3)+
(37v^2 , 27v^3 )+
(16v^2 , 15v^3 ) +
(10v^2 , 15v^3 )=\\
[2](7v^2 , 16v^3)+
[4](7v^2 , 16v^3 )+
[8](7v^2 , 16v^3 ) +
[11](7v^2 , 16v^3 )=\\
[2+4+8+11](7v^2 , 16v^3)=
(7v^2 , 27v^3 )
\end{align*}
So we get $[8](17v^2 , 28v^3 )= (16v^2 , 28v^3 )$

So Bob publishes $MPC_{1,1}$ as well as $POK_{1,1}$ and after that its Carols turn. Lets also assume that Carrol is honest. So Carol looks at Bobs data and compute the transcript according to our rules
\begin{multline*}
transcript_{1,1}=\\ 
MD5('
(33,34)(10v^2,28v^3)(35,28)(35,28)(26,9)(26,9)(16v^2,15v^3)(38,15)') =\\ fe72e18b90014062682a77136944e362
\end{multline*}
We obtain that hash by computing

$printf '\%s' "(33,34)(10v^2,28v^3)(35,28)(35,28)(26,9)(26,9)(16v^2,15v^3)(38,15)" | md5sum$

Carol then computes here contribution. Since she is honest she chooses randomly three secret field values from our prime field $\F_{13}$, by invoking her compter. She found $\alpha = 3$, $\beta=4$ and $s= 9$ and updates $MPC_{1,1}$:  
$$
MPC_{1,2}= \left\{
\begin{array}{lclcl}
([s]g_1, [s]g_2) &=& ([9](33,34),[9](10v^2,28v^3)) &=&  ((26,34),(16v^2,28v^3))\\ 
{}[s^2] g_1 &=& [9\cdot 9](35,28) &=& (13,28)\\
{}[\alpha]g_1 &=& [3](35,28) &=& (13,28) \\ 
{}[\alpha\cdot s]g_1 &=& [3\cdot 9](26,9) &=& (26,9)\\ 
([\beta]g_1,[\beta]g_2) &=& ([4](26,9),[4](16v^2,15v^3)) &=& ((27,34),(17v^2,28v^3))\\ 
{}[\beta \cdot s]g_1 &=& [4\cdot 9](38,15) &=& (35,28)
\end{array}
\right\}
$$
In addition he compute 
$$
POK_{1,2} \left\{
\begin{array}{lcl}
y_{s} &=& POK(9, fe72e18b90014062682a77136944e362) = ((35,15),(17v^2 , 28v^3))\\
y_{\alpha} &=& POK(3, fe72e18b90014062682a77136944e362) = ((38,15),(17v^2 , 15v^3 ))\\ 
y_{\beta} &=& POK(4, fe72e18b90014062682a77136944e362) = ((35,28),(42v^2 , 27v^3 ))\\
\end{array}
\right\}
$$

\begin{align*}
TinyMD5_{2}('(35,15)fe72e18b90014062682a77136944e362') =\\ H_2(MD5('(35,15)fe72e18b90014062682a77136944e362').trunc(3))=\\ H_2(115f145ceffdda73e916dc5ba8ae7354.trunc(3))= H_2(354) = \\
H_2(001 101 010 100)=\\
[8\cdot 4^{0}\cdot 5^{0}\cdot 7^{1}](7v^2 , 16v^3)+
[12\cdot 1^{1}\cdot 3^{0}\cdot 8^{1}](42v^2 , 16v^3 )+\\
[2\cdot 3^{0}\cdot 9^{1}\cdot 11^{0}](17v^2 , 15v^3 ) +
[3\cdot 6^{1}\cdot 9^{0}\cdot 10^{0}](10v^2 , 15v^3 )= \\
[8\cdot 7](7v^2 , 16v^3)+
[12\cdot 8](42v^2 , 16v^3 )+
[2\cdot 9](17v^2 , 15v^3 ) +
[3\cdot 6](10v^2 , 15v^3 )= \\
[4](7v^2 , 16v^3)+
[5](42v^2 , 16v^3 )+
[5](17v^2 , 15v^3 ) +
[5](10v^2 , 15v^3 )= \\
[4](7v^2 , 16v^3)+
[5*3](7v^2 , 16v^3 )+
[5*7](7v^2 , 16v^3 ) +
[5*11](7v^2 , 16v^3 )=\\
(37v^2 , 27v^3)+
(10v^2 , 28v^3 )+
(37v^2 , 16v^3 ) +
(42v^2 , 16v^3 )=\\
[4](7v^2 , 16v^3)+
[2](7v^2 , 16v^3 )+
[9](7v^2 , 16v^3 ) +
[3](7v^2 , 16v^3 )=\\
[4+2+9+3](7v^2 , 16v^3)=
(16v^2 , 28v^3 )
\end{align*}
So we get $[9](16v^2 , 28v^3 )= (17v^2 , 28v^3 )$

\begin{align*}
TinyMD5_{2}('(38,15)fe72e18b90014062682a77136944e362') =\\ H_2(MD5('(38,15)fe72e18b90014062682a77136944e362').trunc(3))=\\ H_2(cc4da0c02c4c1b15e72d6cc6430206ab.trunc(3))= H_2(6ab) = \\
H_2(011 010 101 011)=\\
[8\cdot 4^{0}\cdot 5^{1}\cdot 7^{1}](7v^2 , 16v^3)+
[12\cdot 1^{0}\cdot 3^{1}\cdot 8^{0}](42v^2 , 16v^3 )+\\
[2\cdot 3^{1}\cdot 9^{0}\cdot 11^{1}](17v^2 , 15v^3 ) +
[3\cdot 6^{0}\cdot 9^{1}\cdot 10^{1}](10v^2 , 15v^3 )= \\
[8\cdot 5\cdot 7](7v^2 , 16v^3)+
[12\cdot 3](42v^2 , 16v^3 )+
[2\cdot 3\cdot 11](17v^2 , 15v^3 ) +
[3\cdot 9\cdot 10](10v^2 , 15v^3 )= \\
[7](7v^2 , 16v^3)+
[10](42v^2 , 16v^3 )+
[1](17v^2 , 15v^3 ) +
[10](10v^2 , 15v^3 )= \\
[7](7v^2 , 16v^3)+
[10*3](7v^2 , 16v^3 )+
[1*7](7v^2 , 16v^3 ) +
[10*11](7v^2 , 16v^3 )=\\
(17v^2 , 15v^3)+
(17v^2 , 28v^3 )+
(17v^2 , 15v^3 ) +
(17v^2 , 28v^3 )=\\
[7](7v^2 , 16v^3)+
[4](7v^2 , 16v^3 )+
[7](7v^2 , 16v^3 ) +
[6](7v^2 , 16v^3 )=\\
[7+4+7+6](7v^2 , 16v^3)=
(10v^2 , 15v^3)
\end{align*}
So we get $[3](10v^2 , 15v^3 )= (17v^2 , 15v^3 )$

\begin{align*}
TinyMD5_{2}('(35,28)fe72e18b90014062682a77136944e362') =\\ H_2(MD5('(35,28)fe72e18b90014062682a77136944e362').trunc(3))=\\ H_2(502323bc55c75f7189fad7999c9f1708.trunc(3))= H_2(708) = \\
H_2(011 100 001 000)=\\
[8\cdot 4^{0}\cdot 5^{1}\cdot 7^{1}](7v^2 , 16v^3)+
[12\cdot 1^{1}\cdot 3^{0}\cdot 8^{0}](42v^2 , 16v^3 )+\\
[2\cdot 3^{0}\cdot 9^{0}\cdot 11^{1}](17v^2 , 15v^3 ) +
[3\cdot 6^{0}\cdot 9^{0}\cdot 10^{0}](10v^2 , 15v^3 )= \\
[8\cdot 5\cdot 7](7v^2 , 16v^3)+
[12](42v^2 , 16v^3 )+
[2\cdot 11](17v^2 , 15v^3 ) +
[3](10v^2 , 15v^3 )= \\
[7](7v^2 , 16v^3)+
[12](42v^2 , 16v^3 )+
[9](17v^2 , 15v^3 ) +
[3](10v^2 , 15v^3 )= \\
[7](7v^2 , 16v^3)+
[12*3](7v^2 , 16v^3 )+
[9*7](7v^2 , 16v^3 ) +
[3*11](7v^2 , 16v^3 )=\\
(17v^2 , 15v^3)+
(42v^2 , 27v^3 )+
(10v^2 , 15v^3 ) +
(17v^2 , 15v^3 )=\\
[7](7v^2 , 16v^3)+
[10](7v^2 , 16v^3 )+
[11](7v^2 , 16v^3 ) +
[7](7v^2 , 16v^3 )=\\
[7+10+11+7](7v^2 , 16v^3)=
(37v^2 , 16v^3)
\end{align*}
So we get $[4](37v^2 , 16v^3 )= (42v^2 , 27v^3 )$

Dave thinks he can outsmart the syste, Since he is the last to contribute, he just makes up an entirely new $MPC$, that does not contain any randomness from the previous contributers. He thinks he can do that because, no one can distinguish his $MPC_{1,3}$ from a correct one. If this is done in a smart way, he will even be able to compute the correct $POK$s. 

So Dave choses $s=12$, $\alpha=11$ and $\beta=10$ and he will keep those values, hoping to be able to use them later to forge false proofs in the factorization snark. He then compute  
$$
MPC_{1,3}= \left\{
\begin{array}{lcl}
([s]g_1, [s]g_2) &=& ((13,28),(7v^2,27v^3))\\ 
{}[s^2] g_1 &=& (13,15)\\
{}[\alpha]g_1 &=& (33,9)\\ 
{}[\alpha\cdot s]g_1 &=& (33,34)\\ 
([\beta]g_1,[\beta]g_2) &=& ((38,28),(42v^2,27v^3))\\ 
{}[\beta \cdot s]g_1 &=& (38,15)
\end{array}
\right\}
$$
Dave does not delete $s$, $\alpha$ and $\beta$, because if this is accepted as phase one of the common reference string computation, Dave controls already $3/4$-th of the cheating key to forge proofs. So Dave is careful to get the proofs of knowledge right. He computes the transcript of Carols contribution as 

\begin{multline*}
transcript_{1,2}=\\ 
MD5('
(26,34)(16v^2,28v^3)(13,28)(13,28)(26,9)(27,34)(17v^2,28v^3)(35,28)') =\\ c8e6308fffd47009f5f65e773ae4b499
\end{multline*}

We obtain that hash by computing

$printf '\%s' "(26,34)(16v^2,28v^3)(13,28)(13,28)(26,9)(27,34)(17v^2,28v^3)(35,28)" | md5sum$

\end{example}
